%
% wahrscheinlichkeit.tex -- 1. Kapitel, Wahrscheinlichkeitsbegriff
%
% (c) 2006-2015 Prof. Dr. Andreas Mueller, HSR
%
\rhead{Ereignisse und Wahrscheinlichkeit}
\chapter{Ereignisse und ihre Wahrscheinlichkeit} \label{chapter-ereignisse-und-wahrscheinlichkeit}

Es ist unmöglich den Ausgang eines Sportwettkampfes vorauszusagen.
Trotzdem trauen sich viele Leute zu, auf Grund ihrer Kenntnisse über den
Trainingsstand der Sportler oder ihrer früheren Leistungen,
Prognosen abzugeben.
Sie sind sich dabei oft so sicher, dass sie bereit sind, Geld auf ihren Tipp
zu wetten.
Eine solche Prognose ist nicht immer gleich schwierig.
Wenn an der
Fussball WM der Favorit Brasilien gegen Ghana antritt,
rechnet wohl niemand ernsthaft mit einem Sieg Ghanas.
Die Leute würden
sagen, es sei unwahrscheinlich, dass das Ereignis ``Ghana gewinnt'' eintreffen
würde.
Anders sieht es aus, wenn die Schweiz auf die Ukraine trifft.
Die Prognosen über den Ausgang dieses Matches waren auch abseits
manchmal etwas chauvinistischer Hoffnungen mindestens uneinheitlich.

Wie auch immer, die einzige Möglichkeit herauszufinden, welche Mannschaft
stärker ist, besteht darin, das Spiel tatsächlich durchzuführen.
Vorher würden wir einen Sieg der Schweizer Nati bestenfalls als
wahrscheinlich oder eher unwahrscheinlich einstufen.
Welche der beiden Möglichkeiten tatsächlich eintritt, lässt sich
erst nach dem Spiel sagen.

Wir haben also ein Ereignis, nämlich einen Sieg der Schweizer Nati,
und sind mindestens gefühlsmässig in der Lage, dem Eintreten dieses
Ereignis eine Wahrscheinlichkeit zuzuordnen.
Eine Sicherheit haben wir aber nicht.
Dazu müssen wir das Experiment durchführen, und sehen dann,
welches Ereignis, Sieg oder Niederlage eingetreten ist.

Die Wahrscheinlichkeitsrechnung will solchen Aussagen einen präzisen
mathematischen Sinn geben.
Daher brauchen wir zunächst Definitionen
für die Begriffe ``Ereignis'' und ``Wahrscheinlichkeit''.
Mit geeigneten
Rechengesetzen sollte die Wahrscheinlichkeit berechnet werden können,
und wir sollten eine rationale Grundlage für Sportwetten finden können.

\section{Versuche und Versuchsausgänge}
Die Wahrscheinlichkeitsrechnung befasst sich also mit dem Ausgang von
Versuchen.
Das in der Einleitung beschriebene Fussballspiel ist ein solches Experiment.
Natürlich interessiert den Fussballfan viel mehr als das Ergebnis,
für die Zwecke der Sportwetten genügen jedoch die Schlussresultate.
Ähnliche Experimente kann man sich in grosser Zahl ausdenken:
\begin{enumerate}
\item
\index{Munzwurf@Münzwurf}
Der Wurf einer Münze ist ein Experiment mit, wenigstens
im Prinzip, drei Versuchsausgängen: Kopf oder Zahl, ganz selten könnte
die Münze auch auf der Kante stehen bleiben.
\item
\index{Wurfel@Würfel}
Ein weiteres Experiment ist der Wurf eines Würfels.
Klassische Würfel
haben sechs Seitenflächen, die verschiedene Augenzahlen zeigen.
Nach jeder Durchführung des Experimentes zeigt der Würfel ein neues
Resultat, das Experiment hat sechs verschiedene Ausgänge.
\item
Der gleichzeitige Wurf zweier verschiedenfarbiger Würfel ist ein Experiment
mit 36 verschiedenen möglichen Ausgängen:
\begin{center}
\def\e#1#2{\epsdice[black]{#1}\,\epsdice{#2}}
\begin{tabular}{|c|cccccc|}
\hline
&\epsdice{1}&\epsdice{2}&\epsdice{3}&\epsdice{4}&\epsdice{5}&\epsdice{6}\\
\hline
\epsdice[black]{1}&\e{1}{1}&\e{1}{2}&\e{1}{3}&\e{1}{4}&\e{1}{5}&\e{1}{6}\\
\epsdice[black]{2}&\e{2}{1}&\e{2}{2}&\e{2}{3}&\e{2}{4}&\e{2}{5}&\e{2}{6}\\
\epsdice[black]{3}&\e{3}{1}&\e{3}{2}&\e{3}{3}&\e{3}{4}&\e{3}{5}&\e{3}{6}\\
\epsdice[black]{4}&\e{4}{1}&\e{4}{2}&\e{4}{3}&\e{4}{4}&\e{4}{5}&\e{4}{6}\\
\epsdice[black]{5}&\e{5}{1}&\e{5}{2}&\e{5}{3}&\e{5}{4}&\e{5}{5}&\e{5}{6}\\
\epsdice[black]{6}&\e{5}{1}&\e{6}{2}&\e{6}{3}&\e{6}{4}&\e{6}{5}&\e{6}{6}\\
\hline
\end{tabular}
\end{center}
\item 
Der gleichzeitige Wurf zweier ununterscheidbarer Würfel ist ein 
ein ganz ähnliches Experiment, aber die Zahl der Versuchsausgänge
ist kleiner.
Da die Würfel nicht unterscheidbar sind, kann man das Paar
\epsdice{5}\,\epsdice{6} nicht
vom Paar \epsdice{6}\,\epsdice{5} unterschieden.
Man kann beim Auflisten der Versuchsausgänge die Paare immer aufsteigend
ordnen, es sind also nur die folgenden Versuchsausgänge möglich:
\begin{center}
\def\e#1#2{\epsdice{#1}\,\epsdice{#2}}
\begin{tabular}{|c|cccccc|}
\hline
&\epsdice{1}&\epsdice{2}&\epsdice{3}&\epsdice{4}&\epsdice{5}&\epsdice{6}\\
\hline
\epsdice{1}&\e{1}{1}&\e{1}{2}&\e{1}{3}&\e{1}{4}&\e{1}{5}&\e{1}{6}\\
\epsdice{2}&        &\e{2}{2}&\e{2}{3}&\e{2}{4}&\e{2}{5}&\e{2}{6}\\
\epsdice{3}&        &        &\e{3}{3}&\e{3}{4}&\e{3}{5}&\e{3}{6}\\
\epsdice{4}&        &        &        &\e{4}{4}&\e{4}{5}&\e{4}{6}\\
\epsdice{5}&        &        &        &        &\e{5}{5}&\e{5}{6}\\
\epsdice{6}&        &        &        &        &        &\e{6}{6}\\
\hline
\end{tabular}
\end{center}
Es bleiben also nur noch 
\[
1+2+\dots +6=\sum_{i=1}^6=\frac{6\cdot(6+1)}2=21
\]
Versuchsausgänge übrig.
Das heisst aber noch lange nicht, dass der Versuchsausgang
\epsdice{2}\,\epsdice{3} gleich wahrscheinlich ist wie 
\epsdice{3}\,\epsdice{3}, denn letzterer kann nur auf genau eine
Art entstehen, während es für \epsdice{2}\,\epsdice{3}
zwei Möglichkeiten gibt.
\item 
Ein Gewitter tobt über einem Wald. 
Es ist möglich, dass ein Blitz in einen Baum einschlägt,
die meisten Bäume werden jedoch verschont bleiben.
Wir können dieses Experiment zwar nicht jederzeit durchführen,
aber wir können einfach auf das nächste Gewitter warten.
Ist der Baum getroffen, können wir ihn auch nicht wieder verwenden,
aber wir können die Beobachtung an einem anderen, vergleichbaren
Baum wiederholen.
\item
\index{Ebola}
Menschen, die sich mit Ebola-Viren anstecken, sterben sehr häufig
an dieser Krankheit.
Auch hier liegt ein Experiment vor, welches wir nicht nach belieben
durchführen dürfen, doch wir können darauf warten, dass jemand
erkrankt, und dann vergleichbare Krankheitsfälle untersuchen.
\item
Die Messung mit einem Messgerät ist ebenfalls ein wiederholbares
Experiment, der Versuchsausgang ist der vom Messgerät angezeigte
Messwert.
\end{enumerate}

Bevor also Aussagen über die Wahrscheinlichkeit gemacht werden können,
muss definiert werden, welches Experiment genau durchgeführt worden ist.
Nur Experimente, die im Prinzip wiederholbar sind, sind der Untersuchung
durch die Wahrscheinlichkeitsrechnung zugänglich.
Die Aussage: ``die Wahrscheinlichkeit, dass die Naturkonstanten des
Universums so sind, dass intelligentes Leben entstehen kann, ist\dots''
ist nicht nur deshalb Unsinn, weil wir nicht wissen, welche
Voraussetzungen zu intelligentem Leben führen, sondern vor allem wegen
der Tatsache, dass wir keine anderen Universen zur Verfügung haben, in denen
wir die Frage nach intelligentem Leben erneut stellen könnten.

Ein logisches Argument kann nicht eine Wahrscheinlichkeit haben,
richtig oder falsch zu sein.
Ein Argument ist einfach richtig oder falsch.
Die Voraussetzungen für das logische Argument sind vielleicht Beobachtungen
in der Natur, also Versuchsausgänge anderer Experimente, und sie
können das Argument stützen, aber ein richtiges Argument wird nicht
falsch, wenn die Voraussetzungen nicht mehr erfüllt sind, sondern einfach
nur nicht anwendbar.
Ein besonders krasses Beispiel für völliges Unverständnis
dieser Grundtatsache ist die Conservapedia-Seite {\em Counterexamples
to an Old Earth}\footnote{\url{http://www.conservapedia.com/Counterexamples\_to\_an\_Old\_Earth}}.

Für die Zwecke der Wahrscheinlichkeitsrechnung ist der genaue Ablauf
eines Experimentes gegenstandslos, nur das Resultat interessiert.
Wir führen daher die folgenden Begriffe in:

\begin{definition}
Der Ausgang eines Experimentes heisst {\em Elementarereignis}, die
Menge aller Elementarereignisse wird mit $\Omega$ bezeichnet.
Ein Elementarereignis $\omega$ ist also ein Element von $\Omega$,
$\omega\in\Omega$.
\index{Elementarereignis}
\end{definition}

Die Menge $\Omega$ der Elementarereignisse für jedes der Beispiele weiter
oben ist:
\begin{enumerate}
\item $\Omega=\{\text{Kopf},\text{Zahl},\text{Kante}\}$
\item $\Omega=\{\epsdice{1},
\epsdice{2},
\epsdice{3},
\epsdice{4},
\epsdice{5},
\epsdice{6}\}$
\item $\Omega$ besteht aus allen Paaren von Augenzahlen.
\item $\Omega$ besteht aus allen Paaren von Augenzahlen, in denen die
zweite Zahl mindestens so gross ist wie die erste:
{
\def\e#1#2{\epsdice{#1}\,\epsdice{#2}}
\def\p{\phantom{\epsdice{1}\,\epsdice{1},\,}}
\begin{align*}
\Omega=\{
           &\e{1}{1},\e{1}{2},\e{1}{3},\e{1}{4},\e{1}{5},\e{1}{6},\\
           &\p       \e{2}{2},\e{2}{3},\e{2}{4},\e{2}{5},\e{2}{6},\\
           &\p       \p       \e{3}{3},\e{3}{4},\e{3}{5},\e{3}{6},\\
           &\p       \p       \p       \e{4}{4},\e{4}{5},\e{4}{6},\\
           &\p       \p       \p       \p       \e{5}{5},\e{5}{6},\\
           &\p       \p       \p       \p       \p       \e{6}{6}
\}
\end{align*}
}
\item $\Omega=\{\text{Baum vom Blitz getroffen},\text{Baum verschont}\}$
\item $\Omega=\{\text{an Ebola gestorben},\text{Ebola überlebt}\}$
\item Die Menge der Elementarereignisse ist die Menge aller möglichen
Messwerte, also $\Omega=\mathbb{R}$ oder eine Teilmenge davon.
\end{enumerate}

\section{Ereignisalgebra} \label{section-ereignisse}
Die Modellierung von Versuchsausgängen mit der Menge $\Omega$
allein ist nicht in der Lage, zusätzliche Eigenschaften eines
Versuchsausgangs wiederzugeben.
Sie muss daher um den Begriff des Ereignisses erweitert werden.

\subsection{Ereignisse}
Oft sind die einzelnen Versuchsausgänge nicht von Interesse.
Viele Würfelspiele mit zwei Würfeln haben besondere Regeln, wenn
der Spieler zwei gleiche Augenzahlen wirft, einen sogenannten {\em Pasch}.
Die Regeln sind unabhängig vom Wert, es zählt nur die Tatsache,
dass die beiden Augenzahlen gleich sind.
Das Ereignis ``Pasch'' ist eingetreten, wenn der Versuchsausgang, also das
Paar von Augenzahlen, in der Menge
\[
\def\e#1{\epsdice{#1}\,\epsdice{#1}}
P=\{
\e{1},
\e{2},
\e{3},
\e{4},
\e{5},
\e{6}
\}
\]
liegt.

Auch einzelne Messwerte sind oft nicht interessant.
Ein Überspannungsereignis tritt zum Beispiel ein, wenn die Spannung
einen Wert überschreitet, oder wenn das Elementarereignis, also der Messwert,
in der Menge
\[
U=
\{x\in\mathbb R\,|\, x > x_{\text{Limite}}\}
\]
liegt.

Solche zusammengesetzten Ereignisse sind also immer Teilmengen von $\Omega$.
Wir können dies als eine Definition des Begriffs des Ereignisses
verwenden.

\begin{definition}
Ist $\Omega$ eine Menge von Versuchsausgängen, dann heisst eine Teilmenge
$A\subset\Omega$ ein {\em Ereignis}.
Man sagt, das Ereignis $A$ ist {\em eingetreten}, wenn bei einer Durchführung des
Experimentes ein Versuchsausgang $\omega\in A$ aufgetreten ist.
\end{definition}

\begin{beispiel}
\begin{figure}
\centering
\includegraphics{images/ebola-1.pdf}
\caption{Ereignisse zum Experiment ``Person im Ebola-Gebiet''
\label{image-ebola}}
\end{figure}

In Abbildung~\ref{image-ebola} sind mögliche Ereignisse eines Experiments
dargestellt, bei dem ein zufällig ausgewählte Person daraufhin untersucht
wurde, ob sie mit Ebola in Kontakt kam, daran erkrankte und inzwischen
verstorben ist.
Der einzelne Versuchsausgang ist also die Person, die für die
Untersuchung ausgewählt wurde.
Die Menge $\Omega$ ist die Menge aller Personen.
Folgende Ereignisse wurden untersucht:
\begin{align*}
K&=\{\omega\in\Omega\,|\,\text{$\omega$ ist mit Ebola in Kontakt gekommen}\}
\\
E&=\{\omega\in\Omega\,|\,\text{$\omega$ ist an Ebola erkrankt}\}
\\
T&=\{\omega\in\Omega\,|\,\text{$\omega$ ist verstorben}\}
\end{align*}
Es gilt natürlich $E\subset K$, denn wer an Ebola erkrankt, muss mit
Ebola in Kontakt gekommen sein.
Das umgekehrte gilt nicht, man kann durchaus mit Ebola in Kontakt
kommen, ohne daran zu erkranken.

Es gilt nicht, dass $E\subset T$, denn einerseits gibt es Personen, die
Ebola überlebt haben, und andererseits gibt es an Ebola erkrankte,
die zur Zeit des Experiments noch nicht verstorben sind.
Die Menge $E\cap T$ besteht aus denjenigen Personen, die an Ebola
erkrankt waren und verstorben sind.
Dies heisst aber nicht, dass sie an Ebola gestorben sind.
\end{beispiel}

Zwei Ereignisse $A$ und $B$ können bei der Durchführung des Experimentes
gleichzeitig eintreten.
Der Versuchsausgang $\omega$ ist also so beschaffen, dass mit ihm sowohl
$A$ als auch $B$ eintreten, dass also $\omega\in A$ und $\omega\in B$,
oder $\omega \in A\cap B$.
Gleichzeitiges Eintreten von Ereignis $A$ {\em und} $B$ ist das Ereignis
$A\cap B$.

Tritt ein Ereignis $A$ bei einer Versuchsdurchführung nicht ein, dann trat
ein Versuchsausgang $\omega$ auf, der nicht zu $A$ gehört,
also $\omega\in\Omega\setminus A$.
Dies bedeutet aber, dass das Ereignis $\Omega\setminus A=\overline{A}$
eingetreten ist.
Nichteintreten des Ereignisses $A$ ist das Ereignis
$\overline{A}=\Omega\setminus A$.

Zwei Ereignisse sind speziell.
Die Menge $\Omega\subset\Omega$ hat die Eigenschaft, dass jeder denkbare
Versuchsausgang per Definition in $\Omega$ liegt, das Ereignis $\Omega$
tritt also immer ein.
$\Omega$ heisst daher auch das sichere Ereignis.
\index{Ereignis!sicheres}
Die leere Menge $\emptyset\subset\Omega$ hat genau die gegenteilige
Eigenschaft: was auch immer geschieht, was auch immer für ein 
Elementarereignis $\omega$ realisiert wird, in $\emptyset$ kann es
nicht drin sein, also wird $\emptyset$ nie eintreten.
$\emptyset$ heisst daher auch das unmögliche Ereignis.
\index{Ereignis!unmögliches}

Diese Beispiele zeigen, dass die Modellierung von Ereignissen als Teilmengen
von $\Omega$
mit der umgangssprachlichen Sprechweise von Ereignissen übereinstimmt.
Die Tabelle~\ref{begriffe-zusammenfassung}
fasst die gebräuchlichsten Mengenoperationen und die zugehörige
Sprechweise für Ereignisse zusammen.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Begriff&Modell\\
\hline
Elementarereignis&$\omega$\\
alle Elementarereignisse&$\Omega$\\
Ereignis&$A\subset\Omega$\\
sicheres Ereignis&$\Omega$\\
unmögliches Ereignis&$\emptyset$\\
$A$ und $B$&$A\cap B$\\
$A$ oder $B$&$A\cup B$\\
$A$ hat $B$ zur Folge, $A\Rightarrow B$&$A\subset B$\\
nicht $A$&$\Omega\setminus A$\\
\hline
\end{tabular}
\end{center}
\caption{Begriffe der Wahrscheinlichkeitstheorie und ihre mathematischen
Modellierung\label{begriffe-zusammenfassung}}
\end{table}

\subsection{Beispiele}
\subsubsection{AIDS-Test}
Wenn sich jemand im Bezug auf AIDS riskant verhalten hat, dann ist
eine seiner Sorgen, dass der AIDS-Test nicht sofort das richtige
Resultat anzeigt.
Und selbst wenn er die notwendige Frist abgewartet
hat, hat der Test eine geringe Fehlerrate.
Es können also verschiedene Ereignisse eintreten.
Meistens wird ein
positiver AIDS-Test richtig anzeigen, dass eine Person HIV hat.
Manchmal
wird der Test jedoch positiv sein, obwohl die Person gesund ist
und manchmal wird der Test zwar ein negatives Resultat zeigen,
aber die Person ist an HIV erkrankt.
Glück haben diejenigen, bei denen der Test nicht
anspricht, und die auch tatsächlich gesund sind.


\subsubsection{Euromillions}
\index{Euromillions}
\begin{figure}
\includegraphics[width=\hsize]{graphics/euromillions}
\caption{Besondere Gewinnchance bei Euromillions}
\end{figure}
\begin{figure}
\begin{center}
\includegraphics[height=8cm]{graphics/euromillionsschein}
\end{center}
\caption{Teilnahmeschein für Euromillions\label{euromillionsschein}}
\end{figure}
Am 5.~September 2008 schrieb die Gratiszeitung ``20 Minuten'', dass die
bevorstehende Ziehung der Lotterie Euromillions besondere Gewinnchancen 
ermögliche, weil der Jackpot ungewöhnlich gross sei.
Welche Ereignisse
sind in diesem Spiel relevant?

Auf der Euromillions-Website findet man die Erklärung, wie das Spiel abläuft.
Der Spieler kreuzt auf dem Teilnahmeschein (Abbildung~\ref{euromillionsschein})
fünf Zahlen und zwei Sterne an.
Dann erfolgt die Ziehung, Euromillions ermittelt
5 Gewinnzahlen aus dem Bereich 1--50 und 2 Gewinnsterne im Bereich 1--9.
Jetzt werden die Übereinstimmungen zwischen dem Tipp des Teilnehmers und den
Gewinnzahlen gezählt.
Je besser die Übereinstimmung desto grösser der Gewinn.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Gewinnrang&richtige Zahlen&richtige Sterne&Anteil Gewinnsumme\\
\hline
1&5&2&32.0\%\\
2&5&1&7.4\%\\
3&5&0&2.1\%\\
4&4&2&1.5\%\\
5&4&1&1.0\%\\
6&4&0&0.7\%\\
7&3&2&1.0\%\\
8&3&1&5.1\%\\
9&2&2&4.4\%\\
10&2&0&4.7\%\\
11&1&2&10.1\%\\
12&2&1&24.0\%\\
\hline
\end{tabular}
\end{center}
\caption{Gewinnränge bei Euromillions\label{gewinnraenge}}
\end{table}

Die Tabelle \ref{gewinnraenge} zeigt, wie der Gewinn verteilt wird.
Offenbar spielen dabei sogenannte Gewinnränge eine besondere Rolle.
Bei jedem Tippzettel wird festgestellt, in welchen Gewinnrang er
gehört.

Ausser den Gewinnrängen können auch andere Ereignisse eintreten,
die jedoch für die Auszahlung nicht unbedingt von Bedeutung sind,
oder aus denen sich der Gewinn noch nicht ableiten lässt:
\begin{itemize}
\item {\it Heiris Euromillions Teilnahmeschein fällt in Gewinnrang 10.}
Offenbar teilt sich Heiri 4.7\% der Gewinnsumme mit den anderen Teilnehmern,
die dasselbe Ergebnis erzielt haben.
\item {\it Hanna hatte fünf richtige Zahlen.}
Diese Information reicht noch nicht, um den Gewinn festzulegen.
Hannas Teilnahmeschein fällt in Gewinnrang 1 oder 2 oder 3.
\item {\it Hermine hatte keinen einzigen Stern richtig.}
Hermine könnte
etwas gewonnen haben, nämlich wenn sie 5, 4 oder 2 richtige Zahlen
gehabt hat (Ränge 3, 6 bzw.~10).
Oder sie könnte mit 3, 1 oder 0 richtigen Zahlen nichts gewonnen haben.
\item {\it Hermann hatte richtige Zahlen und Sterne.}
Hermann hatte als 1, 2, 3, 4 oder 5 richtige Zahlen {\em und}
1 oder 2 richtige Sterne.
\item {\it Es trifft nicht zu, dass Hilary einen richtigen Stern hat.}
\item {\it Holger hat auf 47 gesetzt.}
Dieses Ereignis nimmt auf die Ziehung überhaupt keinen Bezug,
trotzdem beschreibt es einen möglichen Ausgang des Experimentes.
\end{itemize}
Wir stellen fest, dass Ereignisse mit {\em und} (beide Ereignisse sind
eingetreten) und {\em oder} (eines der Ereignisse ist
eingetreten) verknüpft werden können.
Ausserdem können Ereignisse negiert werden.

\subsection{Produkte}
\begin{figure}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
(1,1)&(1,2)&(1,3)&(1,4)&(1,5)&(1,6)\\
\hline
(2,1)&(2,2)&(2,3)&(2,4)&(2,5)&(2,6)\\
\hline
(3,1)&(3,2)&(3,3)&(3,4)&(3,5)&(3,6)\\
\hline
(4,1)&(4,2)&(4,3)&(4,4)&(4,5)&(4,6)\\
\hline
(5,1)&(5,2)&(5,3)&(5,4)&(5,5)&(5,6)\\
\hline
(6,1)&(6,2)&(6,3)&(6,4)&(6,5)&(6,6)\\
\hline
\end{tabular}
\end{center}
\caption{Elementarereignisse für das Würfeln mit zwei unterscheidbaren
Würfeln\label{ereignisse-zwei-wuerfel}}
\end{figure}
Wir stellen uns vor, dass ein roter und ein blauer Würfel geworfen werden.
Die von den beiden Würfeln gezeigten Augenzahlen sind verschiedene
Versuchsausgänge, sozusagen ``rote'' und ``blaue'' Zahlen.
Das Resultat
eines Wurfes ist also ein Paar bestehend aus einer ``roten'' und
einer ``blauen'' Augenzahl.
Die Menge aller möglichen Ausgänge
ist also 
\[
\Omega = \Omega_{\text{rot}}\times\Omega_{\text{blau}},
\]
das kartesische Produkt der Mengen $\Omega_{\text{rot}}$ und
$\Omega_{\text{blau}}$ (siehe auch Abbildung \ref{ereignisse-zwei-wuerfel}).

In $\Omega$ lassen sich bereits bedeutend spannendere Ereignisse%
\footnote{\textit{Hinweis zu $A_2$}: $2|x \wedge 2|y$, sprich
	``2 teilt x und 2 teilt y'', ist äquivalent:
	$x \equiv 0 \imod{2} \wedge y \equiv 0 \imod{2}$
} beschreiben:
\begin{align*}
A_1&=\{\text{mindestens eine ungerade Zahl}\}\\
   &=\{(x,y)\in\Omega\;|\;x \equiv 1 \imod{2} \vee y\equiv 1 \imod{2}\},\\
A_2&=\{\text{beide Augenzahlen sind gerade}\}\\
   &=\{(x,y)\in\Omega\;|\;2|x \wedge 2|y\},\\
X_i&=\{(i,y)\in\Omega\},\\
Y_i&=\{(x,i)\in\Omega\},\\
S_s&=\{(x,y)\in\Omega\;|\; x + y = s\},\\
D_d&=\{(x,y)\in\Omega\;|\; |x - y| = d\}.
\end{align*}
Und damit lassen sich auch etwas spannendere Rechnungen durchführen.
Zum Beispiel:
\begin{align*}
A_1&=\Omega \setminus A_2 = \bar A_2,\\
A_1\cap A_2&=\emptyset,\\
A_1&=X_1\cup X_3 \cup X_5\cup Y_1\cup Y_3\cup Y_5,\\
X_4\cap Y_3&=\{(4,3)\},\\
X_5\cap S_7&=\{(5,2)\},\\
X_5\cap D_1&=\{(5,4), (5,6)\}.
\end{align*}
\begin{figure}
\centering
\includegraphics{images/zweiwuerfel-1.pdf}
\caption{Verschiedene Ereignisse in der Ereignisalgebra des Experiments
``Wurf zweier verschiedenfarbiger Würfel\label{zweiwuerfel}''}
\end{figure}

\subsection{Formale Definition}
Damit obige Definition von Ereignissen funktioniert, müssen
alle Operationen in der Tabelle~\ref{begriffe-zusammenfassung}
ausführbar sein.
Die Menge aller Teilmengen von $\Omega$, die Potenzmenge
$P(\Omega)$ erfüllt diese Bedingung,
ist aber sehr gross und hat sehr wenig nützliche Struktur, die
für den Aufbau des Begriffs der Wahrscheinlichkeit im nächsten
Abschnitt benötigt wird.
Insbesondere stellt sich heraus, dass es nicht möglich ist,
den Begriff der Wahrscheinlichkeit konsistent zu definieren, wenn
Ereignisse beliebige Teilmengen einer unendlichen Menge $\Omega$
sein dürfen.

Ein Beispiel für diese Situation sind Messwerte, in diesem
Fall ist $\Omega=\mathbb R$ unendlich.
Es werden aber auch nicht alle Teilmengen von $\mathbb R$ benötigt.
Es wird genügen, wenn wir alle Intervalle zur Verfügung haben sowie
alle Mengen, die sich daraus durch endlich viele Mengenoperationen
konstruieren lassen.
Statt der Menge $P(\Omega)$ wird also eine kleinere Menge $\cal A$
von Ereignissen verwendet.
Man kann zeigen, dass sich aus der Menge der Intervalle immer 
eine Menge von Ereignissen konstruieren lässt, in der alle
nötigen Operationen ausgeführt werden können, und auf der sich
die später einzuführende Wahrscheinlichkeit konsistent definieren
lässt.

Die Einschränkung auf $\cal A$ hat mindestens im Rahmen dieser
Vorlesung keine praktischen Konsequenzen, alle interessierenden
Ereignisse sind von vornherein in $\cal A$, und damit auch alle daraus
abgeleiteten Ereignisse.

Wir nennen eine solche Menge von Ereignissen eine Ereignisalgebra:

\begin{definition}
\label{def-ereignisalgebra}
Eine {\em Ereignisalgebra} $(\Omega,{\cal A})$ ist
eine Menge $\Omega$ mit einer Menge ${\cal A }\subset{\cal P}(\Omega)$
von Teilmengen von $\Omega$, die folgende Bedingungen erfüllen:
\begin{enumerate}
\item Vereinigungen von Elementen von ${\cal A}$ sind ebenfalls in ${\cal A}$,
also
\[
A,B\in {\cal A}\Rightarrow A\cup B\in{\cal A}
\]
\item Differenzen von Elementen von ${\cal A}$ sind in ${\cal A}$, also
\[
A,B\in {\cal A}\Rightarrow A\setminus B\in{\cal A}
\]
\item $\Omega\in{\cal A}$, d.~h.~es gibt das sichere Ereignis.
\end{enumerate}
\end{definition}

Sind nur die Bedingungen 1 und 2 erfüllt, spricht man auch von einem
Mengen-Ring.
Eine Ereignisalgebra heisst manchmal auch ein Mengenkörper.

Aus den Axiomen für die Ereignisalgebra lassen sich sofort erste
Schlussfolgerungen ziehen:
\begin{enumerate}
\item Es gibt auch das unmögliche Ereignis: $\emptyset = \Omega\setminus\Omega\in{\cal A}$.
\item Das Komplement eines Ereignisses ist ebenfalls ein Ereignis: $\bar A=\Omega\setminus A\in{\cal A}$.
\item Der Durchschnitt zweier Ereignisse ist ebenfalls ein Ereignis: $A\cap B = 
(A\cup B) \setminus ((A\setminus B) \cup (B\setminus A))\in{\cal A}$.
\end{enumerate}

\section{Weitere Beispiele von Ereignisalgebren} \label{section-beispiele}
\subsection{Dominosteine}
Die Menge aller möglichen Dominosteine wurde bereits früher untersucht.
Jetzt möchten wir darin einzelne Ereignisse auszeichnen.
Wir beschreiben
einen einzelnen Dominostein als ein paar $(x,y)$, wobei $x\ge y$ sein soll.
Beispiele von Ereignissen:
\begin{align*}
S_k&=\{ \text{Die Augensumme ist $k$}\}\\
S_5&=\{ (5,0), (4,1), (3,2) \}\\
S_4&=\{ (4,0), (3,1), (2,2) \}\\
R&=\{\text{die Augenzahlen sind positiv haben keinen gemeinsamen Teiler $>1$}\}\\
 &=\{ (6,5), (6,1), (5,4), (5,3), (5,2), (5,1), (4,3), (4,1), (3,2), (3,1), (2,1), (1,1) \}
\\
P&=\{\text{beide Augenzahlen sind Primzahlen}\}\\
 &=\{(5,5), (5,3), (5,2), (3,3), (3,2), (2,2) \}\\
\end{align*}

\subsection{Würfeln mit zwei Würfeln}
Bei einem Würfelspiel wirft man jeweils zwei Würfel.
Zeigen beide Würfel
die gleiche Zahl, man nennt dies einen {\em Pasch}, darf man genau ein
weiteres Mal würfeln.
Die Elementarereignisse sind also entweder einfach Paare, also von
der Form $(x,y)$, mit $x\ne y$, oder ein Pasch gefolgt von irgend einem
Würfelresultat, wir schreiben dies $(P_k, (x,y))$, also ein $k$-er-Pasch
gefolgt von einem Paar $(x,y)$, wobei diesmal keine Einschränkungen für
$x$ und $y$ gelten.
Damit kann man jetzt alle möglichen Elementarereignisse auflisten
\begin{align*}
\Omega=\{
&\phantom{(6,6),} (6,5), (6,4), (6,3), (6,2), (6,1)\\
&(5,6), \phantom{(5,5),} (5,4), (5,3), (5,2), (5,1)\\
&(4,6), (4,5), \phantom{(4,4),} (4,3), (4,2), (4,1)\\
&(3,6), (3,5), (3,4), \phantom{(3,3),} (3,2), (3,1)\\
&(2,6), (2,5), (2,4), (2,3), \phantom{(2,2),} (2,1)\\
&(1,6), (1,5), (1,4), (1,3), (1,2), \phantom{(1,1)}
\}
\\
&\cup
\{(P_6,(x,y))\}
\cup
\{(P_5,(x,y))\}
\cup
\{(P_4,(x,y))\}
\\
&\cup
\{(P_3,(x,y))\}
\cup
\{(P_2,(x,y))\}
\cup
\{(P_1,(x,y))\}.
\end{align*}
Man sieht daraus zum Beispiel, dass es $30 + 6\cdot 36=246$
Elementarereignisse gibt.
Darin enthalten sind die Ereignisse
\begin{align*}
P&=\{\text{Pasch im ersten Wurf}\}\\
&=
\{(P_6,(x,y))\}
\cup
\{(P_5,(x,y))\}
\cup
\{(P_4,(x,y))\}
\\
&\qquad \cup
\{(P_3,(x,y))\}
\cup
\{(P_2,(x,y))\}
\cup
\{(P_1,(x,y))\}
\\
\tilde P_k&=\{\text{$k$-er Pasch im ersten Wurf}\}\\
   &=\{(P_k,(x,y))\}
\\
Q&=\{\text{totale Augensumme $\ge 10$}\}\\
&=\{\phantom{(6,6),} (6,5), (6,4), \\
&\phantom{\;=\{}(5,6), \phantom{(5,5), (5,4)} \\
&\phantom{\;=\{}(4,6)\phantom{, (4,5), (4,4)}
\}
\\
&\qquad\cup
\{(P_6,(x,y))\}
\cup
\{(P_5,(x,y))\}
\cup
\{(P_4,(x,y))\,|\, x+y \ge 2\}
\\
&\qquad\cup
\{(P_3,(x,y))\,|\, x+y \ge 4\}
\cup
\{(P_2,(x,y))\,|\, x+y \ge 6\}
\\
&\qquad
\cup
\{(P_1,(x,y))\,|\, x+y \ge 8\}.
\end{align*}

\subsection{AIDS-Test}
Wir mathematisieren das Beispiel des AIDS-Tests.
Die frühere Diskussion führt uns vor Augen,
dass wir hier mit zwei verschiedenen Ereignissen zu tun haben. 
Das Experiment besteht offenbar darin, dass wir jemanden untersuchen,
die Menge der Elementarereignisse ist also die Menge aller betrachteten
Personen.
Darin unterscheiden wir:
\begin{align*}
H&=\{\text{Person hat HIV}\},\\
T&=\{\text{Person hat positiven AIDS-Test}\}.
\end{align*}
Es ist klar, dass $H\ne T$.
Es gibt Personen, die zwar HIV haben, bei
denen der Test dies aber noch nicht zeigen kann, $H\setminus T\ne \emptyset$.
Andererseits gibt es falsche positive Testresultate,
$T\setminus H\ne\emptyset$. 

\subsection{Messwertalgebra}
\begin{table}
\begin{center}
\begin{tabular}{|cc|cc|}
\hline
1&0.990
&11&0.990\\
2&0.989
&12&0.989\\
3&0.991
&13&0.990\\
4&0.991
&14&0.991\\
5&0.991
&15&0.989\\
6&0.989
&16&0.990\\
7&0.990
&17&0.989\\
8&0.989
&18&0.990\\
9&0.992
&19&0.990\\
10&0.992
&20&0.991\\
\hline
\end{tabular}
\end{center}
\caption{Werte von Stichprobe von 20 1k$\Omega$-Widerständen\label{widerstandswerte}}
\end{table}
Bei einer Messung wird ein Messwert ermittelt, dieser kann jedoch
nur mit einer begrenzten Genauigkeit festgehalten werden.
Die Elementarereignisse sind zwar immer noch alle möglichen reellen
Zahlen $\mathbb R$, aber sinnvolle Ereignisse sind nur bestimmte Teilmengen
$A\subset\mathbb R$.
Wir wollen bestimmen, welche Teilmengen das sind.

Zunächst führen wir dies an einem Beispiel durch.
Bei einer am 1.~November 2006
zufällig bei Pusterla in Zürich gekauften Stichprobe von 20 1k$\Omega$
Widerständen ergaben sich beim Nachmessen die Widerstandswerte in 
Tabelle \ref{widerstandswerte}.
Offensichtlich streuen die Messwerte
zwischen 0.989 und 0.992.
Genauere Informationen kann das Messgerät nicht
anzeigen.
Wir können aus den Daten also eigentlich nur folgendes schliessen:
der Widerstand mit der Nummer eins hat einen Wert $0.990\le R_1<0.991$.
Der Widerstandswert $R_1$ ist ein Versuchsergebnis, also ein Elementarereignis,
liegt aber in der Menge
\[
A=[0.990,0.991)\subset \mathbb{R}=\Omega.
\]
Ausserdem werden durch die weiteren Messungen auch noch folgende
Ereignisse realisiert:
\begin{center}
\begin{tabular}{|c|c|}
\hline
Ereignis&Häufigkeit\\
\hline
$[0.989,0.990)$&6\\
$[0.990,0.991)$&7\\
$[0.991,0.992)$&5\\
$[0.992,0.993)$&2\\
\hline
\end{tabular}
\end{center}
Diese speziellen Ereignisse beinhalten offensichtlich alle Information,
die wir über die Widerstände haben können.
Bei den 20 Versuchen treten aber auch noch andere Ereignisse ein,
zum Beispiel
\begin{center}
\begin{tabular}{|c|c|}
\hline
Ereignis&Häufigkeit\\
\hline
$\{R\ge 0.990\}$&14\\
``5\% Toleranz'' = $\{0.95\le R\le1.05\}$&20\\
``1\% Toleranz'' = $\{0.99\le R\le 1.01\}$&14\\
\hline
\end{tabular}
\end{center}

Etwas formaler: Eine Genauigkeitsangabe bei einem Messwert definiert
ein Intervall, in dem sich ein Messwert befinden muss.
Wir
fordern also, dass $\cal A$ alle Intervalle der Form $[a,b]\subset\mathbb R$
enthalten muss.
Aber auch die Aussage ``der Messwert ist kleiner als $a$''
muss einem Ereignis in $\cal A$ entsprechen, also müssen auch Intervalle,
die sich ins unendliche erstrecken in $\cal A$ sein:
\begin{align*}{}
(-\infty,r]&=\{x\in\mathbb R\;|\;x \le r\} \in \cal A\\
% Hack: \; brauchts, damit eqnarray nicht meint [ leite ein Argument ein
\;[r,\infty)&=\{x\in\mathbb R\;|\;x \ge r\} \in \cal A
\end{align*}

Da wir Komplemente bilden können, müssen auch die offenen Intervalle
\begin{align*}
\overline{[a,\infty)}&=\mathbb R\setminus [a,\infty)=(-\infty,a)\in\cal A\\
\overline{(-\infty,b]}&=\mathbb R\setminus (-\infty,b]= (b,\infty)\in\cal A
\end{align*}
Ereignisse sein.
Ein beliebiges offenes Intervall lässt sich
als Durchschnitt zweier solcher Intervalle ausdrücken:
\[
(a,b)=(-\infty, b)\cap(a,\infty).
\]
Durch Bildung von geeigneten Durchschnitten lassen sich also alle
Intervalle bilden.
Ausserdem müssen alle Teilmengen von $\mathbb R$
zu $\cal A$ hinzugenommen werden, die sich durch Bildung von Komplementen,
Durchschnitten und Vereinigungen bilden lassen.
Offensichtlich ist $\cal A$
in diesem Falle sehr kompliziert.

\section{Wahrscheinlichkeit} \label{section-wahrscheinlichkeit}
In der Einleitung haben wir das Ereignis diskutiert, dass die Schweizer
Nati ein bestimmtes Spiel an Fussball WM gewinnt.
Wir sind zum Schluss
gekommen dass wir nicht wissen können, was bei der konkreten
Durchführung des Experimentes passieren wird.
Falls der Gegner
stark ist, zum Beispiel Deutschland, dann wird ein Sieg wohl auch
dann kaum je eintreten, wenn man das Spiel viele Male wiederholt.
Das wenigstens verbinden wir mit einem unwahrscheinlichen Ereignis.
Trotzdem kann das Ereignis eintreten.

\subsection{Verschiedene Wahrscheinlichkeitsbegriffe}
Die Wahrscheinlichkeit soll ein Mass dafür sein, dass ein Ereignis
eintritt.
Es gibt verschiedene Ansätze, wie wir zu einem solchen Mass kommen
könnten:
\begin{itemize}
\item 
Je häufiger ein Ereignis eintritt, desto grösser sollte die
Wahrscheinlichkeit sein.
Dies setzt voraus, dass das Experiment im Prinzip beliebig oft wiederholbar 
ist.
Man nennt dies den {\em frequentistischen} Ansatz.
\item
Die Wahrscheinlichkeit ist ein Mass für die persönliche
Überzeugung, dass ein Ereignis eintreten wird.
Im Unterschied zum frequentistischen Ansatz sollte man bei diesem
Ansatz einem Ereignis auch dann eine Wahrscheinlichkeit geben können,
wenn sich ein Experiment nicht wirklich wiederholen lässt.
Dies ist der Bayessche Ansatz.
\item
Wir könnten eine Reihe von plausiblen Axiomen postulieren, nach denen sich
die Wahrscheinlichkeit zu verhalten hat, und dann zu untersuchen,
ob ein solches Objekt tatsächlich existiert.
Dabei ist uns egal, was der Wahrscheinlichkeitswert genau bedeutet.
Dieser axiomatische Ansatz hat den Vorteil, logisch konsistent zu sein,
was bei den anderen Ansätzen nicht von vornherein garantiert ist.
\end{itemize}
In allen Fällen ergibt sich eine Reihe von Gesetzmässigkeiten
oder Formeln, welche die jeweiligen Wahrscheinlichkeitsbegriffe 
erfüllen müssen.
Soll die Wahrscheinlichkeit eine objektive Grösse sein, dann
muss für wiederholbare Experimente die für den Bayesschen
Ansatz nötige persönliche Überzeugung direkt mit der Häufigkeit
des Eintretens zusammenhängen.
Der frequentistische und der Bayessche Ansatz werden also in diesem
Fall übereinstimmen.
Die Axiome im axiomatischen Ansatz sind natürlich genau die
Rechenregeln, die man sowohl im frequentistischen Ansatz wie
auch im Bayesschen Ansatz von der Wahrscheinlichkeit erwartet.
Man darf daher davon ausgehen, dass die drei Ansätze die gleichen numerischen
Resultate liefern, sie unterscheiden sich höchstens in der Interpretation
der Resultate.
Wir werden daher im Folgenden vor allem den am leichtesten zu
verstehenden frequentistischen Ansatz verfolgen.

\subsection{Wahrscheinlichkeit als relative Häufigkeit}
Wir wollen nun einen Begriff der Wahrscheinlichkeit einführen, der
einer intuitiven Vorstellung zu diesem Wort möglichst nahe kommt.
Wenn ``sehr
wahrscheinlich'' heissen soll ``in der Mehrzahl der Fälle'', dann
bedeutet das offensichtlich, dass man eine grössere Zahl von
Experimenten durchführen soll, und dann die Fälle zählen soll,
in denen das Ereignis tatsächlich eingetreten ist.
Deren Anteil
an der Gesamtzahl heisst {\em relative Häufigkeit} und ist eine erste,
etwas präzisere Fassung des Begriffs der Wahrscheinlichkeit.

Grundlegend für die Wahrscheinlichkeitsrechnung ist die aus der
\index{Wahrscheinlichkeit!als relative Häufigkeit}
Erfahrung mit einer grossen Zahl von Versuchen gewonnene Zuversicht,
dass die relative Häufigkeit mit wachsender Anzahl Versuche gegen
eine Grösse $P(A)$ strebt.
Man verwendet diese Annahme daher als Definition.
Wiederholt man ein Experiment $N$ Male, und tritt dabei das Ereignis $A$
genau $n$ mal ein, dann erwartete man, dass der Quotient $\frac{n}{N}$
eine gute Näherung für $P(A)$ ist.
Je öfter man das Experiment
wiederholt, desto näher sollte der Quotient der Grösse $P(A)$
kommen, also
\[
P(A) := \lim_{N\to\infty}\frac{n}{N}.
\]

Leider ist es nur selten praktikabel, das Experiment tatsächlich
sehr häufig zu wiederholen.
Bei der Prüfung neuer Medikamente
zum Beispiel hindern moralische Bedenken daran, eine grosse
Zahl von Experimenten mit Menschen durchzuführen.
Bei der
Ausbruchswahrscheinlichkeit eines Vulkans oder der
Einschlagwahrscheinlichkeit eines Asteroiden auf der Erde hat man
ganz einfach keine Kontrolle über das Experiment.

Das Ziel der Wahrscheinlichkeitstheorie ist daher, 
die Abbildung $P\colon {\cal A}\to\mathbb{R}$
abstrakt zu definieren und geeignete Axiome zu postulieren, die
der eben skizzierten intuitiven Vorstellung der Wahrscheinlichkeit
einen strengen Sinn geben.
Damit wird die Funktion $P$ in vielen
Fällen berechenbar, meist natürlich unter zusätzlichen Annahmen.
Die grundlegenden Eigenschaften sind in den folgenden, für
relative Häufigkeiten, unmittelbar einleuchtenden Axiomen
% XXX
von Kolmogorov\footnote{Andrei Nikolaiewitsch Kolmogorov, 1903-1987}
festgehalten.
\index{Kolmogorov, Andrei Nikolaiewitsch}

\subsubsection{Axiome  eines Wahrscheinlichkeitsraumes}
\index{Axiome eines Wahrscheinlichkeitsraumes}

\begin{description}
\item[Wertebereich.]Für jedes beliebige Ereignis $A\subset \Omega$
gilt
\begin{equation}
0\le P(A)\le 1.
\label{p-wertebereich}
\end{equation}
\item[Sicheres Ereignis.] Für das sichere Ereignis gilt
\begin{equation}
P(\Omega) = 1.
\label{p-sicheresereignis}
\end{equation}
\item[Vereinigung.] Sind die Ereignisse $A_1,A_2,\dots$ paarweise
disjunkt, also $A_i\cap A_j=\emptyset$ für $i\ne j$, dann gilt
\begin{equation}
P(A_1\cup A_2\cup \dots) = P(A_1) + P(A_2) + \dots
\label{p-summenformel}
\end{equation}
\end{description}

Die Forderung über die Vereinigung kann natürlich nur dann überhaupt
formuliert werden, wenn die Vereinigung auch wirklich ein Ereignis ist, also in
$\cal A$ ist.
Bisher wissen wir nur, dass endliche
Vereinigungen von Ereignissen gebildet werden können, jetzt müssen
wir fordern, dass
auch unendliche Vereinigungen von Ereignissen wieder Ereignisse sind.
D.~h.~wir setzen im Folgenden voraus, dass in einer
Ereignisalgebra in Erweiterung des ersten Axioms auch gilt:

\begin{definition}
$\cal A$ heisst 
$\sigma$-Algebra, wenn 
abzählbare Vereinigungen von Elementen von $\cal A$ ebenfalls
in $\cal A$ sind, d.~h.~falls $A_i\in{\cal A}$ für $i\in\mathbb{N}$, dann ist
\[
A_1\cup A_2\cup\dots=\bigcup_{i=0}^{\infty}A_i \in{\cal A}.
\]
\end{definition}

In allen praktischen Fällen ist diese technische Bedingung automatisch
erfüllt, oder man kann erzwingen, dass sie erfüllt ist.
Für die Praxis ist dies also keine Einschränkung, man kann immer davon
ausgehen, dass die Vereinigungsformel~(\ref{p-summenformel}) ``funktioniert''.

\subsection{Beispiel: Würfeln}
Beim Würfeln mit einem Würfel erwartet man, dass jeder mögliche
Ausgang etwa gleich häufig sein wird.
Bei einer grossen Zahl von
Versuchen wird man also feststellen, dass die relative Häufigkeit
des Ereignisses, dass eine $4$ gewürfelt wird, gegen $\frac16$
konvergieren wird.
Die zusätzliche Annahme steckt in diesem Fall
darin, dass man alle Ausgänge als gleich wahrscheinlich annimmt.

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|}
\hline
Anzahl&6&96&996&9996&99996&999996&9999996&99999996\\
\hline
0& 0& 17& 168& 1642& 16746& 166434& 1665609& 16662248\\
1& 1& 17& 160& 1692& 16747& 166584& 1666841& 16669513\\
2& 1& 11& 178& 1680& 16491& 167031& 1667271& 16663991\\
3& 0& 16& 158& 1660& 16672& 165940& 1665310& 16659406\\
4& 3& 13& 159& 1634& 16756& 167026& 1668244& 16672749\\
5& 1& 22& 173& 1688& 16584& 166981& 1666721& 16672089\\
\hline
Maximum& 3& 22& 178& 1692& 16756& 167031& 1668244& 16672749\\
Minimum& 0& 11& 158& 1634& 16491& 165940& 1665310& 16659406\\
$\Delta$& 3& 11& 20& 58& 265& 1091& 2934& 13343\\
\hline
\end{tabular}
\end{center}
\caption{Computer-Simulation eines fairen Würfels\label{wuerfel-simulation}}
\end{table}

Die tatsächliche Durchführung von ``genügend'' vielen Experimenten
kann hingegen schwierig sein.
Eine Computer-Simulation des
Würfel-Experiments zeigt zum Beispiel die Resultate in Tabelle
\ref{wuerfel-simulation}.
Offensichtlich ist selbst mit $10^8$
Würfen die Wahrscheinlichkeit erst auf drei Stellen nach dem Komma
genau.

\subsection{Folgerungen aus den Axiomen}
\begin{satz}Es gilt
\begin{enumerate}
\item Die Wahrscheinlichkeit des unmöglichen Ereignisses ist
\begin{equation}
P(\emptyset) = 0.
\label{p-emptyset}
\end{equation}
\item Die Wahrscheinlichkeit des komplementären Ereignisses
ist
\begin{equation}
P(\bar A) = P(\Omega\setminus A) = 1 -P(A).
\label{p-negation}
\end{equation}
\item Die Wahrscheinlichkeit der Differenz der Ereignisse $A$ und $B$
ist
\begin{equation}
P(A\setminus B) = P(A) - P(A\cap B)
\label{p-complement} % changed
\end{equation}
\item Die Wahrscheinlichkeit der Vereinigung zweier beliebiger Ereignisse
ist
\begin{equation}
P(A\cup B) = P(A) + P(B) - P(A\cap B)
\label{p-union}
\end{equation}
(Ein-/Ausschaltformel).
\index{Ein-/Ausschaltformel}
\end{enumerate}
\end{satz}

\begin{proof}[Beweis]
Wegen $\Omega = \Omega \cup\emptyset$ und
$\Omega\cap\emptyset = \emptyset$ folgt aus dem Axiom
über die Vereinigung  (\ref{p-summenformel})
\[
P(\Omega) = P(\Omega \cup \emptyset) = P(\Omega) + P(\emptyset)
\]
Indem man auf beiden Seiten der Gleichung $P(\Omega)$ subtrahiert,
folgt $0 = P(\emptyset)$.

Für das komplementäre Ereignis gilt $A\cap\bar A=\emptyset$ und
$A\cup\bar A=\Omega$.
Aus der Summenformel (\ref{p-summenformel}) folgt
\[
1 = P(\Omega) = P(A\cup\bar A) = P(A) + P(\bar A)
\]
oder
\[
P(\bar A) = 1 - P(A).
\]

$A$ ist die disjunkte Vereinigung $A=(A\setminus B) \cup (A\cap B)$,
also gilt
\[
P(A)=P(A\setminus B) + P(A\cap B),
\]
oder, nach $P(A\setminus B)$ aufgelöst
\[
P(A\setminus B) = P(A) - P(A\cap B).
\]

Nach der de Morganschen Regel lässt sich die Vereinigungsmenge
als disjunkte Vereinigung schreiben:
\[
A\cup B =  (A\setminus B) \cup (A\cap B) \cup (B\setminus A)
\]
Also gilt
\begin{align*}
P(A\cup B)&=P(A\setminus B) + P(A\cap B) + P(B\setminus A)\\
&=P(A) - P(A\cap B) + P(A\cap B) + P(B) - P(A\cap B)\\
&=P(A) + P(B) - P(A\cap B).
\qedhere
\end{align*}
\end{proof}
Damit stehen uns Rechenregeln zur Verfügung, mit deren Hilfe wir
die Wahrscheinlichkeit beliebiger Ereignisse auf der Basis einiger
weniger Annahmen berechnen können.

\subsection{Wahrscheinlichkeit als Mass}
Die Wahrscheinlichkeitsfunktion $P$ hat genau die Eigenschaften,
die man von einer Flächenmessung erwartet.
Die Gesamtfläche disjunkter Flächenstücke berechnet man,
indem man den Flächeninhalt der einzelnen Flächenstücke summiert.
Das leere Flächenstück hat keinen Inhalt.
Einzig die zusätzliche Forderung $P(\Omega)=1$, welche
sicherstellt, dass die Wahrscheinlichkeit nicht gross werden kann,
hat keine Entsprechung.

Demzufolge können Ereignisse oft auch als Flächen in einem Diagramm
visualisiert werden,
die umso grösser sind, je grösser die Wahrscheinlichkeit des Ereignisses
ist.
Bei einem Dart-Spiel liegt es nahe, dass die Wahrscheinlichkeit, ein
bestimmtes Feld zu treffen, umso grösser ist, je grösser der Flächen-Anteil
dieses Feldes an der ganzen Scheibe ist.
Natürlich trifft dies nur bei einem
sehr schlechten Dart-Spieler zu, dessen Pfeile gleichmässig verteilt auf
die Scheibe treffen.

Diese
Überlegung kann jedoch dazu verwendet werden, die Wahrscheinlichkeiten
eines Meteoriteneinschlages in der Schweiz und im Fürstentum Liechtenstein
miteinander zu vergleichen.
Wir erwarten, dass die Wahrscheinlichkeit
proportional zur Fläche sein wird, also
\[
\frac{P(\text{CH})}{P(\text{FL})}=
\frac{41285\text{km$\mathstrut^2$}}{160\text{km$\mathstrut^2$}}
=258.03,
\]
die Wahrscheinlichkeit für einen Meteoriteneinschlag im Ländle ist also
über 258mal kleiner.

Umgekehrt könnte man die Idee zu einer Flächenberechnungs- oder
Integra\-tions\-methode ausbauen.
Um den Flächeninhalt einer unregelmässigen
Teilmenge
eines Rechteckes in der Ebene zu bestimmen, simuliert man mit dem
Computer gleichmässig verteilte ``Schüsse'' auf diese ``Zielscheibe''.
Der Anteil der Treffer in der Teilmenge ergibt ein Mass für dessen
Fläche.
Leider ist das Verfahren in dieser Form praktisch nicht
durchführbar, weil viel zu viele ``Schüsse'' notwendig sind, um
eine genügende Genauigkeit zu erreichen.
Es kann aber durchaus
zu einem praktikablen Verfahren verfeinert werden (Monte Carlo Methoden).

Wegen
der Analogie zu einer Flächenmessung heisst eine
Wahrscheinlichkeitsfunktion $P$ oft auch ein {\em Wahrscheinlichkeitsmass}.
Das Tripel $(\Omega,{\cal A}, P)$ heisst ein {\em Wahrscheinlichkeitsraum}.

\section{Laplace-Experimente} \label{section-laplace-ereignisse}
Bisher sind wir nicht in der Lage, die Wahrscheinlichkeit eines Ereignisses
zu berechnen, wir können nur mit Hilfe der Axiome aus bereits bekannten
Wahrscheinlichkeiten neue Wahrscheinlichkeiten berechnen.
Für die Berechnung der Wahrscheinlichkeiten ist zusätzliche
Information nötig.

Bei Experimenten mit endlich vielen Ausgängen ist es manchmal gerechtfertigt
anzunehmen, dass alle Versuchsausgänge gleich häufig und damit gleich
wahrscheinlich sind.
Von einem guten Würfel erwarten wir, dass er alle Seiten gleich häufig
zeigt, eine symmetrische Münze sollte Kopf und Zahl etwa gleich häufig
zeigen.
Unter dieser Annahme können wir die Wahrscheinlichkeit berechnen:

\begin{definition}
Ein Experiment mit $n=|\Omega|$ Ausgängen heisst ein {\em Laplace-Experiment},
wenn jeder Versuchsausgang gleich wahrscheinlich mit Wahrscheinlichkeit
\[
P(\{\omega\})=\frac1n,\qquad\omega\in\Omega
\]
ist.
\end{definition}

Da Laplace-Experimente nur endliche viele Ausgänge haben, kann man aus
den Axiomen auch jede andere Wahrscheinlichkeit berechnen:

\begin{satz}
Bei einem Laplace-Experiment tritt das Ereignis $A\subset\Omega$ mit
Wahrscheinlichkeit
\[
P(A)=\frac{|A|}{|\Omega|}
\]
ein.
\end{satz}
Damit ist die Berechnung der Wahrscheinlichkeit auf das Zählen der 
Versuchsausgänge in $A$ reduziert, also auf eine Kombinatorik-Aufgabe.

\subsection{Münze werfen}
\index{Munze@{Münze, faire}}
Wirft man eine Münze, kann sie nur auf einer von zwei Seiten landen,
die Elementarereignisse sind also $\Omega = \{\text{Kopf}, \text{Zahl}\}$.
Die mathematisch idealisierte Münze ist so dünn, dass sie nicht auf
der Kante stehen kann.
Nach allgemeiner Erfahrung funktioniert für
solche Münzen der Ansatz von Laplace, d.~h.~$P(\text{Kopf}) = \frac12$
und $P(\text{Zahl})=\frac12$.

Selbstverständlich gibt es auch Münzen, für die die Laplace-Annahme
nicht funktioniert, gebogene Münzen fallen zum Beispiel bevorzugt auf die
nach aussen gewölbte Fläche.

\subsection{Würfeln}
\index{Wurfel@{Würfel, fairer}}
Würfel haben sechs Seiten, die gemäss der Annahme eines Laplace-Experiments
mit gleicher
Wahrscheinlichkeit oben liegen werden, die Wahrscheinlichkeit jedes
Resultates ist also gleich $P(i) = \frac16, i\in\{1,2,3,4,5,6\}$.
Wird eine Seite des Würfels beschwert, fällt er bevorzugt auf diese
Seite, und die Wahrscheinlichkeiten sind nicht mehr gleich.

\begin{figure}
\begin{center}
\includegraphics[width=0.8\hsize]{graphics/Wuerfel5}
\end{center}
\caption{Verschiedene Spielwürfel\label{bild-spielwuerfel}}
\end{figure}
Man kann auch aus anderen geometrischen Körpern ``Würfel'' bauen, die
eine grössere Zahl von Ausgängen ermöglichen.
Ein Dodekaeder hat
zwölf Seiten, ein Ikosaeder sogar deren 20, aber auch andere Formen
sind realisierbar, siehe Abbildung~\ref{bild-spielwuerfel}.
Der Einfachheit
halber gehen wir im Folgenden immer von sechsseitigen Würfeln aus.

Das Werfen eines solchen Würfels erzeugt die folgenden Elementarereignisse
oder Versuchsergebnisse:
\[
\Omega=\{1,2,3,4,5,6\}.
\]
Nach dem Laplace'schen Ansatz wird jedem Elementarereignis die
Wahrscheinlichkeit $\frac16$ zugeschrieben.
Damit lassen sich
Wahrscheinlichkeiten berechnen:
\begin{align*}
P(\text{``gerade Zahl''})=P(\{2,4,6\})&=\frac36=0.5\\
P(\text{``durch 3 teilbar''})=P(\{3,6\})&=\frac26=0.333\\
P(< 7)=P(\Omega)&=\frac66=1
\end{align*}

\subsection{Zwei Würfel}
In verschiedenen Spielen wird mit zwei Würfeln gespielt, wobei nur
deren Augensumme interessiert.
Die Ereignisalgebra dazu wurde bereits
früher vorgestellt, die Elementarereignisse sind Paare $(i,j)$ aus
den Augenzahlen der beiden Würfel.
Die Menge der Elementarereignisse
ist also
% hier brauchen wir wirklich ein eqnarray, wegen dem Spacing
\begin{eqnarray*}
\Omega=&\{&\\
&&(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),\\
&&(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),\\
&&(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),\\
&&(4,1),(4,2),(4,3),(4,4),(4,5),(4,6),\\
&&(5,1),(5,2),(5,3),(5,4),(5,5),(5,6),\\
&&(6,1),(6,2),(6,3),(6,4),(6,5),(6,6),\\
&\},&
\end{eqnarray*}
und $|\Omega|=36$.
Wenn nur die Augensumme interessiert, dann will
man offensichtlich die Ereignisse
\begin{align*}
A_2&=\{\Sigma=2\}=\{(1,1)\}\\
A_3&=\{\Sigma=3\}=\{(1,2),(2,1)\}\\
A_4&=\{\Sigma=4\}=\{(1,3),(2,2),(3,1)\}\\
&\vdots\\
A_{12}&=\{\Sigma=12\}=\{(6,6)\}
\end{align*}
untersuchen.
Diese Ereignisse sind einfach auszuzählen, und damit kann man auch
die Wahrscheinlichkeiten bestimmen:
\begin{align*}
P(A_2)=P(A_{12})&=\frac{1}{36},\\
P(A_3)=P(A_{11})&=\frac{2}{36},\\
P(A_4)=P(A_{11})&=\frac{3}{36},\\
P(A_5)=P(A_9)&=\frac{4}{36},\\
P(A_6)=P(A_8)&=\frac{5}{36},\\
P(A_7)&=\frac{6}{36}.
\end{align*}

\subsection{Geburtstagsproblem}
Wie gross ist die Wahrscheinlichkeit, dass unter $n$ Personen zwei
am gleichen Tag Geburtstag haben? Hierbei nimmt man an, dass
jeder Tag im Jahr etwa gleich häufig als Geburtstag
vorkommt\footnote{Dies ist
jedoch nicht ganz richtig, wie Gebärabteilungen in Spitälern
bestätigen können.
Auch haben grössere Stromausfälle häufig
einen Geburtenzuwachs ungefähr neun Monate später zur Folge.}.
Ein Elementarereignis muss also jeder der $n$ Personen ein
Geburtsdatum aus den $366$ möglichen zuweisen.
Dies kann man so
formulieren:
\[
\omega=(d_1, d_2, d_3,\dots,d_n)
\]
Dabei ist $d_1$ das Geburtsdatum der ersten Person, $d_2$ das
Geburtsdatum der zweiten Person, etc.
Der Einfachheit halber kann
man für die $d_i$ einfach Zahlen zwischen 1 und 366 nehmen.

$\Omega$ besteht also aus allen möglichen Auswahlen von Geburtstagen
für die $n$ Personen:
% hier brauchen wir wirklich ein eqnarray, wegen dem Spacing
\begin{eqnarray*}
\Omega=&\{&\\
&&(1,1,1,\dots,1),\\
&&(2,1,1,\dots,1),\\
&&(3,1,1,\dots,1),\\
&&\dots\\
&&(365,1,1,\dots,1),\\
&&(366,1,1,\dots,1),\\
&&(1,2,1,\dots,1),\\
&&(2,2,1,\dots,1),\\
&&\dots\\
&&(366,366,366,\dots,366)\\
&\}&
\end{eqnarray*}
Man kann ablesen, dass $|\Omega|=366^n$ ist.

Gefragt ist nun das Ereignis
``zwei Personen haben am gleichen Tag Geburtstag''.
Selbstverständlich dürfen dabei auch alle am gleichen Tag
Geburtstag haben, oder es darf zwei Paare geben, die jeweils
übereinstimmende Geburtstage haben.
Es darf einfach nicht
sein, dass alle Geburtstage verschieden sind.
Also ist eigentlich
gemeint:
\begin{align*}
A&=\{\text{``zwei haben am gleichen Tag Geburstag''}\}\\
&=\{\text{``nicht alle Geburtstage sind verschieden''}\}\\
&=\overline{\{\text{``alle Geburstage sind verschieden''}\}}.
\end{align*}
In dieser letzten Form ist die Wahrscheinlichkeit einfacher zu
berechnen.
Es gilt ja:
\begin{align*}
P(A)&=P(\{\text{``zwei haben am gleichen Tag Geburstag''}\})\\
&=1-P(\{\text{``alle Geburstage sind verschieden''}\}).
\end{align*}
Die Anzahl der Geburtstagsverteilungen, bei denen alle Geburtstage
verschieden sind, ist einfach zu ermitteln: Für die erste Person
kann man $366$ Geburtstage auswählen, für die zweite $365$, die
dritte $364$ etc.
Insgesamt hat man
\[
366\cdot365\cdot364\dotsm(366-n+1)
\]
Möglichkeiten.
Also ist die Wahrscheinlichkeit, dass unter $n$
Personen zwei am gleichen Tag Geburtstag haben:
\[
P(A)
=
1-\frac{366\cdot365\cdot364\dotsm(366-n+1)}{366^n}
=
1-\frac{366}{366}\cdot
\frac{365}{366}\cdot
\frac{364}{366}
\dotsm
\frac{366-n+1}{366}
.
\]
Man kann diese Wahrscheinlichkeiten mit einem kleinen Programm
ausrechnen, und findet die Resultate in
Tabelle~\ref{geburtstagswahrscheinlichkeit}.
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
$n$&$P$&$n$&$P$&$n$&$P$&$n$&$P$\\
\hline
1&0.0000
&11&0.1408
&21&0.4428
&31&0.7295\\
2&0.0027
&12&0.1666
&22&0.4748
&32&0.7524\\
3&0.0082
&13&0.1939
&23&0.5063
&33&0.7740\\
4&0.0163
&14&0.2226
&24&0.5373
&34&0.7944\\
5&0.0271
&15&0.2523
&25&0.5677
&35&0.8135\\
6&0.0404
&16&0.2829
&26&0.5972
&36&0.8313\\
7&0.0561
&17&0.3143
&27&0.6258
&37&0.8479\\
8&0.0741
&18&0.3461
&28&0.6534
&38&0.8633\\
9&0.0944
&19&0.3783
&29&0.6799
&39&0.8775\\
10&0.1166
&20&0.4106
&30&0.7053
&40&0.8905\\
\hline
\end{tabular}
\end{center}
\caption{Wahrscheinlichkeit dafür, dass unter $n$ Personen zwei
am gleichen Tag Geburtstag haben.\label{geburtstagswahrscheinlichkeit}}
\end{table}
Ab $n=23$ kann man also wetten, dass zwei Personen am gleichen Tag
Geburtstag haben.

\section{Bedingte Wahrscheinlichkeit}
\index{Wahrscheinlichkeit!bedingte}
\begin{figure}
\begin{center}
\includegraphics{images/algebra-1}
\end{center}
\caption{Ereignisalgebra mit ausgezeichneten Ereignis $B$\label{bedingt1}}
\end{figure}
\begin{figure}
\begin{center}
\includegraphics{images/algebra-2}
\end{center}
\caption{Ereignisalgebra wie in Abbildung \ref{bedingt1} eingeschränkt auf das Ereignis $B$\label{bedingt2}}
\end{figure}
In der Praxis fragt man oft nach der Verkettung von Umständen: hat ein
Patient bessere Heilungschancen wenn er dieses neue Medikament nimmt?
Man könnte das auch so formulieren: ist das Ereignis
``Patient wird gesund, unter der Voraussetzung, dass er das Medikament nimmt''
wahrscheinlicher als das Ereignis ``Patient wird gesund''? Wir müssen
also aus der Ereignis\-algebra der Patienten eine neue bilden, die nur
aus den Patienten besteht, welche das Medikament genommen haben.

Wenn $(\Omega, {\cal A})$ eine Ereignisalgebra ist, und 
$B\in{\cal A}$ ein Ereignis (Abbildung~\ref{bedingt1}),
dann kann man eine neue Ereignisalgebra
$(\Omega_{|B}, {\cal A}_{|B})$ wie folgt bilden (Abbildung~\ref{bedingt2}).
Die Menge der Elementarereignisse ist
\[
\Omega_{|B}=B
\]
und die Menge der Ereignisse
\[
{\cal A}_{|B}=\{A\cap B\;|\; A\in{\cal A}\}.
\]
Es wäre noch nachzuprüfen, dass diese Menge alle Axiome einer
Ereignisalgebra erfüllt sind\footnote{Da diese technischen Feinheiten 
für die Zwecke dieses Skripts von untergeordneter Bedeutung sind, verzichten
wir auf die explizite Verifikation.}.
Diese Konstruktion macht $B$ zum sicheren Ereignis, d.~h.~in der ``Welt''
$(\Omega_{|B},{\cal A}_{|B})$ trifft $B$ immer ein (Abbildung~\ref{bedingt2}).
In $(\Omega_{|B},{\cal A}_{|B})$ muss man also davon ausgehen, dass $B$
bereits eingetroffen ist.
Man liest $\Omega_{|B}$ auch als ``$\Omega$ bedingt $B$''.

Wenn nun auf $(\Omega, {\cal A})$ eine Wahrscheinlichkeitsfunktion $P$
gegeben ist, kann man auch $P_{|B}$ bilden, indem man setzt
\begin{equation*}
P_{|B}(A)=\frac{P(A\cap B)}{P(B)},
\end{equation*}
der Nenner $P(B)$ stellt dabei sicher, dass $P_{|B}(\Omega_{|B})=P_{|B}(B)=1$.

\begin{definition}
\label{def-bedingte-wahrscheinlichkeit}
Die bedingte Wahrscheinlichkeit eines Ereignisses $A$ unter der Bedingung
$B$ ist
\[
P(A|B)=\frac{P(A\cap B)}{P(B)}.
\]
Man liest dies auch als ``Wahrscheinlichkeit von $A$ bedingt $B$''.
\end{definition}

Auch hier wäre nachzuprüfen, dass die Rechenregeln für eine
Wahrscheinlichkeitsfunktion erfüllt sind.
Um also die Wahrscheinlichkeit in dieser Welt zu berechnen,
in der $B$ bereits eingetroffen ist, 
muss man $P(A\cap B)$ berechnen können.
Dies sollte Motivation genug sein, Rechenregeln für
$P(A\cap B)$ aufzustellen, die uns bisher fehlen.

\subsection{Wahrscheinlichkeit von \texorpdfstring{$A\cap B$}{A geschnitten B}}
\begin{figure}
\begin{center}
\includegraphics{images/abhaengigkeit-1}
\end{center}
\caption{Unabhängige Ereignisse.
Die Ereignisse sind so dargestellt,
dass ihre Wahrscheinlichkeit proportional zur Fläche ist.
$P(A)$ und $P(B)$
entsprechen dem Teilverhältnis, in dem $A$ bzw.~$B$ die Seite des grossen
Rechtecks teilen.
$P(A\cap B)=P(A)\cdot P(B)$\label{unabhaengig}}
\end{figure}
\begin{figure}
\begin{center}
\includegraphics{images/abhaengigkeit-2}
\end{center}
\caption{Abhängige Ereignisse.
$P(A\cap B)$ ist offensichtlich viel kleiner
als $P(A)\cdot P(B)$.
$A$ wird unwahrscheinlicher, wenn bereits $B$ eingetreten
ist.
\label{abhaengig}}
\end{figure}
Der Grund dafür, dass es keine einfache Rechenregel für die Berechnung von 
$P(A\cap B)$ aus $P(A)$ und $P(B)$ gibt, wird mit der
Visualisierung der Ereignisse im in einem Venn-Diagramm sofort klar.
Die Elementarereignisse seien so angeordnet, dass $A$ das durch eine
vertikale Strecke abgetrennte, linke Teilrechteck von $\Omega$ ist.
Ausserdem
sollen Sie so angeordnet sein, dass die Wahrscheinlichkeit dem
Flächeninhalt im Diagramm entspricht.

Das Ereignis $B$ ist jetzt im Allgemeinen eine Teilmenge, die sowohl $A$ 
als auch $\bar A$ schneidet.
Die Wahrscheinlichkeit $P(A \cap B)$ entspricht
der Fläche der Schnittmenge im Diagramm.
Im Allgemeinen haben wir
nur aus $P(B)$ nicht genügend Information um zu entscheiden, welche
``Form'' $A\cap B$ hat, wir können also nicht erwarten, dass wir $P(A\cap B)$
berechnen können.

Wenn sich $B$ ebenfalls mittels einer horizontalen Strecke als Rechteck
einzeichnen lässt, dann ist $P(B)$ auch das Teilverhältnis, in dem
die Strecke die vertikale Kante des Diagramms teilt.
Somit kann man $P(A\cap B)$
als Flächeninhalt des Schnittrechtecks berechnen: $P(A\cap B)=P(A)\cdot P(B)$.
Die Voraussetzung bedeutet, dass das Eintreffen von $B$ nicht wahrscheinlicher
oder weniger
wahrscheinlich wird, wenn $A$ eintrifft.
Das Eintreffen von $B$ hängt also
nicht mit dem Eintreffen von $A$ zusammen.

\begin{definition}
\label{def-unabhaengige-ereignisse}
Die Ereignisse $A$ und $B$ heissen {\em unabhängig}, wenn gilt:
\[
P(A\cap B) = P(A)\cdot P(B).
\]
\end{definition}

Beim Würfeln sagt die Erfahrung, dass sich ein Würfel nicht an den
letzten Wurf erinnern kann, d.~h.~der Ausgang des letzten Wurfes hat
keinen Einfluss auf den nächsten Wurf.
Die Ereignisse ``im ersten
Wurf wird eine 5 gewürfelt'' und ``im zweiten Wurf wird eine 6 gewürfelt''
sind also unabhängig.
Die Wahrscheinlichkeit, dass mit einem Würfel
erst eine 5, dann eine 6 gewürfelt wird, ist also
\begin{align*}
P(A\cap B)&=P(\text{``erster Wurf: 5''}\cap\text{``zweiter Wurf: 6''})\\
&=P(\text{``erster Wurf: 5''})\cdot P(\text{``zweiter Wurf: 6''})\\
&=\frac1{36}.
\end{align*}

\subsection{Bedingte Wahrscheinlichkeit}
Im vorangegangenen Abschnitt wurde bereits die bedingte Wahrscheinlichkeit
$P_{|B}(A)$ konstruiert, die wir auch
\[
P(A|B)=\frac{P(A\cap B)}{P(B)}
\]
schreiben.
Die Bedeutung des Symbols $P(A|B)$ ist die folgende.
Bei der
Bestimmung der Wahrscheinlichkeit werden nur diejenigen Experimente
berücksichtigt, bei denen das Ereignis $B$ eingetreten ist.
Alle anderen Experimente interessieren nicht.

\subsubsection{Beispiel: Autounfälle und Alkohol}
Wenn jemand sagt, 50\%
der tödlichen Autounfälle geschehen unter Alkoholeinfluss, dann
meint er genau genommen eigentlich folgendes.
Wir machen ein Experiment,
bei dem wir jeden Autounfall untersuchen.
Das Ereignis $T$ umfasst alle
tödlich ausgehenden Autounfälle, das Ereignis $A$ all jene, bei denen
Alkohol im Spiel war.
Uns interessieren jetzt nur noch die tödlichen
Unfälle, also die, bei denen das Ereignis $T$ eingetreten ist. 
Die anderen untersuchen wir gar nicht mehr.
Jetzt möchten wir die
Wahrscheinlichkeit wissen, dass Alkohol im Spiel war, aber nur noch
bei den tödlichen Unfällen.
Dies ist $P(A|T)$, also gilt
$P(A|T)=50\% = 0.5$.
$P(T|A)$ ist hingegen ganz etwas anderes.
Hier fragt man danach,
wie wahrscheinlich es ist, dass bei einem Autounfall unter Alkoholeinfluss
ein Toter zu beklagen ist.

\subsubsection{Beispiel: Rauchen und Lungenkrebs}
\index{Rauchen}
\index{Lungenkrebs}
In einem Zeitungsartikel gefunden: Die Wahrscheinlichkeit, dass ein Raucher
Lungenkrebs entwickelt, ist 15\%, bei einem Nichtraucher ist sie nur 1\%.
Das Experiment besteht darin, einen Menschen zu beobachten.
Einige
davon sind Raucher (Ereignis $R$), einige entwickeln Lungenkrebs (Ereignis $L$).
Betrachtet man nur die Raucher, so ist die Wahrscheinlichkeit für
Lungenkrebs 15\%, also
\[
P(L|R)=0.15
\]
Betrachtet man nur die Nichtraucher, also das Ereignis $\bar R$, findet
man 
\[
P(L|\bar R)=0.01.
\]
Der Artikel liefert aber keine Auskunft darüber, wie viele der
Lungenkrebskranken auch Raucher sind, denn das wäre die Wahrscheinlichkeit
$P(R|L)$.

\subsubsection{Zusammenhang zwischen \texorpdfstring{$P(A\cap B)$}{P(A geschnitten B)}, \texorpdfstring{$P(A|B)$}{P(A bedingt B)} und \texorpdfstring{$P(B|A)$}{P(B bedingt A)}}
In allen drei Fällen geht es um die Wahrscheinlichkeit des Eintretens
von $A$ und von $B$, allerdings jeweils in verschiedenem
Zusammenhang.
\begin{center}
\begin{tabular}{|c|l|c|}
\hline
Wahrscheinlichkeit&Übersetzung&Scope\\
\hline
$P(A\cap B)$&\strut Wahrscheinlichkeit, dass $A$ und $B$ eintreten\strut &$\Omega$\\
%\hline
$P(A|B)$&\begin{minipage}[t]{3.0in}\strut Wahrscheinlichkeit, dass $A$ eintritt, wenn $B$ schon eingetreten ist\strut \end{minipage}&$B$\\
%\hline
$P(B|A)$&\begin{minipage}[t]{3.0in}\strut Wahrscheinlichkeit, dass $B$ eintritt, wenn $A$ schon eingetreten ist\strut \end{minipage}&$A$\\
\hline
\end{tabular}
\end{center}
(Vergleiche auch Abbildung~\ref{condprob}.)
\begin{figure}
\begin{center}
\includegraphics[width=0.3\hsize]{images/abhaengigkeit-3}\quad
\includegraphics[width=0.3\hsize]{images/abhaengigkeit-5}\quad
\includegraphics[width=0.3\hsize]{images/abhaengigkeit-4}
\end{center}
\caption{Wahrscheinlichkeit von $A\cap B$ in jeweils anderer Umgebung
\label{condprob}}
\end{figure}

\subsection{Zerlegung eines Wahrscheinlichkeitsraumes}
Der
Begriff der bedingten Wahrscheinlichkeit ermöglicht,
ein Wahrscheinlichkeitsproblem in möglicherweise einfachere, jedenfalls
kleinere Probleme zu zerlegen.
Wenn bei einem Experiment zwei Zustände
eintreten, die sich gegenseitig ausschliessen, können wir die 
Wahrscheinlichkeit bestimmen, unter denen die weiteren Resultate
des Experiments eintreten, jedoch unter der Vorbedingung, dass einer
der benannten Zustände bereits eingetreten ist.
Wenn wir nun auch
noch die Wahrscheinlichkeit jedes Zustandes kennen, müsste es doch
möglich sein, auch die Wahrscheinlichkeit der weiteren Resultate zu
ermitteln.
% TODO Missing Example
Im obigen Beispiel der Meinungsumfrage ist anschaulich klar, dass man die
Wahrscheinlichkeit für ``Ja'' berechnen kann, wenn man einerseits die
Wahrscheinlichkeit für ``Ja'' in  jeder Altersklasse kennt, und andererseits
weiss, wie die Altersklassen in der Bevölkerung verteilt sind.

\begin{figure}
\begin{center}
\includegraphics{images/total-1.pdf}
\end{center}
\caption{Wahrscheinlichkeitsraum $\Omega$, der sich in die in die
Ereignisse $B_1,B_2,\dots,B_n$ zerlegen lässt.\label{zerlegung}}
\end{figure}
Wir versuchen, das etwas formaler zu schreiben
(siehe Abbildung \ref{zerlegung}):
Sind $B_i$ mit $1\le i$ paarweise
disjunkte Ereignisse
in einer Ereignisalgebra $(\Omega,{\cal A})$, für welche ausserdem
gilt
\[
\bigcup_{i=0}^{n}B_i = \Omega,
\]
dann kann man die Wahrscheinlichkeit von $A$ aus den bedingten
Wahrscheinlichkeiten der Teile $A\cap B_i$ und $B_i$ wieder zusammensetzen.
Bekannt sind die Wahrscheinlichkeiten
$P(B_i)$, die Wahrscheinlichkeit der Zustände, und
$P(A|B_i)$, die Wahrscheinlichkeit, dass $A$ eintritt, unter der Bedingung
das $B_i$ bereits eingetreten ist.
Nach Definition ist $P(A|B_i)=P(A\cap B_i)/P(B_i)$, also
$P(A|B_i)\cdot P(B_i) = P(A\cap B_i)$.
Die Vereinigung der Ereignisse $A\cap B_i$ ist
\[
\bigcup_{i=0}^{n} A\cap B_i=A\cap\bigcup_{i=0}^{n}B_i=A,
\]
und die Mengen $A\cap B_i$ sind disjunkt.
Also kann man die Summe ihrer
Wahrscheinlichkeiten mit der Additionsformel ausrechnen:
\[
\sum_{i=0}^{n}P(A\cap B_i)=P\biggl(\bigcup_{i=0}^{n}A\cap B_i\biggr)=P(A).
\]
Andererseits
haben wir oben bereits $P(A\cap B_i)$ bestimmt, wir können
also einsetzen und erhalten:
\[
P(A)=\sum_{i=0}^{n}P(A|B_i)\cdot P(B_i).
\]
Somit lässt sich tatsächlich die Wahrscheinlichkeit eines Ereignisses
aus den bedingten Wahrscheinlichkeiten und den Wahrscheinlichkeiten
der Bedingungen errechnen.
Wir fassen diese Erkenntnis in folgendem Satz zusammen.
\index{Wahrscheinlichkeit!totale}
\begin{satz}
Ist $B_i$ eine Folge paarweise disjunkter Mengen mit $\bigcup_{i=0}^{n}B_i=\Omega$, dann gilt für jedes Ereignis $A$
\[
P(A)=\sum_{i=0}^{n}P(A|B_i)\cdot P(B_i).
\]
\end{satz}
Dieser Satz heisst auch der {\em Satz von der totalen Wahrscheinlichkeit},
da er die Wahrscheinlichkeit eines Ereignisses aus den bedingten
Wahrscheinlichkeiten unter verschiedenen Voraussetzungen und der
Wahrscheinlichkeit des Eintretens dieser Voraussetzungen rekonstruiert.
Dieser Satz ist die ``wahrscheinlichkeitstheoretisch Form einer
Fallunterscheidung'': man kennt die Fälle $B_i$, und deren Wahrscheinlichkeit
$P(B_i)$, sowie die Wahrscheinlichkeit $P(A|B_i)$, dass $A$ im Falle 
$B_i$ eintritt.
Die Formel über die totale Wahrscheinlichkeit
liefert daraus wieder $P(A)$.

Wir bemerken noch, dass es gar nicht unbedingt nötig ist, dass die Mengen
$B_i$ ganz $\Omega$ ausschöpfen, es würde auch
genügen, wenn die Wahrscheinlichkeit $0$ ist, dass $A$ eintritt
unter der Bedingung,
dass bereits $\bigcup_{i=0}^{n}B_i$ eingetreten ist.
Für den
Experimentator bedeutet das, dass er nur jene Vorbedingungen $B_i$
berücksichtigen muss, unter denen sein Experiment ``funktioniert'',
alle anderen möglichen Zustände haben keinen Einfluss auf
die Wahrscheinlichkeit seiner Resultate.
Man darf beim Experimentieren also durchaus einzelne Resultate
verwerfen, wenn man weiss, was man tut!

\subsubsection{Studienerfolg}
Die folgenden Zahlen sind erfunden, und dienen nur der Illustration
des Prinzips.
Eine Statistik hat die Wahrscheinlichkeit für den
Studienerfolg untersucht, und folgende Resultate erhalten.
Die Studierenden setzen sich zusammen aus 60\% BMS, 30\% Kantonsschüler
und Übertritte von anderen Hochschulen.
Die Wahrscheinlichkeit
das Studium erfolgreich abzuschliessen, ist für BMS 80\%,
für Kantonsschüler 90\%, 85\% für die Übertreter von anderen
Hochschule.
Wie gross ist die Wahrscheinlichkeit für den Studienerfolg?

$\Omega$ ist die Menge aller Studienversuche.
60\% davon bilden das
Ereignis $B$ bestehend aus Studienversuchen, die im Anschluss an die BMS
erfolgen.
30\% machen das Ereignis $K$ mit den Kanti-Abgängern aus,
10\% das Ereignis $A$ mit Übertritten von anderen Hochschulen.
Es ist klar, dass 
\[
\Omega = B \cup K\cup A.
\]
Gefragt ist die Wahrscheinlichkeit des Ereignisses $E$, welches
die erfolgreich abgeschlossenen Studienversuche beinhaltet.
Für die Wahrscheinlichkeit gilt nach dem Satz über die totale
Wahrscheinlichkeit
\begin{align*}
P(E)&=P(E|B)P(B)+P(E|K)P(K)+P(E|A)P(A).
\\
    &= 80\%\cdot 60\%
     + 90\%\cdot 30\%
     + 85\%\cdot 10\%
\\
&=0.8\cdot 0.6 + 0.9 \cdot 0.3 + 0.85 \cdot 0.1 = 0.835.
\end{align*}

\subsubsection{Würfelspiel}
Wir betrachten nochmals das Beispiel Würfeln mit zwei Würfeln, wobei
bei einem Pasch im ersten Wurf noch genau einmal gewürfelt wird.
Wir möchten die Wahrscheinlichkeit berechnen, dass die totale Augensumme
mindestens 10 ist.
Wir nennen dieses Ereignis $Z$.

Zunächst kann man unterteilen für den Fall, dass im ersten Wurf
ein Pasch (Ereignis $P$) geworfen wird.

Es gilt nach dem Satz über die totale Wahrscheinlichkeit
\[
P(Z) = P(Z|P) P(P) + P(Z|\bar P) P(\bar P).
\]
Selbstverständlich ist $P(\bar P)=1-P(P)$, so dass man $P(Z)$ bereits
vereinfachen kann:
\[
P(Z) = P(Z|P) P(P) + P(Z|\bar P) (1-P(P)).
\]
Man kann aber das Ereignis $P$ noch weiter unterteilen in die
Ereignisse $P_k$, mit $1\le k\le 6$, also
\begin{align*}
P(P)&=
P(P_1)+
P(P_2)+
P(P_3)+
P(P_4)+
P(P_5)+
P(P_6),
\\
P(Z|P)
&=
P(Z|P_1)P(P_1|P)+
P(Z|P_2)P(P_2|P)+
P(Z|P_3)P(P_3|P)
\\
&\qquad +
P(Z|P_4)P(P_4|P)+
P(Z|P_5)P(P_5|P)+
P(Z|P_6)P(P_6|P).
\end{align*}
Die Ereignisse haben wir auch früher schon untersucht, daraus kann
man einige Wahrscheinlichkeiten bereits ablesen:
\begin{align*}
P(Z|P_6)&=1,\\
P(Z|P_5)&=1,\\
P(Z|P_4)&=1,
\end{align*}
denn in allen diesen Fällen erreicht man mit dem zweiten Wurf mit
Sicherheit zehn oder mehr.

\subsection{Ziegen und Autos} \label{ziegen:autos}
Marylin vos Savant hat in einer Kolumne ein Gedankenexperiment vorgestellt.
In einer Quiz-Show muss der Kandidat eine von drei Türen wählen, wobei
sich hinter einer der Türen ein Auto versteckt, hinter den anderen Zweien
ein Ziege.
Das Ziel des Spiels ist, das Auto zu gewinnen.
Nach der Wahl
durch den Kandidaten öffnet der Quizmaster eine der Türen, hinter der sich
eine Ziege befindet, nicht aber die Tür, die der Kandidat gewählt hat.
Der Kandidat hat jetzt nochmals die Möglichkeit, die Tür zu wechseln.
Welche Strategie soll er wählen.
Bei der ersten Auswahl bleiben, oder
wechseln?

Um die Frage zu beantworten berechnen wir die Gewinnwahrscheinlichkeit
für jede der Strategien.
Zunächst die ``Bleibe''-Strategie.
Es ist die
Wahrscheinlichkeit für einen Gewinn $P(G)$ auszurechnen.
Hinter der zunächst
gewählten Tür kann sich ein Auto (Ereignis $A$) befinden, oder eine Ziege
(Ereignis $Z$).
Es ist natürlich $A\cap Z=\emptyset$ und $A\cup Z=\Omega$.
Also gilt
\begin{equation}
P(G)=P(G|A) P(A) + P(G|Z)P(Z).
\label{ziegenformel}
\end{equation}
Natürlich ist $P(A)=\frac13$ und $P(Z)=\frac23$.
$P(G|A)$ ist die Wahrscheinlichkeit mit der ``Bleibe''-Strategie zu
gewinnen, wenn hinter der zunächst gewählten Tür ein Auto war: diese
ist natürlich $1$.
Ebenso verliert man mit Sicherheit, wenn hinter der
Tür eine Ziege war: $P(G|Z)=0$, also
\[
P(G)=P(G|A)P(A)+P(G|Z)P(Z)=1\cdot \frac13 + 0\cdot\frac 23=\frac13.
\]

Für die ``Wechsel''-Strategie gilt natürlich auch die Formel
\ref{ziegenformel}, aber die bedingten Wahrscheinlichkeiten sind verschieden.
Wenn man ein Auto gewählt hatte, und wechselt, verliert man, also $P(G|A)=0$.
Hat man ein Ziege erwischt, und wechselt, bekommt man das Auto, denn die
zweite Ziege kann man ja nicht erwischen, deren Tür hat der Quiz-Master
geöffnet.
Also ist $P(G|Z)=1$.
Somit folgt jetzt
\[
P(G)=P(G|A)P(A)+P(G|Z)P(Z)=0\cdot\frac13+1\cdot\frac23=\frac23.
\]
Mit der Wechselstrategie ist also die Wahrscheinlichkeit, zu gewinnen,
doppelt so gross, wechseln lohnt sich also auf jeden Fall.


\subsection{Wahrscheinlichkeitsvektoren und -matrizen}
Der Satz von der totalen Wahrscheinlichkeit kann auch in Matrixform
geschrieben werden:
\begin{align*}
P(A)&=P(A|B_1)P(B_1)+\dots+P(A|B_n)P(B_n)
=
\begin{pmatrix}
P(A|B_1)&\dots&P(A|B_2)
\end{pmatrix}
\begin{pmatrix}
P(B_1)\\\vdots\\P(B_n)
\end{pmatrix}
\end{align*}
Der Spaltenvektor
\[
\begin{pmatrix}
P(B_1)\\\vdots\\P(B_n)
\end{pmatrix}
\]
hat die Eigenschaften, dass alle seine Einträge zwischen $0$ und $1$
liegen, und ihre Summe $1$ gibt.
\index{Wahrscheinlichkeitsvektor}
Ein solcher Vektor heisst {\it Wahrscheinlichkeitsvektor}.

Die Matrizenschreibweise erlaubt, auch die totalen Wahrscheinlichkeiten
für mehrere Ereignisse $A_1,\dots,A_m$ gleichzeitig zu berechnen:
\[
\begin{pmatrix}
P(A_1)\\
P(A_2)\\
\vdots\\
P(A_m)
\end{pmatrix}
=
\begin{pmatrix}
P(A_1|B_1) & P(A_1|B_2) & \dots &P(A_1|B_n)\\
P(A_2|B_1) & P(A_2|B_2) & \dots &P(A_2|B_n)\\
\vdots     & \vdots     & \ddots&\vdots\\
P(A_m|B_1) & P(A_m|B_2) & \dots &P(A_m|B_n)\\
\end{pmatrix}
\begin{pmatrix}
P(B_1)\\
P(B_2)\\
\vdots\\
P(B_n)
\end{pmatrix}.
\]
Sind die Ereignisse $A_i$ ebenfalls paarweise disjunkt und decken ganz
$\Omega$ ab, dann sind auch die Spalten von der Matrix $(P(A_i|B_j))$
jeweils Wahrscheinlichkeitsvektoren.
\index{Wahrscheinlichkeitsmatrix}
Eine solche Matrix heisst {\it Wahrscheinlichkeitsmatrix}.

\subsection{Satz von Bayes} \label{satz-von-bayes}
\index{Bayes, Satz von}
Für zwei beliebige Ereignisse mit $A$ und $B$ mit nicht verschwindender
Wahrscheinlichkeit gilt
\[
P(A|B)\cdot P(B)= P(A\cap B)=P(B|A)\cdot P(A),
\]
und es folgt
\[
P(A|B)=\frac{P(B|A)\cdot P(A)}{P(B)}.
\]
Dieser Zusammenhang ist bekannt als der Satz von Bayes:
\begin{satz}[Satz von Bayes]
Für zwei Ereignisse $A$ und $B$ mit $P(B)\ne0$ gilt
\[
P(A|B)=\frac{P(B|A)\cdot P(A)}{P(B)}.
\]
\end{satz}
Die Bedeutung dieses Satzes besteht darin, dass er die Schlussrichtung
umzukehren erlaubt.
Die bedingte Wahrscheinlichkeit $P(A|B)$ ist ja die
Wahrscheinlichkeit, dass ein das Ereignis $A$ eintritt, wenn $B$ bereits
eingetreten ist.
Sie erlaubt, eine Wette einzugehen, dass $A$ eintreten
wird, wenn $B$ bereits eingetreten ist.
Der Satz von Bayes ermöglicht
also, auch eine Wette für $B$ einzugehen, wenn $A$ bereits eingetreten
ist.
Im Gegensatz zur Schlussweise in der Logik, die niemals umkehrbar ist,
kann man auf Wahrscheinlichkeiten basierende Schlüsse umkehren.

\subsubsection{Was bedeutet ein positiver HIV-Test?}
\begin{figure}
\begin{center}
\includegraphics[width=\hsize]{graphics/aids-300}
\end{center}
\caption{``Blick am Abend'' vom 12.~August 2008\label{aids}}
\end{figure}
Im ``Blick am Abend'' vom 12.~August 2008 erschien der Artikel in Abbildung 
\ref{aids} mit dem Titel ``Warum uns Wahrscheinlichkeiten verwirren''.
Darin ist von folgenden Ereignissen die Rede, die sich auf sich nicht riskant
verhaltende Männer ($=\Omega$) beziehen.
\index{HIV-Test}
\begin{enumerate}
\item Ereignis $H$: Ein Mann ist HIV-infiziert.
\item Ereignis $T$: ein HIV-Test ergibt ein positives Resultat.
\end{enumerate}
Der Artikel teilt uns zudem ein paar Wahrscheinlichkeiten mit:
\begin{itemize}
\item HIV-Rate bei Männern, die sich nicht riskant verhalten:
$P(H)=0.0001$.
\item Wahrscheinlichkeit, dass bei einer infizierten Person der Test
ein positives Resultat ergibt: $P(T|H)=0.999$.
\item Wahrscheinlichkeit, dass bei einer gesunden Person der Test negativ
ist: $P(\bar T|\bar H)=0.9999$.
\end{itemize}
Wie gross ist die Wahrscheinlichkeit, dass ein sich nicht riskant verhaltender
Mann tatsächlich HIV hat, wenn er einen positiven HIV-Test hat? Wie
gross ist $P(H|T)$?

Der Satz von Bayes liefert die Antwort:
\begin{equation}
P(H|T)=\frac{P(T|H)\cdot P(H)}{P(T)}
\label{aidsprobability}
\end{equation}
Darin sind fast alle Grössen direkt aus dem Artikel ablesbar, nur $P(T)$ muss
noch bestimmt werden.
Dazu dient der Satz über die totale Wahrscheinlichkeit,
den wir auf die Tatsache anwenden, dass ein sich nicht riskant verhaltender
Mann entweder HIV hat oder nicht: $\Omega=H\cup \bar H$.
\begin{align*}
P(T)
&=P(T|H)\cdot P(H)+P(T|\bar H)\cdot P(\bar H)\\
&=P(T|H)\cdot P(H)+(1-P(\bar T|\bar H))\cdot (1 - P(H))\\
&=0.999\cdot 0.0001+(1-0.9999)\cdot(1-0.0001)\\
&=0.00019989\simeq 0.0002
\end{align*}
Eingesetzt in (\ref{aidsprobability}) ergibt sich jetzt das Resultat
\[
P(H|T)=\frac{0.999\cdot 0.0001}{0.0002}=.4995\simeq 0.5
\]
oder mit anderen Worten, auch wenn der HIV-Test positiv ist, ist
die Wahrscheinlichkeit für einen sich nicht riskant verhaltenden Mann,
tatsächlich HIV-infiziert zu sein, nur $50\%$.

