%
% schaetzen.tex -- Schaetzen
%
% (c) 2006-2015 Prof. Dr. Andreas MŸller
%
\rhead{Sch"atzen}
\chapter{Sch"atzen} \label{chapter-schaetzen}
Aus der Erfahrung wissen wir, dass viele Experimente normalverteilte Resultate
liefern.
Aber selbst aus mehreren Messungen l"asst sich der Mittelwert
oder die Varianz noch nicht exakt bestimmen,
welche wie in Kapitel \ref{chapter-wahrscheinlichkeitsverteilung}
beschrieben die Normalverteilung eindeutig bestimmen w"urden.
Vielmehr zeigt das Gesetz der grossen Zahlen, dass erst eine
relativ grosse Zahl von Messungen eine einigermassen zuverl"assige
Bestimmung der Parameter der Verteilung gestattet.
Daher braucht es spezielle Sch"atzverfahren, mit denen solche Parameter
aus deutlich weniger Experimenten gesch"atzt werden k"onnen.

\section{Das Sch"atzproblem} \label{section-schaetzproblem}
In bisherigen Beispielen sind wir bei der Bestimmung von Erwartungswert
oder Varianz jeweils davon ausgegangen, dass einige bekannte Werte der
Zufallsvariablen bereits eine repr"asentative Beschreibung der Zufallsvariablen
darstellen.
Wir konnten also aus den Beobachtungen die Wahrscheinlichkeiten
f"ur die beobachteten Werte ableiten und daraus direkt Erwartungswert
und Varianz bestimmen.

Dieses heuristische Vorgehen ist als erste N"aherung durchaus akzeptabel,
und funktioniert f"ur den Erwartungswert auch optimal.
F"ur die Varianz
hingegen zeigt sich, dass die oben skizzierte, simplizistische Rechnung
die Varianz vor allem bei kleinen Stichproben systematisch untersch"atzt.
Damit stellt sich automatisch die Frage, wie denn "uberhaupt ein geeignetes
Sch"atzverfahren f"ur eine Gr"osse wie die Varianz gefunden werden kann.

Etwas genauer geht es um folgendes Problem.
Von einer gegebenen Zufallsvariable
$X$ ist die Verteilung nicht bekannt, es wird angenommen, dass die
Verteilungsfunktion die Form $F(x,\vartheta)$ hat, wobei
$\vartheta$ ein vorl"aufig noch unbekannter Parameter ist.
Bei einer Normalverteilung sind dies zum Beispiel der Erwartungswert und
die Varianz: $\vartheta=(\mu,\sigma^2)$.
Nun wird das
Experiment $n$ mal durchgef"uhrt, dadurch entstehen $n$ neue, unabh"angige
Zufallsvariable $(X_i)_{1\le i\le n}$, welche identisch verteilt sind zu
$X$.

\begin{definition}
Eine Folge von Zufallsvariablen $(X_i)_{1\le i\le n}$ heisst eine
Stichprobe der Zufallsvariablen $X$, wenn die $X_i$ unabh"angig 
und identisch zu $X$ verteilt.
\end{definition}
Das Problem besteht nun darin, die Parameterwerte $\vartheta$
aus den gefundenen Werten $X_1,\dots,X_n$ zu finden.
Man braucht dazu
eine sogenannten Sch"atzer, eine Funktion, die $\vartheta$ aus den
Beobachtungen bildet: $\vartheta=\vartheta(X_1,\dots,X_n)$.

Bei der Konstruktion eines Sch"atzverfahrens kann man sich von verschiedenen,
teils nicht kompatiblen Prinzipien leiten lassen, und man wird verschiedene
``vern"unftige'' Eigenschaften von den Sch"atzern fordern wollen:
\begin{enumerate}
\item {\bf Konsistent:} Vergr"ossert man die Stichprobe, soll der
Sch"atzer gegen den wahren Wert des Parameters streben, also
$\lim_{n\to\infty}\vartheta(X_1,\dots,X_n)=\vartheta$.
\item {\bf Erwartungstreu:} Da die $X_i$ auch wieder Zufallsvariable sind,
kann man den Erwartungswert von $\vartheta(X_1,\dots,X_n)$ bilden. 
Das Verfahren ist besonders vertrauensw"urdig, wenn dieser Erwartungswert
mit dem wahren Wert des Parameters "ubereinstimmt.
\item {\bf Minimaler Fehler:} Selbst wenn der Sch"atzwert im Mittel den
richtigen Parameterwert liefert, m"ochten wir doch, dass dies mit m"oglichst
geringem Fehler passiert, so dass auch eine einzelne Bestimmung des
Parameters bereits grosse Zuverl"assigkeit geniesst.
Der mittlere
quadratische Fehler, der im Zusammenhang mit dem Erwartungswert bereits
einmal diskutiert wurde, kann hier als Prinzip zur Konstruktion von
Sch"atzern erhoben werden.
\item {\bf ``Am Wahrscheinlichsten'':} Ein etwas anderes Prinzip der 
Konstruktion eines Sch"atzers besteht darin, die Beobachtungen $X_1,\dots,X_n$
in die Dichtefunktion der Verteilung einzusetzen und denjenigen Parameterwert
zu w"ahlen, der diese Funktion maximiert.
Diese sogenannten
``maximum likelihood'' Sch"atzer sind oft vern"unftig, haben interessante
asymptotische Eigenschaften, sind aber manchmal nicht erwartungstreu.
\end{enumerate}

Leider zeigt es sich, dass die Forderung nach minimalem quadratischem
Fehler technisch oft nur schwer umzusetzen ist.
Trotzdem liefert das verwandte
Prinzip der kleinsten Quadrate, welches wir schon bei der Charakterisierung
des Erwartungswertes mit Hilfe der Varianz und bei der Regression 
getroffen haben, manchmal n"utzliche Vorschl"age f"ur geeignete
Sch"atzer.

Sobald ein Sch"atzverfahren festgelegt ist, muss man sich mit der Frage der
Zuverl"assigkeit des Verfahrens auseinandersetzen.
Da das Sch"atzverfahren
auch ``nur'' eine Zufallsvariable ist, wird es den Parameter auch nur
mit einer gewissen Unsicherheit liefern k"onnen.
Es stellt sich damit
automatisch die Frage nach der Gr"osse einer m"oglichen Abweichung.
Eine Antwort darauf gibt das sogenannte Konfidenzintervall, ein
Intervall, in dem sich der wahre Wert des Parameters mit einer gewissen
Wahrscheinlichkeit befindet.

In diesem Kapitel betrachten wir zun"achst die bereits bekannten 
Sch"atzer f"ur den Erwartungswert und die Varianz, und untersuchen
sie daraufhin, ob sie erwartungstreu sind.
In den Abschnitten \ref{section-maximum-likelihood-schaetzer}
und \ref{section-weitere-beispiele-von-schaetzern} wird an einigen
Beispielen gezeigt, wie man Sch"atzer nach dem
Maximum-Likelihood-Prinzip konstruieren kann.
In \ref{section-verteilung-der-schaetzwerte}
wird die Verteilung der Sch"atzer studiert, eine Grundlage, damit
anschliessend in \ref{section-konfidenzintervalle} Konfidenzintervall
f"ur die gesch"atzen Werte konstruiert werden k"onnen.

\section{Konsistente Sch"atzer} \label{section-konsistente-schaetzer}
Der Mittelwert der Stichprobe ist ein konsistenter Sch"atzer,
denn nach Bernoulllis Gesetz der grossen Zahlen gilt
\begin{equation}
\lim_{n\to\infty}\frac{X_1+\dots+X_n}{n}=E(X).
\end{equation}
\begin{definition}
Der Sch"atzer
\begin{equation}
\bar X=\frac{X_1+\dots+X_n}{n}
\end{equation}
heisst der Stichprobenmittelwert der Stichprobe $X_1,\dots,X_n$.
\end{definition}
Ganz analog kann gezeigt werden, dass die Sch"atzformel f"ur die
Varianz ebenfalls ein konsistenter Sch"atzer ist.

\section{Erwartungstreue Sch"atzer} \label{section-erwartungstreue-schaetzer}
Nach \ref{section-konsistente-schaetzer} ist der Stichprobenmittelwert
ein konsistenter Sch"atzer,
aber er ist sogar erwartungstreu:
\[
E(\mu(X_1,\dots,X_n))=\frac{E(X_1)+\dots+E(X_n)}{n}=\frac{E(X)+\dots+E(X)}{n}
=E(X),
\]
der Mittelwert ist also durchaus ein Sch"atzer mit ``guten'' Eigenschaften.

F"ur die Sch"atzung der Varianz wurde bisher aus beobachteten Werten
$(x_i)_{1\le i\le n}$ der Zufallsvariable der Ausdruck
\begin{equation}
\frac1n\sum_{i=1}^n x_i^2-\biggl(\frac1n\sum_{i=1}^nx_i\biggr)^2
\end{equation}
verwendet.
Dem entspricht die Sch"atzformel
\begin{equation}
V=f(X_1,\dots,X_n)=\frac1n\sum_{i=1}^nX_i^2-\biggl(\frac1n\sum_{i=1}^nX_i\biggr)^2.
\label{schaetzer-sstern}
\end{equation}
Die Zufallsvariable $V$ soll angeblich die Varianz m"oglichst gut
wiedergeben, also sollte auch der Erwartungswert von $V$ mit der Varianz
"ubereinstimmen.

Ein numerisches Experiment zeigt jedoch ein anderes Bild.
In einer
Simulation wurde jeweils 10000 mal die Varianz einer kleinen Anzahl von
standardnormalverteilten Zufallsvariablen mit obiger Sch"atzformel ermittelt.
In der Tabelle~\ref{varianzschaetzung} sind die Resultate von acht
L"aufen dieser Simulation untereinander dargestellt.
Man kann gut erkennen, 
dass die Sch"atzformel f"ur geringe Anzahlen von Werten, also f"ur kleine
$n$, deutlich falsch liegt.
Statt dem wahren Wert $1$ der Varianz
werden Werte gefunden, die deutlich kleiner sind, die Zahlenwerte
suggerieren, dass die Sch"atzformel statt $1$ im Mittel den Wert
$\frac{n-1}n$ liefert.
Dies ist ein Indiz, dass die Sch"atzformel nicht
erwartungstreu ist.

\begin{table}
\begin{center}
\begin{tabular}{|c|ccccccccc|}
\hline
$n$&2&3&4&5&6&7&8&9&10\\
\hline
&0.502&0.674&0.745&0.802&0.839&0.853&0.871&0.892&0.896\\
&0.488&0.670&0.746&0.800&0.837&0.858&0.875&0.890&0.899\\
&0.501&0.668&0.746&0.800&0.843&0.861&0.872&0.892&0.900\\
&0.487&0.670&0.743&0.805&0.826&0.854&0.868&0.883&0.898\\
&0.506&0.672&0.745&0.797&0.841&0.856&0.873&0.887&0.904\\
&0.506&0.659&0.763&0.794&0.835&0.855&0.874&0.888&0.908\\
&0.513&0.666&0.751&0.795&0.832&0.853&0.868&0.897&0.903\\
&0.499&0.669&0.748&0.799&0.833&0.856&0.873&0.894&0.903\\
\hline
$\frac{n-1}{n}$&0.500&0.667&0.750&0.800&0.833&0.857&0.875&0.889&0.900\\
\hline
\end{tabular}
\end{center}
\caption{Mittelwerte von 10000 Varianzberechnungen von jeweils $n$
Werten einer standardnormalverteilten Zufallsvariable\label{varianzschaetzung}}
\end{table}

Das dies tats"achlich so ist wird deutlich, wenn wir den
Erwartungswert von $V$ berechnen:
\begin{align*}
E(V)
&=
E(f(X_1,\dots,X_n))
\\
&=
E\biggl(
\frac1n\sum_{i=1}^nX_i^2-\biggl(\frac1n\sum_{i=1}^nX_i\biggr)^2
\biggr)
\\
&=
\frac1n\sum_{i=1}^nE(X_i^2)-\frac1{n^2}\sum_{i=1}^n\sum_{j=1}^n E(X_iX_j)
\\
&=
\frac1n\sum_{i=1}^nE(X_i^2)-
\frac1{n^2}\sum_{i=1}^nE(X_i^2)
-\frac1{n^2}\sum_{i\ne j} E(X_iX_j)
\\
&=
\frac{n-1}{n^2}\sum_{i=1}^nE(X_i^2)
-\frac1{n^2}\sum_{i\ne j} E(X_i)E(X_j).
\end{align*}
Nun sind aber alle Zufallsvariable $X_i$ identisch verteilt, insbesondere
sind alle Erwartungswerte der $X_i$ und der $X_i^2$ jeweils identisch,
wir k"onnen also die Indizes weglassen:
\begin{align*}
E(V)
&=
\frac{n-1}{n^2}\sum_{i=1}^nE(X^2) -\frac1{n^2}\sum_{i\ne j} E(X)^2
\\
&=
\frac{n-1}{n^2}E(X^2) -\frac{n^2-n}{n^2}E(X)^2
\\
&=
\frac{n-1}{n}(E(X^2) -E(X)^2)=\frac{n-1}n\operatorname{var}(X).
\end{align*}
Folglich ist der Sch"atzer $V$ f"ur kleine Stichproben eine schlechte
Wahl, man muss erwarten, dass er einen zu kleinen Wert liefert.
Es
w"are besser, ihn mit $\frac{n}{n-1}$ zu multiplizieren, denn der so
gebildete Sch"atzer hat als Erwartungswert wie gew"unscht die Varianz.

\begin{definition}
Ist $X_i$ eine Stichprobe von $X$, dann heisst
\begin{equation}
S^2=\frac1{n-1}\sum_{i=1}^n (X_i -\bar X)^2
\end{equation}
die {\em Stichprobenvarianz} von $X$.
\end{definition}

\begin{satz}Die Stichprobenvarianz ist ein erwartungstreuer Sch"atzer
f"ur die Varianz einer Zufallsvariable.
\end{satz}

Dem Sch"atzer
\begin{equation}
S^{*}\mathstrut^{2}=\frac1n\sum_{i=1}^n(X_i-\mu)^2
\end{equation}
f"ur die Varianz fehlt nicht viel zu einem erwartungstreuen Sch"atzer,
er unterscheidet sich vom eben gefundenen besseren Sch"atzer
\begin{equation}
S^2=\frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2
\end{equation}
nur durch einen Faktor $\frac{n}{n-1}$.
F"ur eine kleine Stichprobe ist
der bisherige Sch"atzer also nicht erwartungstreu, aber je gr"osser $n$
wird, desto kleiner wird die Abweichung von der Erwartungstreue.
Man
nennt $S^{*}\mathstrut^{2}$ einen asymptotisch erwartungstreuen Sch"atzer:
\begin{definition}
Ein Sch"atzer $\vartheta(X_1,\dots,X_n)$ heisst asymptotisch erwartungstreu,
wenn
\begin{equation}
\lim_{n\to\infty}E(\vartheta(X_1,\dots,X_n))=\vartheta.
\end{equation}
\end{definition}

\section{Maximum-Likelihood-Sch"atzer} \label{section-maximum-likelihood-schaetzer}
\subsection{Stetige Verteilung}
Ein weiteres Prinzip, welches ``vern"unftige'' Sch"atzer liefert,
ist das Maximum-Likelihood-Prinzip.
Ist $X$ eine Zufallsvariable, deren
Verteilung eine Dichtefunktion $f(x,\vartheta)$ mit einem unbekannten
Parameter $\vartheta$ besitzt. $\vartheta$ soll gesch"atzt werden.
Wir bilden daraus die Likelihood-Funktion
\begin{equation}
L(x_1,\dots,x_n;\vartheta)=f(x_1,\vartheta)\cdots f(x_n,\vartheta),
\label{likelihood-funktion}
\end{equation}
also die $n$-dimensionale Dichtefunktion f"ur die durch $n$-malige 
Beobachtung entstehende Stichprobe $X_1,\dots,X_n$, bestehend aus
unabh"angigen, identisch zu $X$ verteilten Zufallsvariablen.
Die Likelihood-Funktion wird um so gr"osser, je wahrscheinlicher
es ist, die Werte der Stichprobe in einem $n$-dimensionalen W"urfel
$dx_1\,dx_2\dots dx_n$ zu finden.
Das Maximum-Likelihood-Prinzip
verlangt nun, dass man als Sch"atzwert f"ur $\vartheta$ jenen Wert
w"ahlt, der die Likelihood-Funktion maximiert.

\begin{definition}
$\vartheta(X_1,\dots,X_n)$ heisst Maximum-Likelihood-Sch"atzer f"ur
$\vartheta$, wenn die Likelihood-Funktion (\ref{likelihood-funktion})
maximal wird:
\begin{equation}
L(X_1,\dots,X_n;\vartheta(X_1,\dots,X_n)) \ge L(X_1,\dots,X_n;t)\quad\forall t.
\end{equation}
\end{definition}

Um das Prinzip zu illustrieren, berechnen wir den
Maximum-Likelihood-Sch"atzer f"ur den Erwartungswert
einer normalverteilten Zufallsvariable.
Die Dichtefunktion ist 
\[
f(x,\vartheta)=\frac1{\sqrt{2\pi}}e^{-\frac{(x-\vartheta)^2}{2\sigma^2}}
\]
woraus sich die Likelihood-Funktion als
\begin{equation}
L(x_1,\dots,x_n;\vartheta)=
\frac1{(\sqrt{2\pi})^n}e^{-\frac1{2\sigma^2}\sum_{i=1}^n(x_i-\vartheta)^2}
\label{likelihood-funktion-normalverteilung}
\end{equation}
ergibt.
Zu den gegebenen $X_1,\dots,X_n$ muss nun $\vartheta$ so gefunden
werden, dass die Likelihood-Funktion
(\ref{likelihood-funktion-normalverteilung}) maximiert wird.
Dies geschieht offensichtlich genau dann, wenn die Summe im Exponenten
ihren kleinsten Wert annimmt, wir m"ussen also ein Minimum von
\begin{equation}
\sum_{i=1}^n (x_i-\vartheta)^2
\end{equation}
finden.
Durch Ableiten nach $\vartheta$ entsteht daraus
\begin{equation}
\frac{d}{d\vartheta}
\sum_{i=1}^n (x_i-\vartheta)^2
=-2\sum_{i=1}^n (x_i-\vartheta)=2\biggl(n\vartheta-\sum_{i=1}^nx_i\biggr)
=2n\biggl(\vartheta-\frac1n\sum_{i=1}^nx_i\biggr)
\end{equation}
was genau dann verschwindet, wenn der Klammerausdruck $=0$ ist, also
\begin{equation}
\vartheta=\frac1n\sum_{i=1}^nx_i.
\end{equation}
Der altbekannte Mittelwert einer Stichprobe ist also auch der
Maximum Likelihood Sch"atzer f"ur den Erwartungswert einer normalverteilten
Zufallsvariable.
\begin{satz}
Der Stichprobenmittelwert ist der Maximum-Likelihood-Sch"atzer
f"ur den Erwartungswert einer normalverteilten Zufallsvariable.
\end{satz}
Nach demselben Muster kann man aus die Stichprobenvarianz $S^2$ als
Maximum-Likelihood-Sch"atzer konstruieren.

\subsection{Diskrete Verteilung}
Das Maximum-Likelihood-Prinzip L"asst sich auch bei diskreten Verteilungen
anwenden.
Anstelle der Dichtefunktion treten die Wahrscheinlichkeiten
$p(x, \vartheta)$, welche nur f"ur eine diskrete Menge von $x$-Werten
von $0$ verschieden sind.
Die Likelihood-Funktion ist dann
\begin{equation}
L(x_1,\dots,x_n;\vartheta)=p(x_1,\vartheta)\dots p(x_n,\vartheta),
\label{likelihood-funktion-diskret}
\end{equation}
und die Konstruktion des Maximum-Likelihood-Sch"atzers l"asst sich genau
wie im stetigen Fall durchziehen.

Als Beispiel betrachten wir eine Zufallsvariable $X$ mit Werten $0$ und
$1$, wobei $1$ mit der Wahrscheinlichkeit $p$ angenommen wird,
und versuchen, den Parameter $p$ zu sch"atzen.
Die Wahrscheinlichkeit
f"ur den Wert $0$ ist $1-p$.
Daraus l"asst sich die Likelihood-Funktion konstruieren
\begin{equation}
L(k_1,\dots,k_n;p)=p^{\sum_{i=1}^nk_i}(1-p)^{n-\sum_{i=1}^nk_i},
\label{likelihood-funktion-p}
\end{equation}
wobei die $k_i$ nur die Werte $0$ und $1$ annehmen k"onnen.
Die Summe 
$K=\sum_{i=1}^nk_i$ ist die Anzahl der F"alle, in denen der Wert $1$
angenommen wurde.
Nach dem Maximum-Likelihood-Prinzip ist $p$ so zu bestimmen,
dass die Likelihood-Funktion
(\ref{likelihood-funktion-p})
maximal wird.
Dabei bleibt $K$ unver"andert, es ist also der Ausdruck
\begin{equation}
f(p)=p^K(1-p)^{n-K}
\label{p-schaetzer-funktion}
\end{equation}
zu maximieren.
Die Ableitung von (\ref{p-schaetzer-funktion}) ist
\begin{equation}
f'(p)=Kp^{K-1}(1-p)^{n-K}-(n-K)p^K(1-p)^{n-K-1}=0
\end{equation}
Nach Division durch $p^{K-1}(1-p)^{n-K-1}$ wird daraus
\begin{equation}
K(1-p)-(n-K)p=K-np=0.
\end{equation}
Diese Gleichung kann man nach $p$ aufl"osen: $p=K/n$.
Somit ist die relative H"aufigkeit der Maximum-Likelihood-Sch"atzer
f"ur die Wahrscheinlichkeit $p$.
\begin{satz}
Der Stichprobenmittelwert ist der Maximum-Likelihood-Sch"atzer
f"ur die Wahrscheinlichkeit $p$ eines
positiven Ausgangs eines Bernoulliexperimentes.
\end{satz}

\section{Weitere Beispiele von Sch"atzern} \label{section-weitere-beispiele-von-schaetzern}
\subsection{L"ange eines Intervalls}
Von der Zufallsvariable $X$ ist bekannt, dass sie im Intervall $[0,\vartheta]$
gleichverteilt ist.
Der Parameter $\vartheta$ soll gesch"atzt werden.
Offensichtlich ist $\vartheta \ge \max(X_1,\dots,X_n)$,
aber $T=\max(X_1,\dots,X_n)$ ist fast sicher kleiner als der wahre
Wert von $\vartheta$, wir k"onnen also nicht erwarten, dass $T$ ein
erwartungstreuer Sch"atzer ist.

In der Hoffnung, sp"ater einen erwartungstreuen Sch"atzer konstruieren
zu k"onnen, berechnen wir den Erwartungswert von $T$.
Die Verteilungsfunktion von $X$ auf dem Intervall $[0,\vartheta]$ ist
\begin{equation}
F(x)=P(X\le x)= \frac{x}{\vartheta}.
\end{equation}
Die Verteilungsfunktion von $T(X_1,\dots,X_n)$ ist dann
\begin{align*}
P(\max(X_1,\dots,X_n)\le x)
&=
P(X_1\le x\wedge\dots\wedge X_n\le x)
\\
&=
P(X_1\le x)\dots P(X_n\le x)
\\
&=
F(x)^n=\frac{x^n}{\vartheta^n},
\end{align*}
im Intervall $[0,\vartheta]$ entspricht dies der Dichtefunktion
\begin{equation}
\frac{nx^{n-1}}{\vartheta^n}.
\end{equation}
Damit wird der Erwartungswert
\begin{align*}
E(\max(X_1,\dots,X_n))
&=
\int_0^{\vartheta}x\frac{nx^{n-1}}{\vartheta^n}\,dx
\\
&=
\frac{n}{n+1}\biggl[\frac{x^{n+1}}{\vartheta^n}\biggr]_0^\vartheta
\\
&=
\frac{n}{n+1}\vartheta,
\end{align*}
bis auf den Faktor $\frac{n}{n+1}$ ist das genau die gesuchte Intervalll"ange.
\begin{satz}
Ist $X_i$ eine Stichprobe einer auf dem Intervall $[0,\vartheta]$
gleichverteilten Zufallsvariable $X$, dann ist
\begin{equation}
\vartheta(X_1,\dots,X_n)=\frac{n+1}{n}\max(X_1,\dots, X_n)
\end{equation}
eine erwartungstreuer Sch"atzer f"ur die Intervalll"ange $\vartheta$.
\end{satz}

\subsection{Sch"atzung des Parameters \texorpdfstring{$\lambda$}{lambda} einer Poissonverteilung}
Die Zufallsvariable $X$ sei poissonverteilt, d.~h.
\begin{equation}
p(k, \lambda)=\frac{\lambda^k}{k^!}e^{-\lambda}.
\end{equation}
Der Parameter $\lambda$ soll aus einer Stichprobe gesch"atzt werden.
Dazu bilden wir die Likelihood Funktion
\begin{equation}
L(k_1,\dots k_n;\lambda)=\frac{\lambda^{k_1+\dots+k_n}}{k_1!\dotsm k_n!}
\,e^{-n\lambda}.
\label{poisson-likelihood-funktion}
\end{equation}
Das Maximum der Likelihood-Funktion wird durch Ableiten von
(\ref{poisson-likelihood-funktion}) nach $\lambda$
bestimmt:
\begin{align}
\frac{d}{d\lambda}L(k_1,\dots,k_n;\lambda)
&=
\frac{1}{k_1!\dotsm k_n!}(K\lambda^{K-1}-n\lambda^K)e^{-n\lambda}\nonumber
\\
&=
\frac{1}{k_1!\dotsm k_n!}(K-n\lambda)\lambda^{K-1}e^{-n\lambda}.
\label{poisson-likelihood-ableitung}
\end{align}
Die Ableitung verschwindet genau dann, wenn der Klammerausdruck in
(\ref{poisson-likelihood-ableitung}) verschwindet, also ist
\begin{equation}
\lambda(k_1,\dots,k_n) =\frac1n\sum_{i=1}^nk_i
\end{equation}
der Maximum Likelihood Sch"atzer f"ur $\lambda$.
Offensichtlich ist er
konsistent und erwartungstreu.

\subsection{Sch"atzung von \texorpdfstring{$p$}{p} in einer Binomialverteilung}
Von der Zufallsvariablen $X$ sei bekannt, dass sie binomialverteilt ist
mit Parameter $(m, p)$, der Parameter $p$ ist zu sch"atzen.
Die Likelihood-Funktion ist in diesem Fall
\begin{equation}
L(k_1,\dots,k_n;p)=\biggl(\prod_{i=1}^n\binom{m}{k_i}\biggr)
p^{K}(1-p)^{nm-K},
\label{binomial-likelihood-funktion}
\end{equation}
wobei wir wieder $K=\sum_{i=1}^nk_i$ setzen. 
Das grosse Produkt in (\ref{binomial-likelihood-funktion})
ist nur ein konstanter Faktor, den wir mit $P$ abk"urzen
\begin{equation}
P= \prod_{i=1}^n\binom{m}{k_i}
\end{equation}
Die Ableitung von (\ref{binomial-likelihood-funktion}) ist
\begin{equation}
\frac{d}{dp}L(k_1,\dots,k_n;p)=P
\bigl(Kp^{K-1}(1-p)^{nm-K}-(nm-K)p^K(1-p)^{nm-K-1}\bigr)=0.
\end{equation}
Der zweite Klammerausdruck l"asst sich vereinfachen zu
\[
(K(1-p)-(nm-K)p)p^{K-1}(1-p)^{nm-K-1}=
(K-nmp)p^{K-1}(1-p)^{nm-K-1}=0
\]
er verschwindet genau dann, wenn $p=K/nm$.
Somit ist
\begin{equation}
p(k_1,\dots,k_n)=\frac1{nm}\sum_{i=1}^nk_i
\end{equation}
der Maximum-Likelihood-Sch"atzer f"ur $p$.
Offensichtlich ist er konsistent
und erwartungstreu.
\begin{satz}
Ein erwartungstreuer Sch"atzer f"ur die Wahrscheinlichkeit einer
Binomialverteilung $\operatorname{Bin}(m,k,p)$ ist
$\frac1{m}\bar X$.
\end{satz}

\section{Verteilung der Sch"atzwerte} \label{section-verteilung-der-schaetzwerte}
Sch"atzwerte entstehen dadurch, dass die Werte einer Stichprobe, also
gewisse Realisierungen von Zufallsvariablen in die Sch"atzfunktion
eingesetzt werden.
Somit sind die Sch"atzwerte selbst wieder Zufallsvariablen,
und es stellt sich die Frage, welcher Verteilung sie gehorchen.

F"ur einen Spezialfall haben wir uns um diese Problem bereits gek"ummert.
Bei der Sch"atzung der Intervalll"ange haben wir die Verteilungsfunktion
des Sch"atzwertes bestimmt, um einen erwartungstreuen Sch"atzer konstruieren
zu k"onnen.

Ein anderer Spezialfall ist die Sch"atzung der Parameter einer normalverteilten
Zufallsvariable.
Die Stichprobenelemente $X_i$ sind alle normalverteilt
mit dem gleichen Erwartungswert und der gleichen Varianz wie $X$. 
Somit ist der Sch"atzwert f"ur den Erwartungswert
$\bar X$
ebenfalls normalverteilt mit Erwartungswert $E(X)$ (wir wissen ja bereits,
dass dieser Sch"atzer erwartungstreu ist), und Varianz
$\frac1n\operatorname{var}(X)$.
Wie ist der
Sch"atzwert f"ur die Stichprobenvarianz
$S^2=$
verteilt? Dazu gibt der folgende, vielleicht etwas "uberraschende Satz Auskunft.
\begin{satz}
Seien $X_1,\dots,X_n$ unabh"angige, normalverteilte Zufallsvariablen mit
Erwartungswert $\mu$ und Varianz $\sigma^2$.
Dann gilt
\begin{enumerate}
\item $\bar X$ und $S^2$ sind unabh"angig.
\item $\bar X$ ist normalverteilt mit Erwartungswert $\mu$ und Varianz
$\frac{\sigma^2}{n}$.
\item $\frac{n-1}{\sigma^2}S^2$ ist $\chi^2_{n-1}$-verteilt.
\end{enumerate}
\end{satz}
\begin{proof}[Beweis]
Wir beweisen zun"achst die Unabh"angigkeit mit Hilfe der momenterzeugenden
Funktion.
Zun"achst gilt
\begin{equation}
M_{X_i}(t_i)=\exp\biggl(\mu t_i+\frac{\sigma^2t_i^2}2\biggr).
\end{equation}
Wir betrachten jetzt den Vektor $U=(U_1,\dots,U_n)$, bestehend aus
den Zufallsvariablen $U_i=X_i-\bar X$, und $V=\bar X$.
Wir setzen $\bar s=(s_1+\dots +s_n)/n$.
Die gemeinsame momenterzeugende Funktione
von $U$ und $V$ ist dann
\begin{align}
M_{U,V}(s_1,\dots,s_n,r)
&=
E\biggl(\exp\biggl(Vr+\sum_{i=1}^nU_is_i\biggr)\biggr)
\nonumber\\
&=
E\biggl(\exp\biggl(\bar X(r-n\bar s)+\sum_{i=1}^nX_is_i\biggr)\biggr)
\nonumber\\
&=
E\biggl(\exp\biggl(\sum_{i=1}^nX_i\bigl(s_i-\bar s+\frac{r}{n}\bigr)\biggr)\biggr)
\nonumber\\
&=
M_{X_i}\biggl(s_1-\bar s+\frac{r}{n}\biggr)\cdots
M_{X_n}\biggl(s_n-\bar s+\frac{r}{n}\biggr)
\nonumber\\
&=
\exp\biggl(
\mu\biggl(\sum_{i=1}^n(s_i-\bar s+r/n)\biggr)+\frac{\sigma^2}2\sum_{i=1}^n(s_i-\bar s+r/n)^2
\biggr)
\nonumber\\
&=
\exp\biggl(\mu r+\frac{\sigma^2r^2}{2n}\biggr)
\exp\biggl(\frac{\sigma^2}{2}\sum_{i=1}^n(s_i-\bar s)^2\biggr)\label{umformung-mittelwert}
\\
&=
M_{U,V}(0,\dots,0,r)M_{U,V}(s_1,\dots,s_n,0)
\nonumber
\end{align}
In (\ref{umformung-mittelwert}) haben wir folgende zwei Identit"aten verwendet:
\begin{align}
\sum_{i=1}^n(s_i-\bar s)
&=
\sum_{i=1}^ns_i-n\bar s
\nonumber\\
&=
\sum_{i=1}^ns_i-n\frac1n\sum_{i=1}^ns_i=0 \label{si-mittelwert}
\\
\sum_{i=1}^n(s_i-\bar s+\frac{r}{n})^2
&=
\sum_{i=1}^n(s_i-\bar s)^2
+\frac{2r}{n}\sum_{i=1}^n(s_i-\bar s)
+\sum_{i=1}^n\frac{r^2}{n^2}
\nonumber\\
&=
\sum_{i=1}(s_i-\bar s)^2
+\frac{2r}{n}\sum_{i=1}^ns_i-2r\bar s
+\frac{r^2}{n}
\nonumber\\
&=
\sum_{i=1}^n(s_i-\bar s)^2+\frac{r^2}{n} \label{si-varianz}
\end{align}
Da sich die gemeinsame momenterzeugende Funktion faktorisieren
l"asst, sind die Zufallsvariablen $U_i$ und $V$ unabh"angig,
und damit nat"urlich auch $S^2=\frac1{n-1}(U_1^2+\dots+U_n^2)$ und
$\bar X=V$.

Die Verteilung von $\bar X$ wurde bereits fr"uher bestimmt, und bei
dieser Gelegenheit alles im zweiten Punkt behauptete nachgerechnet.

Da nach obigem die Zufallsvariablen $Z_i=(X_i-\mu)/\sigma$ unabh"angig
standardnormalverteilt sind, ist deren Quadratsumme
\begin{equation}
T=\sum_{i=1}^nZ_i^2=\sum_{i=1}^n\frac{(X_i-\mu)^2}{\sigma^2}
\end{equation}
$\chi^2_{n}$-verteilt.
Da ausserdem $Z=\sqrt{n}(\bar X-\mu)/\sigma$ standardnormalverteilt
ist, ist ihr Quadrat
\begin{equation}
W=Z^2=\frac{n(\bar X-\mu)^2}{\sigma^2}
\end{equation}
$\chi^2_1$-verteilt.
Wir setzen $Y=(n-1)S^2/\sigma^2$.
$W$ und $Y$ sind nach dem eben Bewiesenen unabh"angig.
Wir behaupten
\[
T=W+Y.
\]
In der Tat 
\begin{align}
W+Y
&=
\frac{n(\bar X-\mu)^2}{\sigma^2}
+
\frac{1}{\sigma^2}\sum_{i=1}^n(X_i-\bar X)^2
\nonumber\\
&=
\frac{1}{\sigma^2}\sum_{i=1}^n\bigl((\bar X-\mu)^2+(X_i-\bar X)^2\bigr)
\nonumber\\
&=
\frac{1}{\sigma^2}\sum_{i=1}^n(X_i-\mu)^2
\label{wy-chi2}
\end{align}
Im letzten Schritt (\ref{wy-chi2}) haben wir den Hilfssatz
\ref{hilfssatz-varianz-mittelwert}
weiter unten verwendet.
F"ur die momenterzeugenden Funktionen bedeutet dies
\begin{equation}
M_T(t)=M_W(t)M_Y(t)
\end{equation}
oder nach Satz \ref{chi2}
\begin{equation}
M_Y(t)=\frac{M_T(t)}{M_W(t)}=\frac{(1-2t)^{-n/2}}{(1-2t)^{-1/2}}
=(1-2t)^{-(n-1)/2},
\end{equation}
dies ist die momenterzeugende Funktion einer $\chi^2_{n-1}$-Verteilung.
Somit ist $Y$ $\chi_{n-1}^2$ verteilt, wie behauptet.
\end{proof}

\begin{hilfssatz}
\label{hilfssatz-varianz-mittelwert}
Seien $(x_i)_{1\le i\le n}$ reelle Zahlen und $\bar x$ so gew"ahlt,
dass $\sum_{i=1}^n(x_i-\bar x)^2$ minimal wird.
Dann ist 
\begin{equation}
\bar x=\frac1n\sum_{i=1}^nx_i
\end{equation}
und f"ur jedes $\mu\in\mathbb{R}$ gilt
\begin{equation}
\sum_{i=1}^n\bigl((\bar x-\mu)^2 + (x_i-\bar x)^2\bigr)
=\sum_{i=1}^n(x_i-\mu)^2.
\end{equation}
\end{hilfssatz}
\begin{proof}[Beweis]
Wir betrachten die Vektoren $(t,\dots,t)$, also eine Gerade im
$n$-dimensionalen Raum.
$\bar x$ ist so gew"ahlt, dass $(\bar x,\dots,\bar x)$
der Fusspunkt des Lotes von $(x_1,\dots,x_n)$ auf die Gerade ist.
Insbesondere bilden die Punkte $(\bar x,\dots,\bar x)$, $(\mu,\dots,\mu)$
und $(x_1,\dots,x_n)$ bilden ein rechtwinkliges Dreieck.
Dies erlaubt,
die Entfernung zwischen $(\mu,\dots,\mu)$ und $(x_1,\dots,x_n)$ 
als Hypotenuse mit Hilfe des Satzes von Pythagoras zu berechnen:
\[
\sum_{i=1}^n(x_i-\mu)^2
=\sum_{i=1}^n(\bar x-\mu)^2+\sum_{i=1}^n(x_i-\bar x)^2,
\]
die Behauptung des Hilfssatzes.
\end{proof}

\section{Konfidenzintervalle} \label{section-konfidenzintervalle}
Mit Hilfe eines Sch"atzers k"onnen die Parameter einer Verteilung
gesch"atzt werden.
Wenn wir zum Beispiel bei einer Messapparatur
wissen, dass die Messwerte normalverteilt sind, k"onnen wir durch
Mittelwertbildung eine gute Sch"atzung f"ur den Erwartungswert finden.
Da der Sch"atzwert selbst eine Zufallsvariable ist, kann er
im schlimmsten Fall ziemlich weit weg vom Erwartungswert zu liegen
kommen.
Wir m"ochten herausfinden, wie wahrscheinlich dies ist.

Das Problem w"are offensichtlich gel"ost, wenn wir ein Intervall
angeben k"onnten, in dem der wahre Wert des Erwartungswertes mit
grosser Wahrscheinlichkeit $1-\alpha$ liegen wird.
In der Praxis
wird man f"ur $\alpha$ kleine Werte w"ahlen, zum Beispiel $0.05$ oder $0.01$.
Ein solches Intervall heisst $1-\alpha$-Konfidenzintervall f"ur den
Parameter.

\begin{definition}
Ein Intervall $[L(X_1,\dots,X_n),R(X_1,\dots,X_n)]$
heisst ein $1-\alpha$-Konfidenzintervall
f"ur den Parameter $\vartheta$, wenn der wahre Wert des Parameters
$\vartheta$ mit Wahrscheinlichkeit h"ochstens $\alpha$ ausserhalb
des Intervalls liegt.
\end{definition}

\subsection{Konfidenzintervall bei bekannter Varianz}
Nehmen wir an, die Varianz der Verteilung $\sigma^2$ sei uns bereits bekannt.
Dann ist $\bar X$ eine normalverteilte Zufallsvariable mit Varianz
$\sigma^2/n$, deren Erwartungswert mit dem gesuchten Erwartungswert von $X$
"ubereinstimmt. 

W"are $\mu$ bekannt, w"are es recht einfach, ein Intervall zu finden,
in dem sich der Wert von $\bar X$ mit Wahrscheinlicheit $1-\alpha$ befinden
wird.
Mit Hilfe der Verteilungsfunktion $F$ der Standardnormalverteilung
k"onnten wir zum Beispiel die Werte $x_-$ und $x_+$ finden, f"ur die
gilt $F(x_-)=\frac{\alpha}{2}$ und $F(x_+)=1-\frac{\alpha}{2}$, f"ur
diesen Zweck gibt es spezielle Tabellen, zum Beispiel
\ref{tabelle-normalquantilen}.
Dann hat das Intervall
$[\mu+\sigma x_-,\mu+\sigma x_+]$ die Eigenschaft, dass $\bar X$
mit Wahrscheinlichkeit $1-\alpha$ darin enthalten sein wird.

Nun ist zwar $\mu$ nicht bekannt, aber wenn
$\bar X\in[\mu+\frac{\sigma}{\sqrt{n}} x_-,\mu+\frac{\sigma}{\sqrt{n}} x_+]$,
dann ist sicher auch
$\mu\in[\bar X+\frac{\sigma}{\sqrt{n}} x_-,\bar X+\frac{\sigma}{\sqrt{n}} x_+]$
d.~h.~wir haben ein
Intervall gefunden, in dem sich der Parameter mit Wahrscheinlichkeit $1-\alpha$
befindet.

\subsection{Konfidenzintervall mit gesch"atzer Varianz}
Im allgemeinen ist die Varianz jedoch nicht bekannt, und wir m"ussen
auch f"ur die Varianz eine Sch"atzung verwenden.
Bei bekannter Varianz
konnte ein geeignetes Intervall gefunden werden, indem die
standardnormalverteilte Zufallsvariable
$\frac{\bar X-\mu}{\sigma/\sqrt{n}}$
untersucht wurde.
Die Normalverteilung definierte Intervallgrenzen,
f"ur die
\begin{equation}
P(x_-\le
\frac{\bar X-\mu}{\sigma/\sqrt{n}}
\le x_+)=1-\alpha
\label{konfidenzintervall-bekannte-varianz}
\end{equation}
gilt, woraus sich dann das Konfidenzintervall ergab.

Da nun die Varianz auch gesch"atzt werden muss, ersetzen wir in
(\ref{konfidenzintervall-bekannte-varianz})
$\sigma$ durch $S$ und versuchen wieder Grenzen
$x_-$ und $x_+$ zu finden, so dass
\begin{equation}
P(x_-\le
\frac{\bar X-\mu}{S/\sqrt{n}}
\le x_+)=1-\alpha
\label{konfidenzintervall-geschaetzte-varianz}
\end{equation}
gilt.
Dazu muss die Verteilung der Zufallsvariable
\begin{equation}
\sqrt{n}\frac{\bar X-\mu}{S}
=\frac{\sqrt{n}(\bar X-\mu)/\sigma}{\sqrt{(n-1)S^2/\sigma^2(n-1)}}
\label{konfidenzintervall-verteilung}
\end{equation}
bekannt sein.
Im Nenner ist $(n-1)S^2/\sigma^2$ eine
$\chi_{n-1}^2$-verteilte Zufallsvariable,
der Z"ahler ist standardnormalverteilt.
Der Quotient ist "uber den
vorliegenden Fall hinaus von Bedeutung:

\begin{definition}
Ist $Z$ eine standardnormalverteilte Zufallsvariable, und $V$ ein
$\chi_k^2$, dann heisst die Verteilung von
\[
t=\frac{Z}{\sqrt{V/k}}
\]
die $t$-Verteilung mit $k$ Freiheitsgraden.
\end{definition}

\begin{satz}Die Wahrscheinlichkeitsdichte der $t$-Verteilung ist
\begin{equation}
\varphi_t(t)=\frac{\Gamma(\frac{k+1}{2})}{\sqrt{\pi k}\Gamma(\frac{k}2)}
\biggl(1+\frac{t^2}{k}\biggr)^{-\frac{k+1}2}
\end{equation}
\end{satz}

\begin{proof}[Beweis]
Die Dichtefunktion von $Z$ und $V$ sind
\begin{align*}
\varphi_Z(x)
&=
\frac1{\sqrt{2\pi}}e^{-\frac{x^2}2}
\\
\varphi_V(x)
&=
\gamma_{\frac12,\frac{k}2}(x)=\frac1{\Gamma(\frac{k}2)}\frac1{2^\frac{k}2}x^{\frac{k-2}2}e^{-\frac12x}.
\end{align*}
Die Dichtefunktion von $V/k$ ist $\varphi_{V/k}(x)=k\varphi_V(kx)$, aus
den Resultaten der Abschnitte \ref{verteilungsfunktion-wurzel}
und \ref{verteilungsfunktion-quotient} l"asst sich jetzt die
Dichtefunktion f"ur $T$ berechnen.

Zun"achst berechnen wir die Dichtefunktion f"ur den Nenner:
\[
\varphi_{\sqrt{V/k}}(x)=2x\varphi_{V/k}(x^2)=2xk\varphi_V(kx^2).
\]
Die Dichtefunktion f"ur den Quotienten ist dann
\begin{align*}
\varphi_T(t)
&=
\int_0^\infty \varphi_X(ty)y\varphi_Y(y)\,dy
\\
&=
\int_0^\infty \frac1{\sqrt{2\pi}}e^{-\frac{(ty)^2}2}y\cdot
2yk\varphi_V(ky^2)\,dy
\\
&=
\int_0^\infty \frac1{\sqrt{2\pi}}e^{-\frac{(ty)^2}2}y\cdot
2yk
\frac1{\Gamma(\frac{k}2)}\frac1{2^{\frac{k}2}}(ky^2)^{\frac{k-2}2}e^{-\frac12ky^2}
\,dy
\\
&=
\frac1{\sqrt{k\pi}2^{\frac{k-1}2}\Gamma(\frac{k}2)}
\int_0^\infty e^{-\frac12(1+t^2/k)y^2} \,dy.
\end{align*}
Mit Hilfe der Substitution $s=\frac12(1+\frac{t^2}n)y^2$
oder
\[
y=\frac{2^{\frac12}s^{\frac12}}{(1+\frac{t^2}n)^{\frac12}}
\]
wir daraus
\begin{align*}
\varphi_T(t)
&=
\frac{2^{\frac12}2^{\frac{k}2}}{\sqrt{k\pi}2^{\frac{k-1}2}\Gamma(\frac{k}2)(1+\frac{t^2}k)^{\frac{k+1}2}}
\int_0^\infty \frac12e^{-s}s^{\frac{k-1}2}\,ds
\\
&=
\frac{1}{\sqrt{k\pi}\Gamma(\frac{k}2)(1+\frac{t^2}k)^{\frac{k+1}2}}
\int_0^\infty e^{-s}s^{\frac{k+1}2-1}\,ds
\\
&=
\frac{\Gamma(\frac{k+1}2)}{\sqrt{k\pi}\Gamma(\frac{k}2)}\biggl(1+\frac{t^2}k\biggr)^{-\frac{k+1}2}.
\end{align*}

\end{proof}

Auch zur $t$-Verteilung existieren Tabellen "ahnlich der Normalverteilung,
mit denen sich Werte $t_-$ und $t_+$ finden lassen, sodass
$P(t_-\le t\le t_+)=1-\alpha$.
Mit diesen Werten l"asst sich dann
auch ein Konfidenzintervall f"ur den Erwartungswert geben:

\begin{satz}
Ist $X$ eine normalverteilte Zufallsvariable, und $X_1,\dots,X_n$ eine
Stichprobe, $\bar X$ der Stichprobenmittelwert, 
und $S^2$ die Stichprobenvarianz.
Seien $t_-$ und $t_+$ so bestimmt, dass
$P(t_-\le t_{n-1}\le t_+)=1-\alpha$ f"ur eine $t$-Verteilung $t_{n-1}$ mit
$n-1$ Freiheitsgraden.
Dann ist
\begin{equation}
[\bar X+t_-\frac{S}{\sqrt{n}},\bar X+t_+\frac{S}{\sqrt{n}}]
\end{equation}
ein $1-\alpha$-Konfidenzintervall f"ur den Erwartungswert von $X$.
\end{satz}

\subsubsection{Konfidenzintervall f"ur Juli-Durchschnittstemperatur}
Die Wetterstation in Altendorf hat f"ur Juli 2003 eine Durchschnittstemperatur
von $21.638^\circ\text{C}$ gemessen, bei einer Stichprobenvarianz von
$493.230\text{K}^2$ und $n=44473$.
Der Tabelle der $t$-Verteilung
\ref{tabelle-tverteilung} entnimmt man 
f"ur derart
viele Freiheitsgrade und $\alpha=0.01$ die Werte
$t_{\pm}=\pm2.5758$.
Ein $0.99$-Konfidenzintervall f"ur die
Durchschnittstemperatur ist daher
$[21.332, 21.944]$.

