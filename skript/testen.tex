%SourceDoc ws-skript.tex
%
% c07-testen.tex
%
% (c) 2006 Prof. Dr. Andreas Mueller
% $Id: c07-testen.tex,v 1.6 2008/09/09 14:22:16 afm Exp $
%
\rhead{Testen}
\chapter{Hypothesen testen\label{chapter-hypothesen-testen}}
Francis Bacon (1561--1626) hat gegen die im Mittelalter "ubliche
Art des Spekulierens in den Wissenschaften einen umfassenden Neubau
der Wissenschaften auf der Grundlage der ``unverf"alschten Erfahrung''
versucht. Die Entscheidung "uber Fakten und kausale Zusammenh"ange
sollten nicht durch abgehobenes Debattieren und Berufung auf 
Autorit"aten entschieden werden, sondern durch den Ausgang
m"oglichst eindeutiger Experimente. Ein solches ``experimentum crucis''
sollte einem unvoreingenommenen Beobachter eine klare Antwort geben
k"onnen. In der modernen Wissenschaft hat sich jedoch gezeigt, dass
die Entscheidung selten wirklich so einfach ist. Oft bewegt sich
der Experimentator an der Grenze des messbaren, und er l"auft
st"andig Gefahr, Messfehler f"ur bisher unbekannte Effekte zu halten
oder vor lauter statistischen Schwankungen selbst den theoretisch
gut untermauerten Effekt nicht zu sehen.

Der Hypothesentest ist daher das moderne Gegenst"uck zu Bacons
experimentum crucis. Er erlaubt dem Experimentator auf neutrale
Art zu entscheiden, ob in seinen Messresultaten nun der vermutete
Effekt sichtbar ist, oder die Endeckung auch nur durch die unvermeidlichen
Zufallsschwankungen vorgespiegelt sein k"onnte.

In Kapitel \ref{chapter-schaetzen} wurde gezeigt, wie Parameter einer
Verteilung gesch"atzt werden k"onnen. Trotzdem bleiben viele Fragen
offen:
\begin{itemize}
\item Obwohl man bei der Wiederholung eines Experimentes andere Werte
gemessen hat, m"ochte man mit m"oglichst geringer Fehlerwahrscheinlichkeit
best"atigen k"onnen, dass man immer noch den gleichen Wert gemessen hat.
\item Kann man 95\% sicher sein, dass ein neuer Werkstoff wirklich gr"ossere
Festigkeit aufweist als sein Vorg"anger?
\item Kann man wirklich 95\% sicher sein, dass die Messresultate
in einem Experiment normalverteilt sind?
\end{itemize}
Die Sch"atzung von Parametern kann diese Fragen allein noch nicht beantworten.
Da der Sch"atzer selbst eine Zufallsvariable ist, m"ussen wir noch
mehr "uber dessen Verteilung wissen, um die erste oder die "aquivalente
zweite Frage zu entscheiden. In der dritten Frage stellt die
Verteilungsfunktion selbst in Frage, also die Grundlagen, auf denen
die ganze Theorie der Sch"atzer aufgebaut wird.

Ein Hypothesentest versucht die Frage zu beantworten, ob vorhandene
Messdaten mit der Hypothese vereinbar sind, und falls nicht, mit welcher
Wahrscheinlichkeit zu Unrecht an der G"ultigkeit der Hypothese gezweifelt
wurde.

\section{Allgemeines Prinzip}
Als Illustration des allgmeinen Problems betrachten wir folgende Frage.
Von einer Zufallsgr"osse $X$ wird behauptet, dass sie normalverteilt ist
mit Erwartungswert $0$ und Varianz $1$. Nun wird eine Messung durchgef"uhrt,
bei der der Wert $10$ gemessen wird. Die Normalverteilung verbietet nat"urlich
nicht, dass ein derart hoher Wert auftritt. Allerdings ist die
Wahrscheinlichkeit dass bei einer Standardnormalverteilten Zufallsvariable
ein Wert $\ge 10$ auftritt
$$P(|X|\ge 10)=1.523970\cdot 10^{-23}$$
das beobachtete Ereignis ist also extrem unwahrscheinlich. Da die Beobachtung
real ist, muss man es als extrem unwahrscheinlich ansehen, dass die
Zufallsvariable tats"achlich standardnormalverteilt war.

Nach diesem Grundprinzip funktionieren die meisten Testverfahren.
Auf der Basis einer Hypothese (oft auch die Nullhypothese genannt)
wird eine Testgr"osse $\varphi(\cdot)$ bestimmt, die 
unter der Hypothese ein gewisses Intervall nur mit einer sehr kleinen
Wahrscheinlichkeit verl"asst.
Dann wird eine Beobachtung $X$ durchgef"uhrt, und die Testgr"osse
$\varphi(X)$ ausgewertet.
Die Hypothese wird verworfen, wenn die
Testgr"osse das Intervall in der Beobachtung verl"asst.

Im erw"ahnten Beispiel besteht die Hypothese in der Annahme, dass die
Messgr"osse standardnormalverteilt ist. Der Testgr"osse entspricht dem
Betrag einer einzelnen
Messung. Auf Grund der Annahme, dass die Testgr"osse
standardnormalverteilt ist, kann die Wahrscheinlichkeit daf"ur berechnet
werden, dass $\varphi(X)=|X|\ge10$. Genau dieses Ereignis tritt dann ein.

Hier sind offensichtlich zwei Arten von Fehlern m"oglich:
\begin{enumerate}
\item Fehler erster Art: Die Hypothese war richtig, und wird trotzdem verworfen.
Das Testverfahren kann genau angeben, mit welcher Wahrscheinlichkeit
dieser Fehler gemacht wird.
\item Fehler zweiter Art: Die Hypothese war falsch, wird aber nicht verworfen.
Da die Hypothese falsch war, muss die Berechnung des Intervalls in Zweifel
gezogen werden, in dem sich die Testgr"osse befinden soll. Damit kann
in vielen F"allen "uber die Wahrscheinlichkeit eines Fehlers zweiter
Art nichts ausgesagt werden.
\end{enumerate}
Man beachte, dass der Test gar nicht beweist, dass die Hypothese richtig
war, er sagt bestenfalls aus, dass die vorhandenen Daten nicht ausreichen,
an der Hypothese zu zweifeln.

Im genannten Beispiel ist die Wahrscheinlichkeit, einen Fehler erster
Art zu begehen $1.523970\cdot 10^{-23}$. Meist geht man in der Praxis von der
Wahrscheinlichkeit aus, mit der man einen Fehler erster Art zu begehen
bereit ist, und berechnet daraus das Intervall, in dem sich die
Testgr"osse bewegen darf. Verlangt man zum Beispiel, dass ein Fehler erster
Art h"ochstens mit Wahrscheinlichkeit $0.01$ eintritt, dann muss die 
Testgr"osse in einem Intervall $[-m,m]$ liegen mit $P(-m\le X\le m)=0.99$.
Die Gleichung
$$0.99 = P(-m\le X\le m)
=\frac12\left(\operatorname{erf}(\frac{m}{\sqrt{2}})-\operatorname{erf}(-\frac{m}{\sqrt{2}})\right)
=\operatorname{erf}(\frac{m}{\sqrt{2}})
$$
kann mit Hilfe der inversen Fehlerfunktion aufgel"ost werden:
$$m=\sqrt{2}\operatorname{erf}^{-1}(0.99)=2.575829$$
Somit w"are bereits eine Abweichung von mehr als dem $2.58$-fachen einer
Standardabweichung Grund genug, die Hypothese zu verwerfen.

\begin{definition}
Ein Test auf dem Niveau $\alpha$ ist eine Regel, welche zu einer
Beobachtung $X$ entscheidet, ob die Hypothese zu verwerfen ist, wobei
sie h"ochstens mit Wahrscheinlichkeit $\alpha$ einen Fehler erster
Art begeht.
\end{definition}

Weiter oben haben wir einen sehr primitiven Test auf dem Niveau
$\alpha=0.01$ f"ur die
Hypothese formuliert, dass $X$ standardnormalverteilt ist.

\section{Testen der Wahrscheinlichkeit eines Ereignisses}
Mit Hilfe von Beobachtungen k"onnen wir die relative H"aufigkeit ermitteln, mit
welcher ein Ereignis $A$ eintritt, von dem wir glauben, dass es mit
Wahrscheinlichkeit $p=P(A)$ eintreten sollte.
Damit stellt sich automatisch die Frage, ob wir mit Hilfe der
Beobachtungen entscheiden k"onnen, ob $p$ tats"achlich die Wahrscheinlichkeit
von $A$ ist.

Wir betrachten dazu unabh"angige Zufallsvariable $(X_i)_{1\le i\le n}$,
welche ausschliesslich
die Werte $0$ und $1$ annehmen, und zwar mit Wahrscheinlichkeit
$1-p$ bzw.~$p$. Die Zufallsvariable $X_i$ soll mit dem Wert $1$ anzeigen,
ob bei der $i$-ten Durchf"uhrung des Experiments das Ereignis eingetreten ist.
Die Summe der Zufallsvariablen 
$$X=\sum_{i=1}^nX_i$$
ist eine Zufallsvariable, die angibt, wie oft bei $n$ Versuchen das Ereignis
eingetreten ist. F"ur Erwartungswert und Varianz von $X_i$ finden
wir folgende Werte
\begin{eqnarray*}
E(X_i)&=&0\cdot (1-p)+1\cdot p=p\\
\operatorname{var}(X_i)&=&E(X_i^2)-E(X_i)^2=0^2\cdot (1-p)+1^2\cdot p-p^2
=p-p^2\\
&=&p(1-p)
\end{eqnarray*}
Entsprechend hat $X$ den Erwartungswert $E(X)=np$ und
die Varianz $\operatorname{var}(X)=np(1-p)$.

Nach dem zentralen Grenzwertsatz wird die geeignet standardisierte Summe
von gen"ugend vielen Summanden $X_i$ eine Verteilungsfunktion haben,
die jener einer Normalverteilung sehr nahe kommt, 
\begin{equation}
\frac{X-np}{\sqrt{np(1-p)}}
\label{chi2-primitiv}
\end{equation}
ist also angen"ahert standardnormalverteilt. Daraus liesse sich nach den
Ideen des voranstehenden Abschnitts bereits ein Test konstruieren.
Etwas handlicher ist jedoch das Quadrat von (\ref{chi2-primitiv}), der
sich mit Hilfe der Anzahl der F"alle $Y=n-X$, in denen das Ereignis
nicht eingetreten ist, noch etwas symmetrischer Schreiben l"asst
\begin{eqnarray}
D=\frac{(X-np)^2}{np(1-p)}
&=&
\frac{(X-np)^2}{n}\cdot \frac{1}{p(1-p)}\nonumber\\
&=&
\frac{(X-np)^2}{n}\cdot
\biggl(
\frac{1-p}{p(1-p)} +
\frac{p}{p(1-p)}
\biggr)\nonumber\\
&=&\frac{(X-np)^2}{np}+\frac{(X-np)^2}{n(1-p)}\nonumber\\
&=&\frac{(X-np)^2}{np}+\frac{(n-X-n(1-p))^2}{n(1-p)}\label{chi2-primitiv2}
\end{eqnarray}
Im letzten Ausdruck (\ref{chi2-primitiv2})
steht im ersten Term die Abweichung der
Anzahl der Eintretensf"alle des Ereignisses von der erwarteten H"aufigkeit,
im zweiten Term die Anzahl der Nichteintretensf"alle des Ereignisses
von deren erwarteter H"aufigkeit.
Da wir vom Quadrat einer standardnormalverteilten Zufallsvariable ausgegangen
sind, ist die Diskrepanz $D$ $\chi^2$-verteilt mit einem Freiheitsgrad.

Um die Hypothese zu testen, dass das Ereignis mit Wahrscheinlichkeit $p$
eintritt, brauchen wir nur gen"ugend viele Beobachtungen durchzuf"uhren
(so dass die Approximation durch den zentralen Grenzwertsatz g"ultig wird),
die Diskrepanz $D$ auszurechnen und mit Hilfe der $\chi^2$-Verteilung
zu pr"ufen, ob dieser Wert von $D$ ``unwahrscheinlich genug'' ist.

F"ur einen Test auf dem Niveau $\alpha=0.01$ m"ussten wir jenes
$x$ finden, f"ur welches $F_{\chi^2_1}(x)=0.99$ gilt. Die $\chi^2$-Tabelle
\ref{chi2-tabelle} im Anhang \ref{anhang-tabellen}
liefert $x=6.63$. Sobald also $D\ge 6.63$ w"urde die Hypothese verworfen,
dass $p$ nicht die Wahrscheinlichkeit des Ereignisses $A$ ist.

In einem Experiment wurde eine M"unze 30 mal geworfen, wobei 13 mal
Kopf erschien und 17 mal Zahl. Die Hypothese $p=0.5$ f"uhrt auf eine
Diskrepanz 
$$D=\frac{(13-15)^2}{15}+\frac{(17-15)^2}{15}=\frac{8}{15}=0.53333,$$
also viel zu klein, um an der Hypothese zu zweifeln. Nat"urlich beweist
das auch nicht, dass tats"achlich $p=0.5$. Berechnen wird die Diskrepanz
f"ur irgend ein $p$, dann gilt
$$D(p)=\frac{(13-30p)^2}{30p}+\frac{(17-30(1-p))^2}{30(1-p)}
=\frac{(13-30p)^2}{30p(1-p)}.$$
Die vorhandenen Beobachtungen bringen uns dazu, die Hypothese eines
bestimmten Wertes von $p$ auf dem Niveau $\alpha=0.01$ zu verwerfen,
wenn 
$D(p)\ge 6.63$ ist. Selbst f"ur $p=0.3$ findet man immer noch einen
Wert $D(0.3)=2.54$, welcher nicht gestattet,
die Hypothese $P(\text{Kopf})=0.3$ zu verwerfen. Hingegen liefert 
$D(0.7)=10.16\ge 6.63$ Grund genug, auf dem Niveau $\alpha=0.01$ nicht
l"anger an die Hypothese $P(\text{Kopf})=0.7$ zu glauben.

Der Test l"asst sich "ubrigens auch umkehren. Hat der Experimentator
gemogelt, und seine Daten fabriziert, dann k"onnen wir erwarten, dass
die Diskrepanz kleiner ist, als wir erwarten d"urften. Eine Diskrepanz
von $0$, also Kopf und Zahl gleich h"aufig, ist extrem unwahrscheinlich.

\section{Testen einer diskreten Wahrscheinlichkeitsverteilung}
\label{section-testen-diskreter-wkeitsverteilung}
Karl Pearson hat im Jahre 1900 erkannt, dass sich der Test f"ur die
Wahrscheinlichkeit eines Ereignisses auf einen Test f"ur eine
diskrete Wahrscheinlichkeitsverteilung erweitern l"asst. Wir nehmen
dazu an, dass in einem Experiment $d+1$ Ausg"ange m"oglich sind, und
bezeichnen diese mit $0,\dots,d$. Das Experiment wird $n$ mal
durchgef"uhrt, wobei der Ausgang $i$ $n_i$ mal beobachtet wurde.
Mit diesen Daten m"ochten wir die Hypothese testen, dass $p_i$ die
Wahrscheinlichkeit f"ur den Ausgang $i$ ist.

In Analogie zum Test f"ur eine Wahrscheinlichkeit bilden wird die
Diskrepanz:
\begin{definition}Die Gr"osse
\begin{equation}
D=\sum_{i=0}^d\frac{(n_i-np_i)^2}{np_i} \label{formel-diskrepanz}
\end{equation}
heisst Diskrepanz.
\end{definition}
Es gilt:
\begin{satz}[Pearson] Die Diskrepanz $D$ ist f"ur gen"ugend grosse
$n$ ann"ahernd $\chi^2$-verteilt mit $d$ Freiheitsgraden.
\end{satz}
``Gen"ugend gross'' heisst nach einer verbreiteten Faustregel: so
gross, dass $n_i>5\forall i$. Damit ist anscheinend sichergestellt,
dass f"ur die Zwecke des Tests die Ann"aherung an die Normalverteilung
genau genug ist.
\begin{proof}[Beweis]
Wie beim Fall einer Wahrscheinlichkeit k"onnen wir die Terme
$$\delta_i=\frac{n_i-np_i}{\sqrt{np_i(1-p_i)}}$$
als standardnormalverteilt ansehen. Sie sind jedoch nicht
unabh"angig, da $\sum_{i=0}^d n_i=n$ und $\sum_{i=0}^dp_i=1$
gilt. Das Hauptproblem des Beweises besteht also darin zu zeigen,
dass die Diskrepanz die Quadrat-Summe von $d$ standardnormalverteilten
Zufallsvariablen ist, die sich aus den $\delta_i$ bilden lassen.
Dieses etwas technische Problem bringt uns keine neuen theoretischen
Einsichten, wir verzichten daher auf die Durchf"uhrung des Beweises.
\end{proof}

Mit diesem Satz l"asst sich offensichtlich ein Test auf dem Niveau
$\alpha$ f"ur jede
beliebige diskrete Verteilung bilden. Man geht dazu wie folgt vor:
\begin{enumerate}
\item Bestimme $x$ so, dass die $F_{\chi_{d}^2}(x)=1-\alpha$
\item Berechne 
$$D=\sum_{i=0}^d\frac{(n_i-np_i)^2}{np_i}$$
\item Verwerfe die Hypothese, dass der Ausgang $i$ mit Wahrscheinlichkeit
$p_i$ eintritt, wenn $D>x$ gilt.
\end{enumerate}

In 100 W"urfen mit einem W"urfel wurden die Augenzahlen $1$ bis $6$ gez"ahlt,
und dabei folgende H"aufigkeiten ermittelt
\begin{center}
\begin{tabular}{|r|r|r|}
\hline
$i$&$n_i$&$(n_i-np_i)^2/np_i$\\
\hline
1&21&1.12671\\
2&18&0.10668\\
3&16&0.02666\\
4&17&0.00667\\
5&15&0.16665\\
6&13&0.80664\\
\hline
&&$D=2.24001$\\
\hline
\end{tabular}
\end{center}
Die Tabelle der Quantilen der $\chi^2$-Verteilung \ref{chi2-tabelle}
zeigt, dass f"ur f"unf Freiheitsgrade (sechs m"ogliche Versuchsausg"ange)
selbst auf dem Niveau $\alpha=0.1$ die Diskrepanz mindestens 9.236 sein
m"usste. Die vorliegenden experimentellen Daten geben also noch keinen
Anlass, daran zu zweifeln, dass jede Augenzahl mit der gleichen
Wahrscheinlichkeit $\frac16$ auftreten wird.

\section{Testen einer stetigen Verteilung}
\label{section-testen-stetiger-wkeitsverteilung}
Der im vorangehenden Abschnitt beschriebene $\chi^2$-Test kann prinzipbedingt
nur diskrete Wahrscheinlichkeitsverteilungen testen. Will man eine stetige
Verteilung testen, muss man zun"achst Klassen von Werten bilden,
deren Wahrscheinlichkeiten berechnen, und dann pr"ufen, ob die so
k"unstlich konstruierte diskrete Verteilung im $\chi^2$-Test Bestand hat.
Durch die Notwendigkeit der Klassenbildung f"uhrt man eine k"unstliche
Diskretisierung in das Problem ein. Will man den Test verfeinern, muss man
die Diskretisierung ebenfalls "andern.

\subsection{Testprinzip}
Diese unbefriedigende Situation korrigiert der Kolmogoroff-Smirnov-Test,
kurz KS-Test.
Dieser vergleicht direkt die heuristische, d.h.~aus den Beobachtungen gewonnene
Verteilungsfunktion mit der erwarteten Verteilungsfunktion. Er legt fest,
wie die Unterschiede zwischen den zwei Funktionen gemesssen werden sollen,
und legt fest, wann eine Abweichung zu gross ist.

Wir wollen testen, ob $n$ Beobachtungen $X_1,\dots,X_n$ sich mit
der Hypothese vertragen, dass die Zufallsvariablen $X_i$ die
Verteilungsfunktion $F$ habent. Dazu wird aus den $n$ Beobachtungen
und die zugeh"orige empirische Verteilungsfunktion
$$F_n\colon x\mapsto F_n(x)=\frac{|\{x_i|X_i\le x\}|}{n}$$
gebildet. Wenn die Zufallsvariable $X$ tats"achlich die
Verteilungsfunktion $F$ hat, wird man erwarten,
dass $F$ und $F_n$ sich nicht stark unterscheiden.
Insbesondere sollten die Gr"ossen
\begin{eqnarray}
K_n^+&=&\sqrt{n}\max_{-\infty<x<\infty} F_n(x)-F(x)\\
K_n^-&=&\sqrt{n}\max_{-\infty<x<\infty} F(x)-F_n(x)
\end{eqnarray}
nicht sehr gross werden. Um den Test durchzuf"uhren braucht man also
die Wahrscheinlichkeit, dass $K_n^{\pm}$ einen gewissen Wert "ubersteigt.
Daf"ur gibt es Tabellen, zum Beispiel findet man im Anhang \ref{anhang-tabellen}
die Tabelle \ref{KS-tabelle}, welche zu $n$ und einer Wahrscheinlichkeit
$p$ eine Gr"osse $t_{n,p}$ angibt, f"ur die
$$P(K_n^+\le t)=t_{n,p}.$$

F"ur die Durchf"uhrung des Tests geht man wie folgt vor:
\begin{satz} Sei $X_1,\dots,X_n$ sei eine Stichprobe einer Zufallsvariable $X$.
Der Kolmogorov-Smirnov-Test auf dem Niveau $\alpha$ f"ur die Hypothese,
dass $X$ die Verteilungsfunktion $F$ hat wird wie folgt durchgef"uhrt:
\begin{enumerate}
\item Berechne
$$K_n^+ = \sqrt{n}\max_{-\infty<x<\infty} (F_n(x)-F(x))$$
\item Finde $t_{n,1-\alpha}$ in der Tabelle \ref{KS-tabelle}
\item Falls
$$K_n^+>t_{n,1-\alpha}$$
verwerfe die Hypothese, dass $X$ die Verteilungsfunktion $F$ hat.
\end{enumerate}
\end{satz}

\subsection{Berechnung von $K_n^{\pm}$}
Das Maximum der Differenz $F_n(x)-F(x)$ tritt zwangsl"aufig an einer
Sprungstelle von $F_n$ auf, also bei einem der Werte der Stichprobe
$X_1,\dots,X_n$. Durch sortieren k"onnen wir erreichen, dass die
$X_i$  in aufsteigender Reihenfolge angeordnet sind, also $X_i\le X_j$
f"ur $i\le j$. Dann ist $F_n(X_j)=\frac{j}{n}$, und
die  $K_n^{\pm}$ k"onnen mit Hilfe der einfacheren Formeln
\begin{eqnarray}
K_n^+&=&\sqrt{n}\max_{1\le j\le n}\biggl(\frac{j}{n}-F(X_j)\biggr)
\label{knp-berechnungs-formel}
\\
K_n^-&=&\sqrt{n}\max_{1\le j\le n}\biggl(F(X_j)-\frac{j-1}n\biggr)
\end{eqnarray}
berechnet werden. In dieser Form ist die Berechnung der
Verteilung von $K_n^{\pm}$ leichter m"oglich.

\subsection{Reduktion auf Gleichverteilung}
Das Problem w"are etwas handlicher, wenn die Verteilungsfunktion $F$
die Verteilungsfunktion einer Gleichverteilung w"are.
Der folgende Satz erm"oglicht, eine
beliebige Zufallsvariable mit stetiger Verteilungsfunktion in eine
gleichverteilte Zufallsvariable zu transformieren.
\begin{satz}\label{reduktion-auf-gleichverteilung}
Sei $X$ eine Zufallsvariable mit stetiger Verteilungsfunktion $F$, dann
ist $F(X)$ eine auf $[0,1]$ gleichverteilte Zufallsvariable.
\end{satz}
\begin{proof}[Beweis]
Um die Verteilungsfunktion in $F(X)$ zu bestimmen, m"ussen wir $P(F(X)\le y)$
f"ur ein beliebiges $y\in[0,1]$ berechnen. Da $F$ stetig ist, gibt es ein
gr"osstes $x$, f"ur welches $F(x)= y$, zum Beispiel
$x=\max\{\xi|F(\xi)= y\}$. Mit diesem $x$ folgt
$$P(F(X)\le y)=P(X\le x)=F(x)=y.$$
\end{proof}
Aus dem Satz folgt, dass wir die Zahlen $Y_j=nF(X_j)$ darauf pr"ufen wollen,
ob sie im Intervall $[0,n]$ gleichverteilt sind.

\subsection{Berechnung der Verteilung von $K_n^{\pm}$}
Die Formel (\ref{knp-berechnungs-formel})
f"ur $K_n^+$ kann mit Hilfe der $Y_j$ wie folgt 
geschrieben werden
\begin{equation}
K_n^+=\frac1{\sqrt{n}}\max(1-Y_1, 2-Y_2,\dots,n-Y_n).
\end{equation}
Die Wahrscheinlichkeit, dass $K_n^+\le t/\sqrt{n}$ ist also die
Wahrscheinlichkeit, dass $j-Y_j\le t$ f"ur alle $j$, oder
$Y_j\ge j-t$ f"ur $1\le j\le n$. Da die $Y_j$ gleichverteilt sind,
kann man diese Wahrscheinlichkeit berechnen.

Wir berechnen zun"achst die Wahrscheinlichkeit, dass die Bedingung
f"ur $Y_n$ erf"ullt ist unter der Annahme, dass die "ubrigen
Bedingungen erf"ullt sind. $Y_n$ muss im Intervall $[0,n]$
liegen, darf aber auch nicht kleiner als $n-t$ sein. Setzt
man $\alpha_j=\max(j-t,0)$, dann heisst das, dass $Y_n$ im
Intervall $[\alpha_n, n]$ liegen muss. Ausserdem ist $Y_n$ gleichverteilt,
somit ist die Wahrscheinlichkeit, dass alle Bedingungen f"ur $Y_n$ erf"ullt
sind
\begin{equation}
P(\alpha_n\le Y_n\le n)=\frac{\int_{\alpha_n}^n dy_n}{\int_0^ndy_n}
\label{kpn-erstes-integral}
\end{equation}
Der Nenner dient dazu, die Wahrscheinlichkeit auf $1$ zu normieren.

Nun betrachten wir die analogen Bedingungen f"ur $Y_{n-1}$. Offensichtlich
kann $Y_{n-1}$ zwischen $\alpha_{n-1}$ und $Y_n$ variieren. Es ergibt sich
als Wahrscheinlichkeit, daf"ur, dass die Bedingungen an $Y_{n-1}$ erf"ullt
sind, der Ausdruck
\begin{equation}
\frac{\int_{\alpha_{n-1}}^{y_n}dy_{n-1}}{\int_0^{y_n}dy_{n-1}}.
\label{kpn-zweites-integral}
\end{equation}
In (\ref{kpn-erstes-integral}) hatten wir angenommen,
dass die Bedingungen f"ur $Y_{n-1}$
bereits erf"ullt sind, also die Wahrscheinlichkeit f"ur deren zutreffen $1$
ist, inzwischen haben wir gelernt, dass diese Wahrscheinlichkeit in
Wahrheit eher wie (\ref{kpn-zweites-integral}) aussieht.
Zusammen finden wir
\begin{equation}
P(\alpha_n\le Y_n\le n\wedge \alpha_{n-1}\le Y_{n-1}\le Y_n)
=
\frac{\int_{\alpha_n}^n dy_n\int_{\alpha_{n-1}}^{y_n}dy_{n-1}}{\int_0^ndy_n\int_0^{y_n}dy_{n-1}}
\label{kpn-zwei-integrale}
\end{equation}
In diesem Sinne k"onnen schrittweise die Bedingungen an $Y_{n-2},\dots,Y_1$
erf"ullt werden, wir erhalten
\begin{equation}
\frac{\int_{\alpha_n}^n dy_n\int_{\alpha_{n-1}}^{y_n}dy_{n-1}\dots\int_{\alpha_1}^{y_2}dy_1}{\int_0^ndy_n\int_0^{y_n}dy_{n-1}\dots\int_0^{y_2}dy_1}
\label{kpn-alle-integrale}
\end{equation}
Diese Integrale sind nicht ganz selbstverst"andlich auszurechnen, weshalb
wir dies hier schrittweise durchf"uhren.

\begin{satz}
\label{kn-elementarvolumen}
Es gilt
\begin{equation}
\int_0^xdy_n\int_0^{y_n}dy_{n-1}\dots\int_0^{y_2}dy_1=\frac{x^n}{n!}
\end{equation}
\end{satz}
\begin{proof}[Beweis] Wir f"uhren den Beweis mit vollst"andiger
Induktion. F"ur $n=1$ muss das Integral
$$\int_0^xdy_1=[y_1]_0^x=x$$
berechnet werden, was offensichtlich mit $\frac{x^1}{1!}=x$
"ubereinstimmt.

Nehmen wir an, dass die Formel f"ur $n-1$ stimmt, dann k"onnen wir den
Fall $n$ wie folgt verifizieren:
$$
\int_0^xdy_n\int_0^{y_n}dy_{n-1}\dots\int_0^{y_2}dy_1
=
\int_0^x\frac{y_n^{n-1}}{(n-1)!}\,dy_n
=
\left[\frac{y_n^n}{n!}\right]_0^x=\frac{x^n}{n!}
$$
womit die Behauptung bewiesen ist.
\end{proof}
Der Nenner in (\ref{kpn-alle-integrale}) ist also $\frac{n^n}{n!}$,
dabei ist $n^n$ das
Volumen eines $n$-dimensionalen W"urfels mit Kantenl"ange $n$, dies wird
durch $n!$ geteilt, weil die $Y_j$ solange vertauscht werden m"ussen, bis
sie aufsteigend geordnet sind. Nur eine der $n!$ Permutationen der $Y_j$
erf"ullt diese Bedingung.

Wir k"ummern uns nun um den Z"ahler von (\ref{kpn-alle-integrale}).
Die unteren Grenzen der
Integrale sind $\alpha_j=\max(j-t,0)$. Bei gegebenem $t$ wird $\alpha_j=j-t$
sein f"ur gen"ugend grosse $j$, f"ur alle anderen wird $\alpha_j=0$ sein.
Es gibt also eine Zahl $k$ so dass der Z"ahler von (\ref{kpn-alle-integrale})
die Form
$$P_{nk}(x)=\int_{n-t}^xdx_n\int_{n-1-t}^{x_n}dx_{n-1}\dots\int_{k+1-t}^{x_{k+2}}dx_{k+1}\int_0^{x_{k+1}}dx_k\dots\int_0^{x_2}dx_1$$
hat, es ist dies das gr"osste $k$, f"ur welches $k-t\le 0$ ist, also
$k=\lfloor t\rfloor$. Der Z"ahler von (\ref{kpn-alle-integrale})
ist also $P_{n\lfloor t\rfloor}(n)$.
Mit Hilfe der folgenden S"atze berechnen wir $P_{nk}(x)$ schrittweise.

Wir halten zun"achst noch den Spezialfall
\begin{equation}
P_{nn}(x)=\frac{x^n}{n!}
\label{spezialfall-pnn}
\end{equation}
fest, der unmittelbar aus dem Satz \ref{kn-elementarvolumen} folgt.

\begin{satz}
\label{kn-variablentransformation}
Es gilt
\begin{equation}
P_{nk}(x)=\int_{n}^{x+t}dx_n\int_{n-1}^{x_n}dx_{n-1}\dots\int_{k+1}^{x_{k+2}}dx_{k+1}\int_t^{x_{k+1}}dx_k\dots\int_t^{x_2}dx_1
\label{kpn-variablen-transformation}
\end{equation}
\end{satz}
\begin{proof}[Beweis]
Wir substituieren $x_i+t=\xi_i$. Dazu m"ussen zu den Intervallgrenzen
$t$ hinzuaddiert werden:
\begin{eqnarray}
P_{nk}(x)
&=&\int_{n-t}^xdx_n\int_{n-1-t}^{x_n}dx_{n-1}\dots\int_{k+1-t}^{x_{k+2}}dx_{k+1}\int_0^{x_{k+1}}dx_k\dots\int_0^{x_2}dx_1\nonumber\\
&=&\int_{n}^{x+t}d\xi_n\int_{n-1}^{x_n+t}d\xi_{n-1}\dots\int_{k+1}^{x_{k+2}+t}dx_{k+1}\int_t^{x_{k+1}+t}d\xi_k\dots\int_t^{x_2+t}d\xi_1\nonumber\\
&=&\int_{n}^{x+t}d\xi_n\int_{n-1}^{\xi_n}d\xi_{n-1}\dots\int_{k+1}^{\xi_{k+2}}dx_{k+1}\int_t^{\xi_{k+1}}d\xi_k\dots\int_t^{\xi_2}d\xi_1\nonumber\\
&=&\int_{n}^{x+t}dx_n\int_{n-1}^{x_n}dx_{n-1}\dots\int_{k+1}^{x_{k+2}}dx_{k+1}\int_t^{x_{k+1}}dx_k\dots\int_t^{x_2}dx_1 \label{kpn-umbenennung}
\end{eqnarray}
Im letzten Schritt (\ref{kpn-umbenennung}) haben wir die Integrationsvariablen
wieder auf die vertrauten $x_1,\dots,x_n$ umbenannt.
\end{proof}
Besonders einfach sind die F"alle $k=0$, da dann keine Integral mit
unterer Grenze $t$ auftreten. Der Parameter $t$ tritt in diesem Fall nur
in der Grenze des letzten Integrals auf:

\begin{satz}F"ur $n>0$ gilt
\begin{equation}
P_{n0}(x)=\frac{(x+t)^n}{n!}-\frac{(x+t)^{n-1}}{(n-1)!}
\end{equation}
\end{satz}
\begin{proof}[Beweis]
Mittels vollst"andiger Induktion. F"ur $n=1$ gilt
$$P_{10}(x)=\int_1^{x+t}dx_1=[x_1]_1^{x+t}=(x+t)-1=\frac{(x+t)^1}{1!}-\frac{(x+t)^0}{0!}.$$
Nehmen wir an, die Behauptung w"are f"ur $n-1$ bereits bewiesen, dann
berechnen wir $P_{n0}(x)$ wie folgt:
\begin{eqnarray*}
P_{n0}(x)
&=&\int_n^{x+t}dx_n\int_{n-1}^{x_n}dx_{n-1}\dots\int_1^{x_2}dx_1\\
&=&\int_n^{x+t}dx_n\,P_{n-1,0}(x_n-t)\\
&=&\int_n^{x+t}\frac{x_n^{n-1}}{(n-1)!}-\frac{x_n^{n-2}}{(n-2)!}\,dx_n\\
&=&\biggl[\frac{x_n^n}{n!}-\frac{x_n^{n-1}}{(n-1)!}\biggr]_n^{x+t}\\
&=&
\frac{(x+t)^n}{n!}-\frac{(x+t)^{n-1}}{(n-1)!}
-\frac{n^n}{n!}+\frac{n^{n-1}}{(n-1)!}\\
&=&
\frac{(x+t)^n}{n!}-\frac{(x+t)^{n-1}}{(n-1)!}
\end{eqnarray*}
wegen
$$\frac{n^n}{n!}=\frac{n^{n-1}n}{(n-1)! n}=\frac{n^{n-1}}{(n-1)!},$$
womit der Satz bewiesen ist.
\end{proof}

Der Fall $P_{n0}(x)$ bezieht sich auf kleine Werte von $t$. Sobald 
$t\ge 1$ ist, werden Integrale mit $t$ als unterer Grenze auftreten,
diese lassen sich jedoch rekursiv aus den bereits berechneten
Integralen bestimmen.

\begin{satz}\label{kn-rekursion}
F"ur $1\le k\le n$ gilt
\begin{equation}
P_{nk}(x)-P_{n,k-1}(x)
=
\frac{(k-t)^k}{k!}P_{n-k,0}(x-k)
\end{equation}
\end{satz}

\begin{proof}[Beweis]
Wir schreiben
$$\Delta_{nk}(x)= P_{nk}(x)-P_{n,k-1}(x)
$$
dann gilt
\begin{eqnarray*}
\Delta_{nk}(x)
&=&
\int_n^{x+t}dx_n\int_{n-1}^{x_n}dx_{n-1}\dots\int_{k+1}^{x_{k+2}}dx_{k+1}
\int_t^{x_{k+1}}dx_k\dots\int_t^{x_2}dx_1\\
&-&\int_n^{x+t}dx_n\int_{n-1}^{x_n}dx_{n-1}\dots\int_{k+1}^{x_{k+2}}dx_{k+1}
\int_{k}^{x_{k+1}}dx_k\dots\int_t^{x_2}dx_1\\
&=&
\int_n^{x+t}dx_n\int_{n-1}^{x_n}dx_{n-1}\dots\int_{k+1}^{x_{k+2}}dx_{k+1}
\int_{t}^{k}dx_k\dots\int_t^{x_2}dx_1
\end{eqnarray*}
Da das Integral "uber $x_k$ feste Grenzen $t$ und $k$ hat, reduziert es
sich auf eine Konstante $c_k$, welche wir weiter unten berechnen werden.
Die verbleibenden Integrale sind
\begin{eqnarray*}
\Delta_{nk}(x)
&=&
c_k\int_n^{x+t}dx_n\int_{n-1}^{x_n}dx_{n-1}\dots\int_{k+1}^{x_{k+2}}dx_{k+1}\\
&=&
c_k\int_{n-k}^{x-k+t}dx_{n-k}\int_{n-k-1}^{x_{n-k}}dx_{n-k-1}\dots\int_1^{x_2}dx_1\\
&=&c_kP_{n-k,0}(x-k)
\end{eqnarray*}
dabei haben wir die selbe Variablentransformation vorgenommen wie im
Beweis von Satz \ref{kn-variablentransformation}, und ausserdem haben
wir die Integrationsvariablen von $x_n,\dots,x_{k+1}$ in $x_{n-k},\dots,x_1$
umbenannt.

Wir m"ussen uns noch um die Konstante $c_k$ k"ummern.
Aus der Definition folgt mit Hilfe von Satz \ref{kn-elementarvolumen}
\begin{equation}
c_k = \int_{t}^{k}dx_k\dots\int_t^{x_2}dx_1=P_{kk}(k-t)=\frac{(k-t)^k}{k!},
\end{equation}
die Behauptung.
\end{proof}
Mit den eben bewiesenen S"atzen l"asst sich jetzt jedes beliebige 
$P_{nk}(x)$ berechnen.

\begin{satz}\label{pnk-allgemein}
\begin{equation}
P_{nk}(x)
=
\sum_{0\le r\le k}\frac{(r-t)^r}{r!}
\cdot
\frac{(x-r+t)^{n-r-1}}{(n-r)!}
(x-n+t)
\end{equation}
\end{satz}
\begin{proof}[Beweis]
Um $P_{nk}(x)$ zu berechnen schreiben wir
\begin{eqnarray*}
P_{nk}(x)
&=&\sum_{1\le r\le k}(P_{nr}(x)-P_{n,r-1}(x))+P_{n0}(x)\\
&=&\sum_{1\le r\le k}\frac{(r-t)^r}{r!}P_{n-r,0}(x-r)+P_{n0}(x)\\
&=&\sum_{1\le r\le k}\frac{(r-t)^r}{r!}\biggl(
\frac{(x-r+t)^{n-r}}{(n-r)!}-
\frac{(x-r+t)^{n-r-1}}{(n-r-1)!}\biggr)\\
&&+\frac{(x+t)^{n}}{n!}- \frac{(x+t)^{n-1}}{(n-1)!}\\
&=&\sum_{0\le r\le k}\frac{(r-t)^r}{r!}\biggl(
\frac{(x-r+t)^{n-r}}{(n-r)!}-
\frac{(x-r+t)^{n-r-1}}{(n-r-1)!}\biggr)\\
&=&\sum_{0\le r\le k}\frac{(r-t)^r}{r!}
\cdot
\frac{(x-r+t)^{n-r-1}}{(n-r-1)!}
\biggl( \frac{x-r+t}{n-r}-1 \biggr)\\
&=&\sum_{0\le r\le k}\frac{(r-t)^r}{r!}
\cdot
\frac{(x-r+t)^{n-r-1}}{(n-r-1)!}
\frac{x-n+t}{n-r}\\
&=&\sum_{0\le r\le k}\frac{(r-t)^r}{r!}
\cdot
\frac{(x-r+t)^{n-r-1}}{(n-r)!}
(x-n+t)
\end{eqnarray*}
\end{proof}
Die Verteilungsfunktion f"ur $K_n^+$ findet man jetzt, indem man $x=n$
einsetzt:

\begin{satz}\label{kn-verteilung}
F"ur die Verteilungsfunktion von $K_n^+$ gilt
\begin{equation}
P(K_n^+\le t/\sqrt{n})=\frac{t}{n^n}\sum_{0\le r\le t}\binom{n}{r}(r-t)^r(t+n-r)^{n-r-1}
\end{equation}
\end{satz}
\begin{proof}[Beweis]
Die gesuchte Wahrscheinlichkeit ist $P_{n\lfloor t\rfloor}(n)/P_{nn}(n)$,
damit folgt die Behauptung aus Satz \ref{pnk-allgemein}
\end{proof}

\section{$t$-Test} \label{section-t-test}
Bei der Herstellung eines Produktes sind Verbesserungen durchgef"uhrt worden,
so dass sich ein charakteristischer Parameter des Produktes verbessert hat.
In der Qualtit"atssicherung wird der Parameter durch Messungen in einer
Stichprobe ermittelt. Die vor der Verbesserung gemessene Stichprobe
$X_1,\dots,X_n$
einer normalverteilten Zufallsvariable $X$ liefert den Mittelwert
$\bar X=\frac1n(X_1+\dots+X_n)$, nach
der Verbesserung liefert die Stichprobe $Y_1,\dots,Y_m$ der normalverteilten
Zufallsvariable $Y$ den Mittelwert
$\bar Y=\frac1m(Y_1+\dots+Y_m)$. Selbst wenn $\bar X$ und $\bar Y$
verschieden sind: Kann man daraus wirklich schon schliessen, dass die
Verbesserung eine "Anderung des Parameters gebracht hat, oder k"onnte
das nicht auch einfach nur eine zuf"allige Schwankung sein?

Offensichtlich geht es hier darum die Hypothese zu testen, dass sich
durch die Verbesserung nichts ge"andert hat. Das Beobachtungsmaterial
soll dann entscheiden, ob die Hypothese noch haltbar ist. Ist sie es
auf dem Niveau $\alpha$ nicht,
d"urften wir schliessen, dass die Verbesserung tats"achlich eine "Anderung
des Parameters gebracht hat, und wir w"urden uns h"ochstens mit 
Wahrscheinlichkeit $\alpha$ irren.

Wir nehmen an, dass die Streuung der Messwerte der Stichprobe durch die
Ver"anderung des Produktionsprozesses nicht ver"andert wurde. Aber durch
die zus"atzlichen Messungen d"urfte die Varianz genauer bekannt sein.
Daher bildet man aus der Stichprobenvarianz $S_X^2$ und $S_Y^2$
die gepoolte Stichprobenvarianz
\begin{equation}
S_p^2=\frac{(n-1)S_X^2+(m-1)S_Y^2}{m+n-2},
\label{pooled-variance}
\end{equation}
sie ist ein verbessertes Mass f"ur die Messfehler.

Man m"ochte nun die die Hypothese testen, dass $\delta=\mu-\nu=0$, wobei
$\mu=E(X)$ und $\nu=E(Y)$ ist. Dazu braucht man offensichtliche einen
Sch"atzer f"ur $\delta$, und nicht wirklich "uberraschend ist
$\hat\delta=\bar X-\bar Y$ ein erwartungstreuer Sch"atzer f"ur $\delta$.
Man kann weiter zeigen, dass $S_p^2$ ein erwartungstreuer Sch"atzer
f"ur die gemeinsame Varianz von $X$ und $Y$ ist. Unter der Annahme der
Hypothese ist $\hat\delta$ eine normalverteilte Zufallsvariable
mit Varianz $\sigma^2(1/n+1/m)$ ist. $S_p^2(1/n+1/m)$ ist $\chi^2$-verteilt
mit $(m+n-2)$ Freiheitsgraden. Somit ist 
\begin{equation}
T=\frac{\hat\delta}{S_p\sqrt{1/n+1/m}}
=\frac{\bar X-\bar Y}{\sqrt{(n-1)S_X^2+(m-1)S_Y^2}}\sqrt{\frac{nm(n+m-2)}{n+m}}
\label{t-test-ausdruck}
\end{equation}
$t$-verteilt mit $m+n-2$ Freiheitsgraden.

Der $t$-Test auf Gleichheit der Erwartungswerte spielt sich also wie folgt
ab. Aus den beiden Stichproben $X_1,\dots,X_n$ und $Y_1,\dots,Y_m$ wird
der Ausdruck $T$ (\ref{t-test-ausdruck}) gebildet.
"Ubersteigt $T$ den $t$-Wert f"ur die $1-\alpha$-Quantile der $t$-Verteilung
mit $n+m-2$ Freiheitsgraden, wird die Hypothese $\mu=\nu$ verworfen.

\subsubsection{War der Juli 2003 wirklich besonders warm?\label{julitemperaturen}}
Aus den Messdaten der Wetterstation in Altendorf soll entschieden werden,
ob der Juli 2003 wirklich signifikant w"armer war als dr Juli 2004.
Dazu werden die im Laufe des Monats gemessenen Temperaturwerte
ermittelt:

\begin{center}
\begin{tabular}{|r|r|r|r|r|}
\hline
Jahr&$T$&$T^2$&$\operatorname{var}(T)$&$n$\\
\hline
2001&20.351&440.855&26.659&44540\\
2002&19.906&415.302&19.051&44086\\
2003&21.638&493.230&25.023&44473\\
2004&19.582&406.576&23.102&44589\\
2005&19.861&419.796&25.319&44637\\
2006&24.009&603.028&26.573&44623\\
\hline
\end{tabular}
\end{center}

Um einen signifikanten Unterschied der Temperatur nachweisen zu k"onnen,
m"ussen wir f"ur jedes Paar von Jahren den Ausdruck (\ref{t-test-ausdruck})
berechnen, wir erhalten dabei folgende Tabelle:

\begin{center}
\begin{tabular}{|r|rrrrrr|}
\hline
&2001&2002&2003&2004&2005&2006\\
\hline
2001& & 13.849& -37.767& 23.013& 14.352& -105.860\\
2002& -13.849& & -54.881& 10.505& 1.422& -127.863\\
2003& 37.767& 54.881& & 62.542& 52.864& -69.666\\
2004& -23.013& -10.505& -62.542& & -8.469& -132.656\\
2005& -14.352& -1.422& -52.864& 8.469& & -121.646\\
2006& 105.860& 127.863& 69.666& 132.656& 121.646& \\
\hline
\end{tabular}
\end{center}

Da die Zahl der Freiheitsgrade sehr gross ist, m"ussen wir
uns nicht um die genaue Anzahl k"ummern, sondern k"onnen stattdessen
die Grenzwerte f"ur unendlich viele Freiheitsgrade verwenden. Auf
dem Niveau $\alpha=0.01$ finden wir $t_{\infty}^{0.005}=2.576$.
Da alle Eintr"age in der Tabelle bis auf das Paar $(2002,2005)$
betragsm"assig gr"osser sind, k"onnen wir auf dem Niveau $\alpha=0.01$
schliessen, dass die Juli-Durschnittstemperatur zwischen
2001 und 2006 verschieden war. Nur f"ur die Jahre 2002 und 2005
k"onnen wir den Unterschied nicht auf dem Niveau 0.01 sicherstellen.
Da jedoch $t_{\infty}^{0.1}=1.282$ k"onnten wir auf dem Niveau
$0.3$ beweisen, dass die Juli-Durchschnittstemperatur in den
Jahren 2002 und 2005 verschieden war, wir w"urden dabei aber 
mit Wahrscheinlichkeit $0.3$ einen Fehler erster Art machen.

\section{$F$-Test}
In der Diskussion des $t$-Tests haben wir f"ur die dort beschriebene
Situation postuliert, dass die Varianzen von $X$ und $Y$ gleich sind.
Wie k"onnte man dies testen? Gibt es einen Test, der die Hypothese
$\operatorname{var}(X)=\operatorname{var}(Y)$ testet? 

Bei der Entwicklung eines Messger"ates wurde eine Verbesserung angebracht,
die angeblich die Messgenauigkeit erh"oht. Der Messung einer Referenzgr"osse
mit dem urspr"unglichen Messger"at entspreche die normalverteilte
Zufallsvariable $X$ mit Varianz $\sigma_X^2$, die Messung mit dem verbesserten
Messger"at entspreche der ebenfalls normalverteilten Zufallsvariablen
$Y$ mit Varianz $\sigma_Y^2$. Ob die m"oglicherweise kostspielige 
Verbesserung tats"achlich etwas gebracht hat, k"onnte ein Test aufdecken,
der die Hypothese $\sigma_X=\sigma_Y$ testet.

Da die Varianz positiv ist, bietet sich $\gamma=\sigma_X^2/\sigma_Y^2$ als
Testgr"osse an. Der Test f"ur $\sigma_X=\sigma_Y$ ist dann einfach nur
ein Test auf $\gamma=1$.

Offensichtlich brauchen wir einen Sch"atzer f"ur $\gamma$, ein solcher ist
schnell gefunden:
\begin{equation}
\hat\gamma=\frac{S_X^2}{S_Y^2}.
\label{ftest-gamma-schaetzer}
\end{equation}
Um einen Test zu konstruieren, muss jetzt nur noch die Verteilung von
$\hat\gamma$ ermittelt werden.

Die Gr"ossen $(n-1)S_X^2/\sigma_X^2$ und $(m-1)S_Y^2/\sigma_Y^2$ sind jeweils
$\chi^2$-verteilte Zufallsvariablen mit $n-1$ bzw.~$m-1$ Freiheitsgraden.
Per Definition ist der Quotient zweier $\chi^2$-verteilter Zufallsvariablen
$F$-verteilt:
\begin{definition}
Sind $X$ und $Y$ $\chi^2$-verteilte Zufallsvariable mit $n$ bzw.~$m$
Freiheitsgraden, dann heisst die Verteilung von $X/Y$ eine $F$-Verteilung
mit $n,m$-Freiheitsgraden, auch $F_{n,m}$ geschrieben.
\end{definition}
Der Quotient der genannten Gr"ossen ist also
\begin{equation}
\frac{(n-1)S_X^2\sigma_Y^2}{(m-1)S_Y^2\sigma_X^2}
=\frac{n-1}{m-1}\frac{\hat\gamma}{\gamma}
\end{equation}
$F_{n-1,m-1}$-verteilt.

Unter der Hypothese ist $\sigma_X=\sigma_Y$, also $\gamma=1$.
Der Test auf dem Niveau $\alpha$ wird also wie folgt durchgef"uhrt.
Zun"achst berechnet man die Sch"atzung $\hat\gamma=S_X^2/S_Y^2$. Dann
findet man in der Tabelle der Quantilen der $F$-Verteilung mit $n-1,m-1$
Freiheitsgraden den kritischen Wert $F_{n-1,m-1}^\alpha$. Ist
$\hat\gamma>F_{n-1,m-1}^\alpha$, wird die Hypothese verworfen,
d.h.~man muss davon ausgehen, dass die beiden Zufallsvariablen verschiedene
Varianz haben.

\subsubsection{Sind die Temperaturvarianzen gleich?}
Im Abschnitt \ref{julitemperaturen} haben wir die
Juli-Durchschnittstemperaturen mit Hilfe des $t$-Tests untersucht.
Dabei haben wir nicht untersucht, ob die Varianzen, die f"ur den
$t$-Test ja gleich sein m"ussen, auch wirklich gleich sein k"onnten.
Inzwischen haben wir gelernt, dass genau dies mit dem $F$-Test getestet
werden kann, und holen dies noch nach.

Zun"achst berechnen wir die $F$-Werte aus der Tabelle der Temperatur-Varianzen
aus Abschnitt \ref{julitemperaturen}

\begin{center}
\begin{tabular}{|r|rrrrrr|}
\hline
&2001&2002&2003&2004&2005&2006\\
\hline
2001&      & 1.414& 1.067& 1.153& 1.051& 1.001\\
2002& 0.707&      & 0.755& 0.815& 0.743& 0.708\\
2003& 0.937& 1.325&      & 1.080& 0.985& 0.939\\
2004& 0.868& 1.226& 0.926&      & 0.911& 0.869\\
2005& 0.952& 1.346& 1.016& 1.097&      & 0.953\\
2006& 0.999& 1.412& 1.066& 1.151& 1.049&      \\
\hline
\end{tabular}
\end{center}

Die Interpretation dieser Zahlen gestaltet sich etwas schwierig: F"ur derart
viele Freiheitsgrade l"asst sich $F_{n,k}$ wegen der darin
vorkommenden $\Gamma$-Funktionen fast nicht berechnen, der kritische Wert
ist jedenfalls kleiner als $1.22$. Dies bedeutet, dass in etwa f"unf der
f"unfzehn Paare die Hypothese verworfen werden muss, dass also die Varianzen
verschieden sind. Damit ist nat"urlich auch die G"ultigkeit der Schlussweise
beim Vergleich der Juli-Durchschnittstemperaturen in Frage gestellt.
