%
% verteilung.tex -- Kapitel über den Verteilungsbegriff
%
% (c) 2006-2015 Prof. Dr. Andreas Mueller, Hochschule Rapperswil
%
\rhead{Verteilung}
\chapter{Wahrscheinlichkeitsverteilung} \label{chapter-wahrscheinlichkeitsverteilung}

Das Kapitel~\ref{chapter-erwartungswert-und-varianz} hat gezeigt, dass die
Begriffe Erwartungswert und Varianz n"utzlich sind, das Verhalten von
Zufallsvariablen zu beschreiben.
Insbesondere konnten wir zum Beispiel verstehen, wie die Mittelung von
Messwerten deren Qualit"at verbessert, und wir konnten sogar quantifizieren,
mit welcher Geschwindigkeit ($1/\sqrt{n}$) dies geschieht.
Und mit der linearen Regression haben wir ein Werkzeug kennengelernt, 
um Abh"angigkeiten zwischen Gr"ossen zu detektieren, zu quantifizieren
und deren Qualit"at zu bewerten.

All dies war m"oglich, ohne dass wir allzu viele Details "uber die
Wahrscheinlichkeiten wissen mussten, wir mussten einzig in der Lage sein,
den Erwartungswert und die Varianz zu berechnen.
Gelungen ist uns dies allerdings erst f"ur diskrete Zufallsvariablen,
die nur endlich viele verschiedene Werte annehmen.
F"ur Zufallsvariable, die jeden beliebigen Wert in einem Intervall annehmen
k"onnen, funktioniert die bisher verwendete Formel
``$\text{Wert}\times\text{Wahrscheinlichkeit}$''
nicht.

Etwas Mitschuld an diesem Versagen tr"agt die Tatsache, dass das Modell
$X\colon\Omega\to\mathbb R$ einer Zufallsvariable zwar die gesamte verf"ugbare
Information "uber einen Zufallsprozess enth"alt, aber f"ur die Berechnung 
von Erwartungswerten eigentlich sogar zu viel Information.
Ziel dieses Kapitels ist daher, das Modell so zu vereinfachen, dass
es gerade noch ausreicht, Erwartungswerte zu berechnen.
Dabei werden wir andere Informationen aufgeben m"ussen,
zum Beispiel ob gewisse Zufallsvariablen abh"angig sind.

F"ur die Beschreibung kontinuierlicher Ph"anomene hat die Analysis
eine grosse Zahl m"achtiger Werkzeuge bereitgestellt.
Der Plan ist daher, die in $\Omega$, $P$ und $X$ codierte
Information zu reduzieren auf eine einzige Funktion $F\colon\mathbb R\to\mathbb R$,
die Verteilungsfunktion.
Die Funktion $F$ soll nur noch die Information enthalten, wie wahrscheinlich
einzelne Werte oder Werte-Intervalle sind.
Auf die Funktion $F$ wollen wir dann die Analysis anwenden um 
Erwartungswert zu berechnen.

\section{Wahrscheinlichkeitsverteilung}
\begin{figure}
\begin{center}
\includegraphics[width=0.7\hsize]{images/verteilungsfunktion-1}
\end{center}
\caption{"Ubergang von $\Omega$ zu Werten
der Zufallsvariablen $X$.
\label{bilduebergangzurverteilungsfunktion}}
\end{figure}
In diesem Abschnitt reduzieren wir die Information in einer Zufallsvariable
$X\colon\Omega\to\mathbb R$ auf einfache Funktionen einer reellen Ver"anderlichen.
Wir werden feststellen, dass die Verteilungsfunktion sowohl stetige wie auch
diskrete Zufallsvariablen gleichermassen beschreiben kann, dass wir
aber verschiedene analytische Methoden verwenden m"ussen, um daraus
Erwartungswerte zu berechnen.

\subsection{Verteilungsfunktion}
\index{Verteilungsfunktion}
\begin{figure}
\begin{center}
\includegraphics{images/verteilungsfunktion-2}
\end{center}
\caption{Die Verteilungsfunktion $F$ einer Zufallsvariablen $X$,
$F(x)=P(X\le x)$, zeigt die Wahrscheinlichkeit daf"ur, dass $X$ den Wert $x$ nicht
"uberschreitet.
\label{bildverteilungsfunktion}}
\end{figure}
Aus einem Wahrscheinlichkeitsmass $P$ auf $\Omega$ kann man mit Hilfe
von $X$ auch ein Wahrscheinlichkeitsmass auf $\mathbb{R}$ bilden.
F"ur die
Wahrscheinlichkeit f"ur das Intervall $[a,b]\subset\mathbb{R}$ setzt
man einfach
\[
P((a,b])=P(a< X\le b)
%=P(X^{-1}((a,b])),
\]
das letzte $P$ ist dabei als Wahrscheinlichkeit eines Ereignisses $\Omega$
bereits bekannt.
Noch etwas handlicher ist die folgende Definition.

\begin{definition}
Die Funktion $F\colon\mathbb{R}\to\mathbb{R}:x\mapsto P(X\le x)$
heisst Verteilungsfunktion von $P$.
\end{definition}
Die folgenden Eigenschaften folgen unmittelbar aus dieser Definition:
\begin{enumerate}
\item Der Wertebereich von $F(x)$ ist im Intervall $[0,1]$ enthalten.
\item Die Funktion $F(x)$ ist monoton wachsend:
Da $\{ X \le a\} \subset \{X\le b\}$ falls $a\le b$ folgt
$F(a)=P(X\le a)\le P(X\le b)=F(b)$.
\item
Die Wahrscheinlichkeit des unm"oglichen Ereignisses ist $0$, also
ist
\[
\lim_{x\to-\infty}F(x)=0.
\]
\item
Die Wahrscheinlichkeit des sicheren Ereignisses ist $1$, also gilt.
\[
\lim_{x\to\infty}F(x)=1.
\]
\end{enumerate}

Die Verteilungsfunktion $F$ beinhaltet alle Information, um $P$ zu
rekonstruieren.
Man kann mit ihrer Hilfe auch die Wahrscheinlichkeit
abgeschlossener Intervalle oder offener Intervalle berechnen:
\begin{satz} Sei $F$ die Verteilungsfunktion der Zufallsvariablen $X$,
dann gilt
\begin{align*}
P((a,b])&=F(b)-F(a)\\
P([a,b])&=\lim_{\xi\to a-} P((\xi,b])=F(b)-\lim_{\xi\to a-}F(\xi)\\
P((a,b))&=\lim_{\xi\to b-} P((a,\xi])=\lim_{\xi\to b-}F(\xi)-F(a)\\
P([a,b))&=\lim_{\eta\to b-}\lim_{\xi\to a-} P((\xi,\eta])=\lim_{\eta\to b-}F(\eta)-\lim_{\xi\to a-}F(\xi)\\
\end{align*}
\end{satz}
\begin{proof}[Beweis]Durch Nachrechnen unmittelbar aus den Definitionen.
\end{proof}

\subsection{Sprungstellen und diskrete Werte einer Zufallsvariable}
Als Spezialfall erh"alt man aus $F(x)$ auch die Wahrscheinlichkeit f"ur einen
einzelnen Wert
\[
P(a)=P([a,a])=F(a)-\lim_{\xi\to a-}F(\xi).
\]
Solange die Funktion $F$ stetig ist, wird diese Wahrscheinlichkeit
verschwinden.
Andernfalls hat die Verteilungsfunktion am Punkt $a$ eine
Sprungstelle, und die Wahrscheinlichkeit, dass der Wert $a$ eintritt,
ist gerade die Sprungh"ohe.

Man kann zeigen, dass eine monotone Funktion nur abz"ahlbar viele Sprungstellen
hat.
Es gibt also eine Folge $(a_k)_{k\in\mathbb{N}}$, welche alle
Sprungstellen enth"alt.
Diese Werte zeichnen sich dadurch aus, dass ihre
Wahrscheinlichkeit nicht $0$ ist und der Sprungh"ohe entspricht:
\[
P(a_k)=F(a_k)-\lim_{x\to a_k-}F(x).
\]
Aus diesen Sprungstellen kann man jetzt eine Funktion konstruieren,
die nur die Sprungstellen ber"ucksichtigt:
\[
F_d(x)=\sum_{k=1, a_k\le x}^\infty P(a_k)
\]
Die Funktion $F_d$ hat genau die gleichen Sprungstellen wie $F$, und
auch die gleichen Sprungh"ohen.
$F_d$ kann sich nur "andern, wenn in der
Summe ein neuer Summand hinzukommt, also is $F_d$ zwischen den Sprungstellen
konstant.
\index{Sprungstelle}

\begin{figure}
\begin{center}
\includegraphics{images/verteilungsfunktion-3}
\[
+
\]
\includegraphics{images/verteilungsfunktion-4}
\[
=
\]
\includegraphics{images/verteilungsfunktion-2}
\end{center}
\caption{Zerlegung einer beliebigen Verteilungsfunktion (unten) in diskrete (oben)
und stetige (Mitte) Komponente\label{bildzerlegungverteilungsfunktion}}
\end{figure}
\index{Verteilung!diskrete}
\index{Verteilung!stetige}

Die Differenz $F_s=F-F_d$ kann offensichtlich keine Sprungstellen haben,
wenigstens keine Sprungstellen bei Ann"aherung von links.
Damit liegt
die Vermutung nahe, dass $F_s$ eine stetige Funktion ist (daher der Index
$s$).
Das ist tats"achlich so:
\begin{satz}Eine Verteilungsfunktion $F$ kann immer zerlegt werden in eine
st"uckweise konstante, monoton wachsende, rechtsseitig stetige Funktion
$F_d$ und eine stetige Funktion $F_s$:
\[
F=F_d+F_s,
\]
ausserdem ist diese Zerlegung eindeutig.
\end{satz}
Die Zerlegung der Verteilungsfunktion $F$ ist in
Abbildung~\ref{bildzerlegungverteilungsfunktion} dargestellt.
Der nachfolgende Beweis des Satzes ist etwas technisch, wird aber sp"ater nicht
weiter ben"otigt, er kann daher auch "ubersprungen werden.
{\small
\begin{proof}[Beweis]
Es gen"ugt offensichtlich zu zeigen, dass $F$ rechtsseitig stetig ist, also
keine Sprungstellen bei Ann"aherung von rechts hat.
Die Differenz
$F(a+\varepsilon)-F(a)$ ist die Wahrscheinlichkeit, dass der Wert der
Zufallsvariable in das Intervall $(a,a+\varepsilon]$
f"allt.
Es gen"ugt, wenn man f"ur $\varepsilon=\frac1n$ zeigen kann,
dass $F(a+\frac1n)\to F(a)$.
Das Intervall $(a,a\frac1n]$ kann man
als Vereinigung unendlich vieler kleiner Teilintervalle
schreiben, also
\[
\left(a,{\textstyle a+\frac1n}\right]
= \left(a,a+{\textstyle \frac1N}\right]\cup
\bigcup_{k=n}^{N-1}\left(a+{\textstyle\frac1{k+1}},a+{\textstyle\frac1k}\right],
\]
wobei die Teilintervalle alle disjunkt sind.
Nimmt man nun die Wahrscheinlichkeit auf beiden Seiten, folgt aus den
Axiomen f"ur $P$
\[
P\left(\left(a,a+{\textstyle\frac1n}\right]\right)
= P\left(\left(a,a+{\textstyle \frac1N}\right]\right)+
\sum_{k=n}^{N-1}P\left(\left(a+{\textstyle\frac1{k+1}},a+{\textstyle\frac1k}\right]\right)
\]
Der Ausdruck $P\left(\left(a,a+{\frac1N}\right]\right)$ ist also der
Rest einer unendlichen Reihe mit ausschliesslich positiven Gliedern,
mit der $P\left(\left(a,a+{\textstyle\frac1n}\right]\right)$
berechnet werden kann.
 Nach den Axiomen f"ur $P$ gilt wegen
\[
\left(a,{\textstyle a+\frac1n}\right]
= 
\bigcup_{k=n}^{\infty}
\left(a+{\textstyle\frac1{k+1}},a+{\textstyle\frac1k}\right]
\]
auch
\[
P\left(\left(a,{\textstyle a+\frac1n}\right]\right)
= 
\sum_{k=n}^{\infty}
P\left(\left(a+{\textstyle\frac1{k+1}},a+{\textstyle\frac1k}\right]\right)
\]
d.~h.~die Reihe ist konvergent.
Also streben die einzelnen Glieder gegen null,
oder
\[
0=\lim_{n\to\infty}P\left(\left(a,{\textstyle a+\frac1n}\right]\right)
=\lim_{n\to\infty}F(a+{\textstyle\frac1n}) - F(a)=\lim_{x\to a+} F(x)-F(a),
\]
also genau die Aussage, dass die rechtsseitigen Grenzwerte mit den
Funktionswerten "ubereinstimmen, mithin die Funktion $F$ rechtsseitig
stetig ist.
Somit ist $F_s$ rechts- und linksseitig stetig.
\end{proof}
}
Im Allgemeinen besteht also eine Verteilungsfunktion $F$ aus einem
Teil, der Spr"unge darstellt, also bestimmte Werte der Zufallsvariable,
die mit nicht verschwindender Wahrscheinlichkeit auftreten k"onnen, und
einem Teil, der ``verschmiert'' ist "uber den ganzen Wertebereich.
Wir
betrachten jeden dieser Teile separat.

\subsection{St"uckweise konstante Verteilungsfunktion}
\begin{figure}
\begin{center}
\includegraphics{images/verteilungsfunktion-3}
\includegraphics{images/verteilungsfunktion-5}
\end{center}
\caption{Verteilungsfunktion (oben) und Wahrscheinlichkeiten einzelner
$x$-Werte (unten) einer diskreten Verteilung
\label{bilddiskreteverteilungsfunktion}}
\end{figure}
Ist die Verteilungsfunktion st"uckweise konstant, dann ist
$P((a,b])=0$ falls im Intervall $(a,b]$ keine Sprungstelle
vorhanden ist.
Ist $x_0$ eine Sprungstelle, dann ist ihr H"ohe
\[
\lim_{a\to x_0-}P((a,x_0])=\lim_{a\to x_0-}(F(x_0)-F(a))
F(x_0)-\lim_{a\to x_0-}F(a).
\]
Dies bedeutet, dass $P(\{x_0\})=F(x_0)-\lim_{a\to x_0-}F(a)$.
Offensichtlich haben also nur die Sprungwerte der Funktion eine
von 0 verschiedene Wahrscheinlichkeit, alle anderen Intervalle ohne
einen Sprungpunkt haben Wahrscheinlichkeit 0.
Die Wahrscheinlichkeiten
der einzelnen Sprungwerte k"onnen wie in
Abbildung~\ref{bilddiskreteverteilungsfunktion} visualisiert werden.

Ist $S=\{\,x\;|\;F(x)\ne \lim_{a\to x-}F(a)\,\}$ die Menge der Sprungpunkte 
der Verteilungsfunktion, dann kann man damit Wahrscheinlichkeiten und
Erwartungswerte wie folgt berechnen.
Die Wahrscheinlichkeit eines
Ereignisses h"angt davon ab, welche Sprungpunkte es enth"alt:
\[
P(A)=\sum_{x\in S}P(\{x\}),
\]
der Erwartungswert von $f(X)$ ist
\[
E(f(X))=\sum_{x\in S}f(x)P(\{x\}).
\]
Man kann also alle relevanten Gr"ossen berechnen, wenn man f"ur jede
Sprungstelle deren Wahrscheinlichkeit weiss, also die Funktion
\[
p\colon S\to\mathbb{R}:x\mapsto p(x)=P(\{x\}).
\]
Diese Funktion muss nat"urlich den Eigenschaften einer
Wahrscheinlichkeitsfunktion Rechnung tragen, was erf"ullt ist, wenn
$p(x)\ge 0$ f"ur alle Sprungpunkte und 
\[
\sum_{x\in S}p(x)=1.
\]
\begin{definition}Die Funktion $p\colon S\to\mathbb{R}:x\mapsto p(x)$
heisst diskrete Wahrscheinlichkeitsverteilung auf der Menge $S$.
\end{definition}
\begin{satz}F"ur eine diskrete Wahrscheinlichkeitsverteilung $p$ gilt
\begin{enumerate}
\item $\displaystyle P(A)=\sum_{x\in A\cap S}p(x)$,
\item $\displaystyle E(f(X))=\sum_{x\in S}f(x)p(x)$
\end{enumerate}
\end{satz}

\subsection{Stetige Verteilungsfunktion}
\begin{figure}
\begin{center}
\includegraphics{images/verteilungsfunktion-4}
\includegraphics{images/verteilungsfunktion-6}
\end{center}
\caption{Verteilungsfunktion (oben) und Wahrscheinlichkeitsdichte 
(unten) einer stetigen Verteilung\label{bildstetigeverteilungsfunktion}}
\end{figure}
Die Formel
\[
P((a,b])=F(b)-F(a)
\]
erinnert an die Berechnung eines Integrals mit Hilfe einer
Stammfunktion.
W"are $F$ differenzierbar, k"onnte man die Ableitung bilden,
$F$ w"are dann eine Stammfunktion von $\varphi=F'$.
Dann g"alte
\[
P(X\le x)=F(x)=\int_{-\infty}^x\varphi(\xi)\,d\xi.
\]
Im Allgemeinen ist selbst eine stetige Verteilungsfunktion nicht
differenzierbar.
Trotzdem gibt es dank eines Satzes, den wir hier nicht beweisen,
immer eine Funktion $\varphi$, die sich "ahnlich verh"alt wie die
Ableitung.
\begin{definition}
Die Funktion $\varphi$ heisst Wahrscheinlichkeitsdichte der Verteilungsfunktion
\index{Wahrscheinlichkeitsdichte}
\index{Dichtefunktion}
$F$ falls
\[
P(X\le x)=F(x)=\int_{-\infty}^x\varphi(\xi)\,d\xi
\]
gilt.
Man spricht in diesem Fall von einer stetigen Wahrscheinlichkeitsverteilung.
\end{definition}
Abbildung \ref{bildstetigeverteilungsfunktion} zeigt die
Wahrscheinlichkeitsdichte der stetigen Komponente der Verteilungsfunktion
aus Abbildung \ref{bildverteilungsfunktion}.

Bei diskreten Wahrscheinlichkeitsverteilungen folgte die Formel f"ur den
Erwartungswert von $X$ sofort aus der Definition, f"ur den stetigen
Fall haben wir aber noch keine exakte Definition.
Wenn wir $\mathbb{R}$
in viele kleine Intervalle $I_1=(\underline x_1,\overline x_1],\dots, I_n$
unterteilen, dann gilt
\[
\sum_{i}P(I_i)\underline x_i\le
E(X)
\le
\sum_{i}P(I_i)\overline x_i
\]
Da die Wahrscheinlichkeit des Intervalls $I_i$ mit Hilfe der
Wahrscheinlichkeitsdichte berechnet werden kann:
$P(I_i)=\int_{\underline x_i}^{\overline x_i}\varphi(\xi)\,d\xi,$
kann man die Ungleichung noch etwas weiter fassen:
\[
\sum_{i}\underline x_i\bigl(\min_{\xi\in I_i}\varphi(\xi)\bigr)(\overline x_i-\underline x_i)\le
E(X)
\le
\sum_{i}\overline x_i\bigl(\max_{\xi\in I_i}\varphi(\xi)\bigr)(\overline x_i-\underline x_i)
\]
L"asst man jetzt die Intervalll"ange immer kleiner werden, wird wegen der
Stetigkeit von $\varphi$ der Unterschied zwischen
$\max_{\xi\in I_i}\varphi(\xi)$ und
$\min_{\xi\in I_i}\varphi(\xi)$ beliebig klein, und auch jener
zwischen $\underline x_i$ und $\overline x_i$.
Beim Grenz"ubergang
zu beliebig kleinen Intervallen werden die beiden Seiten der Ungleichung
gegen das Integral
\[
\int_{-\infty}^{\infty}x\varphi(x)\,dx
\]
streben, d.~h.~der folgende Satz kann als Definition des Erwartungswertes
gesehen werden:
\begin{satz}
Ist $X$ eine Zufallsvariable mit stetiger
Wahrscheinlichkeitsverteilung mit Wahrscheinlichkeitsdichte
$\varphi$, dann
ist die Verteilungsfunktion $F$ "uberall dort differenzierbar, wo
$\varphi$ stetig ist, und es gilt dort
$F'(x)=\varphi(x)$.
Erwartungswerte von $X$ k"onnen mit Hilfe von
\begin{align*}
E(X)&=\int_{-\infty}^{\infty}x\varphi(x)\,dx\quad\text{und}\\
E(f(X))&=\int_{-\infty}^{\infty}f(x)\varphi(x)\,dx
\end{align*}
berechnet werden.
\end{satz}

Man beachte, dass diese Formel das im
Kapitel~\ref{chapter-erwartungswert-und-varianz} einf"uhrte Konzept
``Wert $\mathstrut \times\mathstrut $ Wahrscheinlichkeit''
auch f"ur stetige Zufallsvariablen realisiert, $x$ bzw.~$f(x)$ ist
der Wert-Teil, w"ahrend $\varphi(x)\,dx$ f"ur die Wahrscheinlichkeit
steht.
Aus der Summe ist ein Integral geworden.

\subsection{Wahrscheinlichkeit grosser Abweichungen}
Im vorhergehenden Kapitel haben wir mit dem Satz von Tschebyscheff ein
Hilfsmittel kennengelernt, welches erlaubt abzusch"atzen, dass eine
gross Abweichung vom Erwartungswert eintritt.
Der Satz stellte einen
Zusammenhang mit der Varianz her:
\[
P(|X-\mu|>\varepsilon)\le\frac{\operatorname{var}(X)}{\varepsilon^2}.
\]
Es wurde damals schon bemerkt, dass f"ur ``weniger wilde'' Verteilung mit
besseren Absch"atzungen gerechnet werden kann.
Insbesondere ist es mit
einer Dichtefunktion m"oglich, die Wahrscheinlichkeit einer grossen
Abweichung exakt zu berechnen:
\begin{satz} Ist $X$ eine Zufallsvariable, deren Verteilung die
Verteilungsfunktion $F$ hat, dann ist
\[
P(|X-\mu|>\varepsilon)=
1-F(\mu+\varepsilon)+\lim_{\xi\to (\mu-\varepsilon)-}F(\xi)
\]
Ist $\varphi$ eine Dichtefunktion, dann gilt
\[
P(|X-\mu|>\varepsilon)=1-\int_{\mu-\varepsilon}^{\mu+\varepsilon}\varphi(x)\,dx
\]
\end{satz}
\begin{proof}[Beweis]Die Formel f"ur die Dichtefunktion folgt offensichtlich
aus der Aussage "uber die Verteilungsfunktion, denn dann ist $F$ stetig
und der einseitige Grenzwert ist gerade der Funktionswert von $F$ an der
Stelle.
Es gen"ugt also letzteres
zu untersuchen.
\begin{align*}
P(|X-\mu|\le\varepsilon)
&=1-P(\{X\ge\mu-\varepsilon\}\cap\{X\le\mu+\varepsilon\})\\
&=1-F(\mu+\varepsilon)+\lim_{\xi\to (\mu-\varepsilon)-}F(\xi).
\end{align*}
\end{proof}
Eine Wahrscheinlichkeitsfunktion erlaubt also eine wesentlich
genauere Berechnung der Wahrscheinlichkeit einer grossen Abweichung,
nicht mehr nur eine grobe Absch"atzung.

\section{Beispiel: Gleichverteilung}
\index{Gleichverteilung}
Als Beispiel und zur Gegen"uberstellung stetiger und diskreter Zufallsvariablen
behandeln wir in diesem Abschnitt die diskrete und die stetige
Gleichverteilung.
Weitere Informationen zur Gleichverteilung sind
im Abschnitt~\ref{section-gleichverteilung-stetig} zu finden.

\subsection{Ganzzahlige Gleichverteilung}
\index{Gleichverteilung!ganzzahlig}
Eine Zufallsvariable $X$ nehme die ganzzahligen Werte $\{a,a+1,\dots,b\}$ mit 
gleicher Wahrscheinlichkeit $\frac1{b-a+1}$ an.
Die Verteilungsfunktion
(Abbildung~\ref{diskrete-gleichverteilung})
ist also
\[
F(x)=
\frac{\lfloor x \rfloor -a+1}{b-a+1}
\]
wobei $\lfloor x \rfloor$ die gr"osste ganze Zahl kleiner als $x$
bezeichnet:
\[
\lfloor x\rfloor = \max\{k\in\mathbb Z|k\le x\}.
\]
Die Wahrscheinlichkeitsverteilung ist
\[
P(X=x)=\begin{cases}
\displaystyle \frac1{b-a+1}&\qquad a\le x\le b,x\in\mathbb Z\\
0&\qquad\text{sonst}.
\end{cases}
\]
Damit kann jetzt auch Erwartungswert und Varianz bestimmt werden.
{\allowdisplaybreaks
\begin{align*}
E(X)
&=
\sum_{k=a}^b kP(X=k)
=
\sum_{k=a}^b \frac{k}{b-a+1}
=
\frac1{b-a+1}\sum_{k=a}^bk
\\
&=
\frac1{b-a+1}\biggl(
\sum_{k=1}^bk-\sum_{k=1}^{a-1}k
\biggr)
=
\frac1{b-a+1}\biggl(
\frac{b(b+1)}2-\frac{a(a-1)}2
\biggr)
\\
&=
\frac12\frac{b^2+b-a^2+a}{b-a+1}=\frac{a+b}2.
\\
E(X^2)
&=
\sum_{k=a}^b k^2P(X=k)
=
\sum_{k=a}^b \frac{k^2}{b-a+1}
=
\frac1{b-a+1}\sum_{k=a}^bk^2
\\
&=
\frac1{b-a+1}\biggl(
\sum_{k=1}^bk^2-\sum_{k=1}^{a-1}k^2
\biggr)
=
\frac{ 2(a^2+ab+b^2)+b-a}{6}
\\
\operatorname{var}(X)
&=
\frac{ 2(a^2+ab+b^2)+b-a}{6}
-
\frac{a^2+2ab+b^2}{4}
\\
&=
\frac1{12}\bigl(
4a^2+4ab+4b^2+2b-2a-3a^2-6ab-3b^2
\bigr)
\\
&=
\frac1{12}\bigl(
a^2-2ab+b^2+2b-2a+1-1
\bigr)
=
\frac1{12}\bigl((b-a+1)^2 - 1\bigr)
\end{align*}
}
Die Varianz ist also im Wesentlichen die quadrierte Intervalll"ange
geteilt durch 12.
\begin{figure}
\centering
\includegraphics{images/gl-2.pdf}
\caption{Verteilungsfunktion und Wahrscheinlichkeitsverteilung der diskreten
Gleichverteilung auf den Werten $1,\dots,n=10$
\label{diskrete-gleichverteilung}}
\end{figure}

\subsection{Stetige Gleichverteilung}
\begin{figure}
\centering
\includegraphics{images/verteilungsfunktion-7.pdf}
\caption{Verteilungsfunktion und Wahrscheinlichkeitsdichte der stetigen
Gleichverteilung im Intervall $[a,b]$
\label{stetige-gleichverteilung}}
\end{figure}
\index{Gleichverteilung!stetig}
\index{Gleichverteilung!auf einem Intervall}
Eine Zufallsvariable $X$ nehme die Werte im Intervall $[a,b]$
mit gleicher Wahrscheinlichkeit an, d.~h.~f"ur jedes Teilintervall
$[\xi,\eta]\subset[a,b]$ gilt 
\[
P(\xi\le X\le \eta)=\frac{\xi-\eta}{b-a}.
\]
Die Wahrscheinlichkeitsdichte ist dann (Abbildung~\ref{stetige-gleichverteilung})
\[
\varphi(x)=\begin{cases}
\displaystyle \frac1{b-a}&\qquad a\le x\le b\\
0&\qquad \text{sonst}
\end{cases}
\]
Erwartungswert und Varianz k"onnen damit direkt berechnet werden:
{\allowdisplaybreaks
\begin{align*}
E(X)
&=
\int_{-\infty}^\infty x\varphi(x)\,dx
=
\int_a^bx\frac1{b-a}\,dx
=
\frac1{b-a}\left[\frac12x^2\right]_a^b
=
\frac12\frac{b^2-a^2}{b-a}
\\
&=
\frac{a+b}2
\\
E(X^2)
&=
\int_{-\infty}^{\infty}x^2\varphi(x)\,dx
=
\int_a^bx^2\frac1{b-a}\,dx
=
\frac1{b-a}\left[\frac13x^3\right]_a^b
=
\frac13\frac{b^3-a^3}{b-a}
\\
&=
\frac13(b^2+ab+a^2)
\\
\operatorname{var}(X)
&=
\frac13(b^2+ab+a^2)
-
\frac14(a^2+2ab+b^2)
\\
&=
\frac1{12}(4b^2+4ab+4a^2-3a^2-6ab-3b^2
=
\frac1{12}(a^2-2ab+b^2)
\\
&=
\frac{(b-a)^2}{12}.
\end{align*}
}
Auch in diesem Fall ist die Varianz die quadrierte Intervalll"ange
geteilt durch 12.

\section{L"osungsstrategie f"ur Probleme "uber Zufallsvariablen}
Die f"ur die Berechnung aller wesentlichen wahrscheinlichkeitstheoretischen
Kennzahlen einer Zufallsvariablen n"otige Information ist in der
Verteilungsfunktion codiert.
Probleme "uber Zufallsvariablen k"onnen daher mit folgender L"osungsstrategie
angegangen werden.
\begin{enumerate}
\item
Zun"achst muss festgestellt werden, welche Verteilung die Zufallsvariablen
haben.
Zu jeder sp"ater zu besprechenden Verteilung ist also auch
festzuhalten, f"ur welchen Zweck sie geeignet ist.
\item 
Die Verteilungen haben Parameter, die als n"achstes bestimmt werden
m"ussen.
Dazu k"onnen die bekannten Formeln f"ur Kennzahlen einer
Verteilung wie Erwartungswert oder Varianz.
So k"onnen zum Beispiel aus Erwartungswert $\mu$ und Varianz $\sigma^2$ 
einer auf einem Intervall gleichverteilten Zufallsvariable  die
Gleichungen
\begin{align*}
a+b&=2\mu\\
b^2-a^2&=12\sigma^2\quad\Rightarrow\quad b-a=\frac{6\sigma^2}{\mu}
\end{align*}
abgeleitet werden, welche $a$ und $b$ zu bestimmen erlauben.
\item
Sobald die Parameter festliegen, k"onnen beliebige Wahrscheinlichkeiten
oder Erwartungswerte von $X$ oder Funktionen $f(X)$ ermittelt werden,
womit sich so ziemlich jede Frage beantworten l"asst.
\end{enumerate}
Im Folgenden geht es also vor allem darum, einen geeignet grossen
Katalog von Verteilungen bereitzustellen.
Dazu dienen neben einigen
Grundverteilungen auch Rechenregeln, welche aus bereits bekannten
Verteilungen neue Verteilungen abzuleiten gestatten.
Diese Rechenregeln
sind der Inhalt des n"achsten Abschnittes.
Im folgenden Kapitel wird
dann eine Auswahl von praktisch n"utzlichen Verteilungen mit ihren
Eigenschaften vorgestellt.

\section{Rechenregeln f"ur die Verteilungsfunktion}
Beim "Ubergang von $\Omega$ und $P$ mit
der Zufallsvariable $X$ zur Verteilungsfunktion $F$ 
ging jede Information
"uber den Zufallsmechanismus verloren, der im Hintergrund f"ur das zuf"allige
Auftreten der verschiedenen Werte von $X$ verantwortlich war.
Insbesondere
ist es bei zwei Zufallsvariablen $X$ und $Y$ nicht mehr m"oglich zu
rekonstruieren, ob sie unabh"angig oder wie gegebenenfalls die Abh"angigkeit
genau ausgesehen hat.
Ohne Kenntnis von $\Omega$ ist es daher im Allgemeinen
nicht mehr m"oglich, Erwartungswerte von beliebigen Funktionen von $X$ und $Y$
zu berechnen.
Insbesondere $E(XY)$ l"asst sich nur berechnen, wenn man weiss,
dass $X$ und $Y$ unabh"angig sind.

Es zeigt sich also in einigen interessanten F"allen durchaus m"oglich, nur mit
Hilfe der Wahrscheinlichkeitsdichte f"ur unabh"angige
Zufallsvariable interessante Erwartungswerte zu berechnen.

\subsection{Transformation auf Gleichverteilung}
Sei $X$ eine Zufallsvariable mit Verteilungsfunktion $F(x)$.
Wendet man die Funktion $F$ auf die Werte $X(\omega)$ der Zufallsvariable,
erhalten wir die Zufallsvariable $F\circ X$, wir schreiben sie auch
$F(X)$.
Wir berechnen die Verteilungsfunktion von $F(X)$
\begin{align*}
F_{F(X)}(y)
&=
P(F(X)\le y)
=
\begin{cases}
1&\qquad y > 1\\
y&\qquad 0\le x\le y\\
0&\qquad y < 0
\end{cases}
%=
%y
\end{align*}
Die Verteilungsfunktion von $F(X)$ ist also die Verteilungsfunktion einer
Gleichverteilung auf dem Intervall $[0,1]$.

\subsubsection{Anwendung: Erzeugung von Zufallszahlen mit Verteilungsfunktion \texorpdfstring{$F$}{F}}
Typischerweise stellen Bibliotheken Zufallszahlgeneratoren f"ur Zufallszahlen
im Intervall $[0,1]$ zur Verf"ugung.
Um Zufallszahlen mit einer beliebigen Verteilungsfunktion $F$ zu erzeugen,
verwendet man zun"achst einen solchen Zufallszahlgenerator und wendet dann die
Funktion $F^{-1}$ an, sofern diese existiert.
Die resultierenden Zufallszahlen haben Verteilungsfunktion $F$.

\begin{beispiel}
Die Verteilungsfunktion einer Gleichverteilung im Intervall $[a,b]$ ist
\[
F(x)=\frac{x-a}{b-a}
\]
mit der inversen Funktion
\[
F^{-1}(y)=a+(b-a)y.
\]
Ist $Y$ eine im Intervall $[0,1]$ gleichverteilte Zufallsvariable, dann ist
$F^{-1}(Y)=a+(b-a)Y$ eine im Intervall $[a,b]$ gleichverteilte Zufallsvariable.
\end{beispiel}

\subsection{Variablentransformation}
\index{Variablentransformation}
Wendet man auf die Werte einer Zufallsvariablen eine Funktion an, entsteht
eine neue Zufallsvariable, die im Allgemeinen eine g"anzlich andere Verteilung
haben wird.
In diesem Abschnitt soll die Verteilungsfunktion und falls
eine Dichte existiert auch diese der neuen Zufallsvariablen bestimmt werden.

\index{Abbildung!umkehrbare}
Sei $f$ eine umkehrbare, differenzierbare Abbildung
$f\colon\mathbb{R}\to\mathbb{R}$.
Dann ist $Y=f\circ X$ ebenfalls eine
Zufallsvariable.
Bezeichnen wir die Verteilungsfunktionen von $X$ und $Y$
mit $F_X$ bzw.~$F_Y$, dann gilt offensichtlich
\[
F_Y(y)=P(f\circ X\le y)=P(X\le f^{-1}(y))=F_X(f^{-1}(y))
\]
oder k"urzer $F_Y=F_X\circ f^{-1}$.

Wir nehmen nun an, dass die Verteilungsfunktionen ausserdem eine
Dichtefunktion haben, welche wir mit $\varphi_X$ bzw.~$\varphi_Y$
bezeichnen.
Wir schreiben $y=f(x)$, und versuchen die obigen
Wahrscheinlichkeiten mit den Dichtefunktionen zu berechnen:
\[
P(a<Y\le b)
=\int_a^b\varphi_Y(y)\,dy
=\int_{f^{-1}(a)}^{f^{-1}(b)}\varphi_Y(f(x)) f'(x)\,dx
\]
Also muss $\varphi_Y(f(x))f'(x)$ die Wahrscheinlichkeitsdichte von
$F_X$ sein, also 
\begin{align*}
\varphi_Y(y)&=\frac{\varphi_X(f^{-1}(y))}{f'(f^{-1}(y))}\\
\varphi_Y(f(x))&=\frac{\varphi_X(x)}{f'(x)}
\end{align*}
Wir fassen diese Resultate in einem Satz zusammen

\begin{satz}
\label{satz-variablentransformation}
Ist $f\colon\mathbb{R}\to\mathbb{R}$ eine invertierbare
differenzierbare Funktion, deren Ableitung nirgends verschwindet,
und $X$ eine Zufallsvariable mit Verteilungsfunkion $F_X$.
Dann ist
$f\circ X$ eine Zufallsvariable mit Verteilungsfunktion
$F_Y=F_X\circ f^{-1}$.
Hat $F_X$ die Dichtefunktion $\varphi_X$, dann
hat $F_Y$ die Dichtefunktion
\[
\varphi_Y=\frac{\varphi_X}{f'}\circ f^{-1}.
\]
\end{satz}

Ohne zus"atzliche Annahmen ist es nicht m"oglich, Erwartungswert und
Varianz zu berechnen.
Hingegen gibt es eine Gr"osse, die sich in jedem
Fall bestimmen l"asst:
\begin{definition}
Ist $X$ eine Zufallsvariable, dann heisst die Zahl $\operatorname{med}(X)$,
f"ur die
$F(\operatorname{med}(X))=\frac12$ ist, der Median von $X$. 
Falls mehrere Zahlen diese Bedingung erf"ullen, ist der Median als
deren Infimum definiert:
\[
\operatorname{med}(X)=\inf \{x\in\mathbb{R}\;|\;F(x)\ge{\textstyle \frac12}\}
\]
\end{definition}
F"ur den Median gilt $\operatorname{med}(Y)=f(\operatorname{med}(X))$.

\subsection{Standardisierung} \label{section-standardisierung}
\index{Standardisierung}
Ein wichtiger Spezialfall ist der folgende:
aus einer Zufallsvariable $X$ mit Erwartungswert
$\mu$ und Varianz $\sigma^2$ l"asst sich eine neue Zufallsvariable $Y$ mit
Erwartungswert $0$ und Varianz $1$ konstruieren, indem man setzt
\[
Y=\frac{X-\mu}{\sigma}.
\]
Dies ist eine Variablentransformation mit der Funktion
$y=f(x)=\frac1\sigma(x-\mu)$ und der Umkehrfunktion $f^{-1}(y)=\sigma y+\mu$.
In diesem Fall kann man den Erwartungswert von $Y$ mit Hilfe der Rechenregeln
ausrechnen
\[
E(Y)=\frac1{\sigma}(E(X)-\mu)=0,
\]
und ebenso die Varianz
\[
\operatorname{var}(Y)=\frac1{\sigma^2}\operatorname{var}(X)=1.
\]
Der Satz \ref{satz-variablentransformation} l"asst sich hier unmittelbar
anwenden, es ergeben sich Verteilungs- und Dichtefunktion von $Y$
wie folgt:
\begin{align*}
F_Y(y)&=F_X(y\sigma+\mu)\\
\varphi_Y(y)&=\sigma\varphi_X(y\sigma+\mu)
\end{align*}
Wir fassen diese Resultate in einem Satz zusammen:

\begin{satz}
\label{satz-standardisierung}
Ist $X$ eine Zufallsvariable mit Erwartungswert $\mu$ und
Varianz $\sigma^2$, dann ist
\[
Y=\frac{X-\mu}\sigma
\]
eine Zufallsvariable mit Erwartungswert 0 und Varianz 1.
Zwischen den Verteilungsfunktionen $F_X$ bzw.~$F_Y$ von $X$ bzw.~$Y$ 
bestehen die Beziehungen
\[
F_Y(y)=F_X(y\sigma+\mu)\qquad F_X(x)=F_Y\left(\frac{x-\mu}\sigma\right)
\]
Hat die Verteilungsfunktion eine Dichte, dann gilt ausserdem
\[
\varphi_Y(y)=\sigma\varphi_X(y\sigma+\mu)\qquad
\varphi_X(x)=\frac1{\sigma}\varphi_Y\left(\frac{x-\mu}\sigma\right)
\]
\end{satz}


\subsection{Zwei Zufallsvariablen}
Wir beschr"anken uns darauf, die Wahrscheinlichkeitsdichten von Summen
und Produkten von Zufallsvariablen zu berechnen, die wir als unabh"angig
voraussetzen.
Wir betrachten also zwei Zufallsvariablen $X_1$ und $X_2$ mit reellen
Werten.
Da die Werte voneinander unabh"angig sind, ist
die passende Ereignisalgebra die Produktalgebra, also
$(x_1,x_2)\in
\mathbb{R}\times\mathbb{R}$,
und die Ereignisse sind kartesische Produkte von Intervallen.
Dann sind
die Wahrscheinlichkeiten $P(X_1\le x_1)$  und $P(X_2\le x_2)$ unabh"angig
voneinander, insbesondere ist $P(X_1\le x_1)=F_1(x_1)$ und
$P(X_2\le x_2)=F_2(x_2)$.

Die Wahrscheinlichkeit, dass $X_1$ in einem Intervall $(a_1,b_1]$,
$X_2$ im Intervall $(a_2,b_2]$ liegt ist 
\begin{align*}
P(a_1<X_1\le b_1\wedge a_2<X_2\le b_2)
&= P(a_1<X_1\le b_1) P(a_2<X_2\le b_2)\\
&=\int_{a_1}^{b_1}\varphi_1(x_1)\,dx_1
\int_{a_2}^{b_2}\varphi_2(x_2)\,dx_2\\
&=\int_{a_1}^{b_1}dx_1
\int_{a_2}^{b_2}dx_2\,
\varphi_1(x_1) \varphi_2(x_2).
\end{align*}
Die zweidimensionale
Verteilungsfunktion $F(x_1,x_2)=P(X_1\le x_1\wedge X_2\le x_2)$ hat
also eine zweidimensionale Wahrscheinlichkeitsdichte
\[
\varphi(x_1,x_2)=\varphi_1(x_1)\varphi_2(x_2).
\]

\subsection{Maximum zweier Zufallsvariablen}
\index{Maximum zweier Zufallsvariablen}
Seien $X$ und $Y$ zwei unabh"angige Zufallsvariablen mit Verteilungsfunktion
$F_X(x)$ und $F_Y(y)$.
Gesucht ist die Verteilungsfunktion der Zufallsvariablen
$\max(X,Y)$.
Es gilt
\begin{align*}
F_{\max(X,Y)}(z)
&=
P(\max(X,Y)\le z)
=
P(X\le z\wedge Y\le z)
=
P(X\le z)\,P(Y\le z)
=
F_X(z)F_Y(z).
\end{align*}

\subsection{Summe zweier Zufallsvariablen}
\index{Summe zweier Zufallsvariablen}
Seien $X_1$ und $X_2$ zwei Zufallsvariable mit Verteilungsfunktion $F_1$ und
$F_2$.
Gesucht ist die Verteilungsfunktion $F$ von $X_1+X_2$, sowie die
Wahrscheinlichkeitsverteilung und gegebenenfalls die Wahrscheinlichkeitsdichte.

Zun"achst ist nach Definition der Verteilungsfunktion
\[
F(z)=P(X_1+X_2\le z).
\]
Die Berechnung von $F$ aus $F_1$ und $F_2$ verl"auft f"ur diskrete und
kontinuierliche Wahrscheinlichkeitsverteilungen verschieden.
\subsubsection{Diskreter Fall}
In diesem Fall hat $X_i$ die Wahrscheinlichkeitsverteilung
$p_i(x)$ auf der Menge $S_i$, und es gilt
\begin{align*}
F(z)&=\sum_{x_1\in S_1, x_2\in S_2, x_1+x_2\le z}p(x_1)p(x_2)\\
&=\sum_{x_1\in S_1}p(x_1)\sum_{x_2\in S_2, x_2 \le z - x_1}p(x_2)\\
&=\sum_{x_1\in S_1}p(x_1) F_2(z-x_1)
\end{align*}
Oder, wenn vor allem die Wahrscheinlichkeitsverteilung interessiert:
\[
p(z)=\sum_{x_1\in S_1, x_2\in S_2, x_1+x_1=z}p(x_1)p(x_2).
\]
\subsubsection{Stetiger Fall}
Im stetigen Fall kann man schreiben
\begin{align*}
F(z)&=\int_{-\infty}^{\infty}\int_{-\infty}^{z-x_1}\varphi_1(x_1)\varphi_2(x_2)
\,dx_2\,dx_1\\
&=\int_{-\infty}^{\infty}
\varphi_1(x_1)
\int_{-\infty}^{z-x_1}
\varphi_2(x_2)
\,dx_2\,dx_1\\
&=\int_{-\infty}^{\infty}
\varphi_1(x_1)
F_2(z-x_1)
\,dx_1\\
\end{align*}
Nun interessiert uns vor allem die Wahrscheinlichkeitsdichte, also die
``Ableitung'' von $F$:
\[
\varphi(z)=\int_{-\infty}^{\infty}\varphi_1(x_1)\varphi_2(z-x_1)\,dx_1
\]
Diese Integral heisst die Faltung der Funktionen $\varphi_1$ und $\varphi_2$,
$\varphi=\varphi_1*\varphi_2$.
\index{Faltung}

\subsection{Verteilung von \texorpdfstring{$X^2$}{X hoch 2}}
Wir bestimmen die Wahrscheinlichkeitsdichte f"ur das Quadrat 
$X^2$ einer stetig verteilte Zufallsvariable $X$.
Sei $F_{X^2}$ die Verteilungsfunktion von $X^2$, $F_X$ jene von $X$.
Dann gilt f"ur $z\ge 0$
\begin{align*}
F_{X^2}(z)&=P(X^2\le z)=P(-\sqrt{z}\le X\le \sqrt{z})\\
&=F_X(\sqrt{z})-F(-\sqrt{z})\\
&=\int_{-\infty}^{\sqrt{z}}\varphi(x)\,dx-\int_{-\infty}^{-\sqrt{z}}\varphi(x)\,dx\\
&=\int_{-\sqrt{z}}^{\sqrt{z}}\varphi(x)\,dx
\end{align*}
F"ur negative $z$ verschwindet die Verteilungsfunktion nat"urlich, denn
die Wahrscheinlichkeit dass $X^2\le z <0$ ist nat"urlich 0.

Wir interessieren uns aber wieder nur f"ur Dichtefunktion, die wir
durch Ableiten finden k"onnen:
\[
\varphi_{X^2}(z)=\begin{cases}
\frac{1}{2\sqrt{z}}\left(\varphi_X(\sqrt{z})+\varphi_X(-\sqrt{z})\right)&\qquad z\ge 0\\
0&\qquad z<0
\end{cases}
\]
Weitere Beispiele f"ur aus mehreren unabh"angigen Zufallsvariablen
verkn"upften Verteilungen werden wir weiter unten behandeln, wobei
dann auch eine konkrete Wahrscheinlichkeitsdichtefunktion gegeben sein
wird.

\subsection{Verteilung von \texorpdfstring{$\sqrt{X}$}{Wurzel X}} \label{verteilungsfunktion-wurzel}
Sei $X$ eine Zufallsvariable mit $X\ge 0$.
Hat $X$ Verteilungsfunktion $F_X$
und Dichtefunktion
$\varphi_X$, dann ist die Verteilungsfunktion von $\sqrt{X}$
\[
F_{\sqrt{X}}(x)=P(\sqrt{X}\le x)=P(X\le x^2)=F_X(x^2)
\]
F"ur die Dichtefunktionen bedeutet das
\[
F_{\sqrt{X}}(x)=\int_0^{x^2}\varphi_X(t)\,dt
\]
woraus man die Dichtefunktion $\varphi_{\sqrt{X}}$ durch ableiten
bestimmen kann:
\[
\varphi_{\sqrt{X}}(x)=2x\varphi_X(x^2).
\]

\subsection{Verteilung von \texorpdfstring{$X/Y$}{X/Y}} \label{verteilungsfunktion-quotient}
Seien $X$ und $Y$ Zufallsvariablen mit $Y>0$, dann ist $T=X/Y$ ebenfalls
eine Zufallsvariable.
Ist $\varphi_Y$ die Dichtefunktion von $Y$,
und $F_X$ die Verteilungsfunktion von $X$, dann kann man die
Verteilungsfunktion von $T$ wie folgt berechnen:
\[
P(T\le t)=P(X\le tY)=\int_0^\infty F_X(ty)\varphi_Y(y)\,dy
\]
Die Ableitung nach $t$ ergibt die zugeh"orige Dichtefunktion
\[
\varphi_T(t)=\int_0^\infty\varphi_X(ty)y\varphi_Y(y)\,dy.
\]

\subsection{Verteilung von \texorpdfstring{$XY$}{XY}} \label{verteilungsfunktion-produkt}
\index{Produkt zweier Zufallsvariable}
Seien $X$ und $Y$ unabh"angige, positive Zufallsvariable, dann ist $T=XY$
ebenfalls eine Zufallsvariable.
Wir suchen die Warhscheinlichkeitsdichte $\varphi_T$ von $T$.
Zun"achst gilt nach Definition
\[
F_T(t)=P(XY\le t).
\]
Die Wahrscheinlichkeit, dass $X$ einen Wert in einem sehr schmalen Intervall
$[x,x+dx]$ annimmt ist $\varphi_X(x)\,dx$.
Damit beim Eintreffen dieses Ereignisses auch das Produkt
$XY\le t$ ist, muss $Y$ einen Wert im Intervall $[0,t/x]$ annehmen.
Also gilt wegen der Unabh"angigkeit von $X$ und $Y$
\begin{align*}
P(XY\le t\;\wedge\; X\in[x,x+dx])&=P(X\in[x,x+dx]\;\wedge\; Y\le t/x)\\
&=P(X\in[x,x+dx])\cdot P(Y\le t/x)\\
&=\varphi_X(x)\,dx\cdot F_Y(t/x).
\end{align*}
Um nun die gesamte Wahrscheinlichkeit $P(XY\le t)$ zu finden, m"ussen die eben
berechneten Terme ``summiert'' werden:
\[
P(XY\le t)=\int_0^\infty\varphi_X(x)F_Y(t/x)\,dx
\]
Durch Ableiten nach $t$ findet man
\[
\varphi_{XY}(t)=\frac{d}{dt}F_{XY}(t)=\int_0^\infty \varphi_X(x)\varphi_Y(t/x)\,\frac{dx}x.
\]



