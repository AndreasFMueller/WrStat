In der linearen Algebra lernt man, dass ein überbestimmtes Gleichungssystem
mit $m$ Gleichungen und $n<m$ Unbekannten der Form $Ax=b$ im Sinne kleinster Quadrate
gelöst werden kann, indem man aus der $m\times n$-Matrix $A$ und dem 
$m$-dimensionalen Vektor das Gleichungssystem
\[
A^tA x= A^tb
\]
mit genau $n$ Gleichungen für die $n$ Unbekannten $x_1,\dots,x_n$ löst.
In einem linearen Regressionsproblem für die $m$ Datenpunkte $(x_i,y_i)$
möchte man Koeffizienten $a$ und $b$ so finden, dass die Datenpunkte
die Gleichung $y_i=ax_i+b$ ``möglichst gut'' erfüllen.

Führen Sie das Least-Squares-Verfahren aus der linearen Algebra durch und
kontrollieren Sie, ob sie die gleichen Formeln für $a$ und $b$ erhalten
wie im linearen Regressionsverfahren.

\begin{loesung}
Die Unbekannten des Least-Squares-Problems sind die Zahlen $a$ und $b$,
wir schreiben das überbestimmte Gleichungssystem in der Form
\[
\begin{linsys}{2}
  ax_1&+&     b&=&y_1\phantom{.}\\
  ax_2&+&     b&=&y_2\phantom{.}\\
\vdots& &      & &   \\
  ax_n&+&     b&=&y_n.
\end{linsys}
\]
Die Matrix des Gleichungssystems und der Vektor der rechten Seite ist 
\[
A=
\begin{pmatrix}
x_1&1\\
x_2&1\\
\vdots&\vdots\\
x_n&1
\end{pmatrix}
\qquad
\text{bzw.}
\qquad
b=\begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{pmatrix}.
\]
Das Least-Squares-Verfahren verlangt die Berechnung von $A^tA$ und $A^tb$,
man erhält
\[
\begin{aligned}
A^tA
&=\begin{pmatrix}
x_1&x_2&\dots&x_n\\
  1&  1&\dots&1
\end{pmatrix}
\begin{pmatrix}
x_1&1\\
x_2&1\\
\vdots&\vdots\\
x_n&1
\end{pmatrix}
=
\begin{pmatrix}
\displaystyle \sum_{i=1}^nx_i^2& \displaystyle \sum_{i=1}^nx_i\\
\displaystyle \sum_{i=1}^nx_i  & n
\end{pmatrix}
\qquad\text{und}
\\
A^tb
&=\begin{pmatrix}
x_1&x_2&\dots&x_n\\
  1&  1&\dots&1
\end{pmatrix}
\begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{pmatrix}
=\begin{pmatrix}
\displaystyle \sum_{i=1}^n x_iy_i\\
\displaystyle \sum_{i=1}^n y_i
\end{pmatrix}.
\end{aligned}
\]
Wenn man $A^tA$ und $A^tb$ beide durch $n$ dividiert, ändert dies nichts an
der Lösung des Gleichungssystems $A^tAx=A^tb$, dann hat man aber ein
Gleichungssystem mit Koeffizientenmatrix und rechter Seite wie folgt:
\begin{align*}
\frac1nA^tA&=\begin{pmatrix}
\displaystyle \frac1n\sum_{i=1}^nx_i^2& \displaystyle \frac1n\sum_{i=1}^nx_i\\
\displaystyle \frac1n\sum_{i=1}^nx_i  & 1
\end{pmatrix}
\simeq
\begin{pmatrix}
E(X^2)&E(X)\\E(X)&1
\end{pmatrix}
&&\text{und}
&
\frac1nA^tb&=\begin{pmatrix}
\displaystyle \frac1n\sum_{i=1}^n x_iy_i\\
\displaystyle \frac1n\sum_{i=1}^n y_i
\end{pmatrix}
\simeq
\begin{pmatrix}
E(XY)\\E(Y)
\end{pmatrix}
\end{align*}
Dabei haben wir die Summen als empirische Erwartungswerte interpretiert.
Das war genau die Matrix und die rechte Seite, die in der Vorlesung gefunden
und mit dem Verfahren von Kramer gelöst wurde, um die Koeffizienten $a$ und $b$
der linearen Regression zu finden.
Lineare Regression ist also ein Spezialfall des Least-Squares-Verfahrens.
\end{loesung}
